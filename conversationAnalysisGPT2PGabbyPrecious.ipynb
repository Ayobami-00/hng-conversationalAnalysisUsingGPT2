{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conversationAnalysisGPT2PGabbyPrecious.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8-uuHLeghTv",
        "colab_type": "code",
        "outputId": "4f971ecd-c8d6-4988-cbc9-06c02e5fab8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Loading up our drive on google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DopvVDkPhSB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "import pandas as pd\n",
        "from numpy import int64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Iimo_fmhg3S",
        "colab_type": "code",
        "outputId": "b263f7f1-7305-47fe-8f15-fcb5454208e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Cloning the gpt-2 model\n",
        "!git clone https://github.com/tenoke/gpt-2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'gpt-2' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVooGwRmhqW6",
        "colab_type": "code",
        "outputId": "c900fff9-a414-40b3-be7c-707218a445f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#changing the root directory\n",
        "cd gpt-2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9qLK9QSiSnS",
        "colab_type": "code",
        "outputId": "ad98b4cd-e7f6-4e2b-ff49-83b52f868e3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "#Installing the required files in the requirements.txt file\n",
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.8MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Collecting tqdm==4.31.1 (from -r requirements.txt (line 4))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 22.3MB/s \n",
            "\u001b[?25hCollecting toposort==1.5 (from -r requirements.txt (line 5))\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103527 sha256=e1eaa4ee80d3858cc93eeb1732fbafb31efd9de92265e0ce5d958f6af6a1731a\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=533173 sha256=efd7fc511e98393319c38200e565f6cb2a3c6a5bb2c68da59477477f51613995\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "Installing collected packages: fire, regex, tqdm, toposort\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed fire-0.2.1 regex-2017.4.5 toposort-1.5 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW0RHKHhifTO",
        "colab_type": "code",
        "outputId": "0f17d7a4-7f5e-411d-fdbf-a3d4fd873762",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#downloading the model\n",
        "!python download_model.py 345M"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 601kit/s]                                                      \n",
            "\rFetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\rFetching encoder.json: 1.04Mit [00:00, 40.0Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.00Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:25, 56.5Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 6.54Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 26.4Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 40.3Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s-QwIcRi-h2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#setting up the kaggle api so that we can access our ubuntu corpus dataset\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJcUq_WjjJCl",
        "colab_type": "code",
        "outputId": "b75e9f91-c261-481e-8ee2-9a0be3be1d2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls -lha kaggle.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 69 Oct 12 16:24 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLfkFDD2j-Ht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Installing the Kaggle API client\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu7fimx6kFeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4riBarrkNxX",
        "colab_type": "code",
        "outputId": "41044d65-1c43-45c8-e769-9a6ff02538a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!kaggle datasets download -d rtatman/ubuntu-dialogue-corpus"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ubuntu-dialogue-corpus.zip to /content/gpt-2\n",
            " 99% 788M/799M [00:07<00:00, 104MB/s] \n",
            "100% 799M/799M [00:07<00:00, 117MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD8itpaikWKL",
        "colab_type": "code",
        "outputId": "81232d5d-a9da-4d39-8a09-ee6dd3b858a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!unzip ubuntu-dialogue-corpus.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ubuntu-dialogue-corpus.zip\n",
            "  inflating: Ubuntu-dialogue-corpus/dialogueText.csv  \n",
            "  inflating: Ubuntu-dialogue-corpus/dialogueText_196.csv  \n",
            "  inflating: Ubuntu-dialogue-corpus/dialogueText_301.csv  \n",
            "  inflating: toc.csv                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeMfM98lke38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir ubuntu-data ubuntu-npz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVzru-VklxPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datasets = pd.read_csv('Ubuntu-dialogue-corpus/dialogueText_196.csv', parse_dates=['date'], chunksize=1200000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFvgdnkQk5ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 1\n",
        "\n",
        "for dataset in datasets:\n",
        "  dataset['date'] = dataset['date'].astype(int64) // 10**9\n",
        "  \n",
        "  text_corpus = ''\n",
        "  current = None\n",
        "  for msg in dataset.itertuples():\n",
        "    if msg.dialogueID != current:\n",
        "      current = msg.dialogueID\n",
        "      text_corpus += '\\n\\n'\n",
        "    try:\n",
        "      text_corpus += f\"({msg.date}) {msg._4}: {msg.text}\\n\"\n",
        "    except KeyError:\n",
        "      pass\n",
        "  \n",
        "  with open(f'ubuntu-data/ubuntu-cleaned-{i}.txt', 'w') as f:\n",
        "    f.write(text_corpus)\n",
        "  del(text_corpus)\n",
        "  i +=1\n",
        "    \n",
        "del(datasets)\n",
        "del(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjlfJRyEk9s3",
        "colab_type": "code",
        "outputId": "70587d47-7eb3-4eb0-f9fc-288e1c6b2019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!du -h ubuntu-data/*"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "91M\tubuntu-data/ubuntu-cleaned-1.txt\n",
            "92M\tubuntu-data/ubuntu-cleaned-2.txt\n",
            "90M\tubuntu-data/ubuntu-cleaned-3.txt\n",
            "89M\tubuntu-data/ubuntu-cleaned-4.txt\n",
            "89M\tubuntu-data/ubuntu-cleaned-5.txt\n",
            "86M\tubuntu-data/ubuntu-cleaned-6.txt\n",
            "85M\tubuntu-data/ubuntu-cleaned-7.txt\n",
            "60M\tubuntu-data/ubuntu-cleaned-8.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_YiVOr4lBya",
        "colab_type": "code",
        "outputId": "e65a6c04-e63f-47f9-a85f-60a6dfd744e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-1.txt ubuntu-npz/ubuntu-cleaned-1.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-2.txt ubuntu-npz/ubuntu-cleaned-2.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-3.txt ubuntu-npz/ubuntu-cleaned-3.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-4.txt ubuntu-npz/ubuntu-cleaned-4.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-5.txt ubuntu-npz/ubuntu-cleaned-5.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-6.txt ubuntu-npz/ubuntu-cleaned-6.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-7.txt ubuntu-npz/ubuntu-cleaned-7.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-8.txt ubuntu-npz/ubuntu-cleaned-8.txt.npz --model_name 345M"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading files\n",
            "100% 1/1 [03:23<00:00, 203.31s/it]\n",
            "Writing ubuntu-npz/ubuntu-cleaned-1.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [03:26<00:00, 206.89s/it]\n",
            "Writing ubuntu-npz/ubuntu-cleaned-2.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [03:12<00:00, 192.88s/it]\n",
            "Writing ubuntu-npz/ubuntu-cleaned-3.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [03:20<00:00, 200.44s/it]\n",
            "Writing ubuntu-npz/ubuntu-cleaned-4.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [03:10<00:00, 190.23s/it]\n",
            "Writing ubuntu-npz/ubuntu-cleaned-5.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [03:08<00:00, 188.30s/it]\n",
            "Writing ubuntu-npz/ubuntu-cleaned-6.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [02:59<00:00, 179.14s/it]\n",
            "Writing ubuntu-npz/ubuntu-cleaned-7.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [02:10<00:00, 130.59s/it]\n",
            "Writing ubuntu-npz/ubuntu-cleaned-8.txt.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLwHLUH8lRPz",
        "colab_type": "code",
        "outputId": "a70bf986-f24b-418e-b26a-b3239ce85efc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!du -h ubuntu-npz/*"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41M\tubuntu-npz/ubuntu-cleaned-1.txt.npz\n",
            "46M\tubuntu-npz/ubuntu-cleaned-2.txt.npz\n",
            "41M\tubuntu-npz/ubuntu-cleaned-3.txt.npz\n",
            "41M\tubuntu-npz/ubuntu-cleaned-4.txt.npz\n",
            "39M\tubuntu-npz/ubuntu-cleaned-5.txt.npz\n",
            "39M\tubuntu-npz/ubuntu-cleaned-6.txt.npz\n",
            "37M\tubuntu-npz/ubuntu-cleaned-7.txt.npz\n",
            "27M\tubuntu-npz/ubuntu-cleaned-8.txt.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "407cM3sPjXjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r ubuntu-npz/ /content/drive/My\\ Drive/ubuntu-npz/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj4t0dLVlI5F",
        "colab_type": "code",
        "outputId": "051738a1-5ed8-4e02-d694-22c862c1bf1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!PYTHONPATH=src ./train.py --dataset ubuntu-npz/ --sample_every=250 --stop_after=1501 --model_name 345M"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:73: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:76: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-10-12 17:05:24.539670: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-10-12 17:05:24.540134: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1615480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-12 17:05:24.540177: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-10-12 17:05:24.568243: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-10-12 17:05:24.696126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 17:05:24.697075: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6dbd180 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-12 17:05:24.697123: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-10-12 17:05:24.697722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 17:05:24.698437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-12 17:05:24.709001: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-12 17:05:24.886450: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-12 17:05:24.985159: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-12 17:05:25.014509: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-12 17:05:25.219512: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-12 17:05:25.367142: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-12 17:05:25.792129: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-12 17:05:25.792442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 17:05:25.793243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 17:05:25.793908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-12 17:05:25.798795: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-12 17:05:25.800491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-10-12 17:05:25.800527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-10-12 17:05:25.800541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-10-12 17:05:25.801788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 17:05:25.802671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 17:05:25.803511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From ./train.py:77: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From ./train.py:100: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:113: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:89: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:120: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:122: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:125: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:129: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "Loading dataset...\n",
            "100% 8/8 [00:08<00:00,  1.15s/it]\n",
            "dataset has 233929304 tokens\n",
            "Training...\n",
            "2019-10-12 17:06:43.970571: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 16.91] loss=3.34 avg=3.34\n",
            "[2 | 19.90] loss=2.96 avg=3.15\n",
            "[3 | 22.88] loss=2.13 avg=2.80\n",
            "[4 | 25.88] loss=2.95 avg=2.84\n",
            "[5 | 28.89] loss=2.57 avg=2.79\n",
            "[6 | 31.88] loss=2.85 avg=2.80\n",
            "[7 | 34.88] loss=2.82 avg=2.80\n",
            "[8 | 37.89] loss=2.77 avg=2.80\n",
            "[9 | 40.91] loss=2.52 avg=2.76\n",
            "[10 | 43.91] loss=3.17 avg=2.81\n",
            "[11 | 46.89] loss=2.77 avg=2.80\n",
            "[12 | 49.90] loss=2.51 avg=2.78\n",
            "[13 | 52.92] loss=2.59 avg=2.76\n",
            "[14 | 55.94] loss=2.56 avg=2.75\n",
            "[15 | 58.94] loss=2.80 avg=2.75\n",
            "[16 | 61.95] loss=3.00 avg=2.77\n",
            "[17 | 64.97] loss=2.00 avg=2.72\n",
            "[18 | 67.97] loss=2.44 avg=2.70\n",
            "[19 | 71.00] loss=2.38 avg=2.68\n",
            "[20 | 74.01] loss=2.52 avg=2.67\n",
            "[21 | 77.02] loss=2.55 avg=2.67\n",
            "[22 | 80.03] loss=1.29 avg=2.60\n",
            "[23 | 83.05] loss=2.59 avg=2.60\n",
            "[24 | 86.07] loss=2.90 avg=2.61\n",
            "[25 | 89.09] loss=2.45 avg=2.60\n",
            "[26 | 92.11] loss=2.53 avg=2.60\n",
            "[27 | 95.12] loss=2.98 avg=2.62\n",
            "[28 | 98.13] loss=2.56 avg=2.62\n",
            "[29 | 101.15] loss=2.24 avg=2.60\n",
            "[30 | 104.17] loss=2.65 avg=2.60\n",
            "[31 | 107.17] loss=2.88 avg=2.61\n",
            "[32 | 110.20] loss=2.68 avg=2.62\n",
            "[33 | 113.21] loss=2.33 avg=2.60\n",
            "[34 | 116.22] loss=2.33 avg=2.60\n",
            "[35 | 119.24] loss=2.14 avg=2.58\n",
            "[36 | 122.25] loss=2.47 avg=2.58\n",
            "[37 | 125.27] loss=2.92 avg=2.59\n",
            "[38 | 128.28] loss=3.31 avg=2.61\n",
            "[39 | 131.30] loss=3.06 avg=2.62\n",
            "[40 | 134.32] loss=2.51 avg=2.62\n",
            "[41 | 137.32] loss=2.52 avg=2.62\n",
            "[42 | 140.33] loss=2.58 avg=2.62\n",
            "[43 | 143.35] loss=2.26 avg=2.61\n",
            "[44 | 146.36] loss=2.76 avg=2.61\n",
            "[45 | 149.37] loss=1.87 avg=2.59\n",
            "[46 | 152.37] loss=3.12 avg=2.61\n",
            "[47 | 155.39] loss=2.41 avg=2.60\n",
            "[48 | 158.41] loss=2.45 avg=2.60\n",
            "[49 | 161.42] loss=2.58 avg=2.60\n",
            "[50 | 164.45] loss=2.21 avg=2.59\n",
            "[51 | 167.46] loss=2.75 avg=2.59\n",
            "[52 | 170.48] loss=2.46 avg=2.59\n",
            "[53 | 173.49] loss=2.62 avg=2.59\n",
            "[54 | 176.51] loss=2.54 avg=2.59\n",
            "[55 | 179.53] loss=2.53 avg=2.59\n",
            "[56 | 182.54] loss=2.55 avg=2.58\n",
            "[57 | 185.56] loss=3.25 avg=2.60\n",
            "[58 | 188.59] loss=2.32 avg=2.59\n",
            "[59 | 191.62] loss=2.79 avg=2.60\n",
            "[60 | 194.62] loss=2.17 avg=2.59\n",
            "[61 | 197.64] loss=2.50 avg=2.59\n",
            "[62 | 200.66] loss=2.42 avg=2.58\n",
            "[63 | 203.69] loss=2.68 avg=2.58\n",
            "[64 | 206.70] loss=2.23 avg=2.58\n",
            "[65 | 209.74] loss=2.89 avg=2.58\n",
            "[66 | 212.77] loss=2.09 avg=2.57\n",
            "[67 | 215.78] loss=2.00 avg=2.56\n",
            "[68 | 218.79] loss=2.40 avg=2.56\n",
            "[69 | 221.81] loss=3.13 avg=2.57\n",
            "[70 | 224.82] loss=2.46 avg=2.57\n",
            "[71 | 227.84] loss=2.42 avg=2.56\n",
            "[72 | 230.85] loss=2.47 avg=2.56\n",
            "[73 | 233.86] loss=2.50 avg=2.56\n",
            "[74 | 236.88] loss=2.39 avg=2.56\n",
            "[75 | 239.90] loss=2.97 avg=2.57\n",
            "[76 | 242.92] loss=2.54 avg=2.57\n",
            "[77 | 245.94] loss=2.60 avg=2.57\n",
            "[78 | 248.95] loss=2.42 avg=2.56\n",
            "[79 | 251.96] loss=2.77 avg=2.57\n",
            "[80 | 254.97] loss=2.36 avg=2.56\n",
            "[81 | 257.98] loss=3.15 avg=2.57\n",
            "[82 | 260.99] loss=2.04 avg=2.56\n",
            "[83 | 264.00] loss=2.62 avg=2.57\n",
            "[84 | 267.02] loss=2.64 avg=2.57\n",
            "[85 | 270.04] loss=2.20 avg=2.56\n",
            "[86 | 273.04] loss=3.09 avg=2.57\n",
            "[87 | 276.06] loss=2.13 avg=2.56\n",
            "[88 | 279.08] loss=2.06 avg=2.55\n",
            "[89 | 282.11] loss=2.77 avg=2.56\n",
            "[90 | 285.11] loss=2.54 avg=2.56\n",
            "[91 | 288.13] loss=2.73 avg=2.56\n",
            "[92 | 291.15] loss=2.70 avg=2.56\n",
            "[93 | 294.16] loss=2.37 avg=2.56\n",
            "[94 | 297.17] loss=2.47 avg=2.56\n",
            "[95 | 300.18] loss=2.70 avg=2.56\n",
            "[96 | 303.19] loss=2.25 avg=2.55\n",
            "[97 | 306.20] loss=2.16 avg=2.55\n",
            "[98 | 309.20] loss=2.26 avg=2.54\n",
            "[99 | 312.23] loss=2.93 avg=2.55\n",
            "[100 | 315.25] loss=2.38 avg=2.55\n",
            "[101 | 318.27] loss=2.82 avg=2.55\n",
            "[102 | 321.29] loss=2.13 avg=2.54\n",
            "[103 | 324.30] loss=2.41 avg=2.54\n",
            "[104 | 327.31] loss=2.36 avg=2.54\n",
            "[105 | 330.33] loss=2.58 avg=2.54\n",
            "[106 | 333.34] loss=1.76 avg=2.53\n",
            "[107 | 336.35] loss=2.82 avg=2.53\n",
            "[108 | 339.36] loss=2.56 avg=2.53\n",
            "[109 | 342.38] loss=2.85 avg=2.54\n",
            "[110 | 345.40] loss=2.27 avg=2.53\n",
            "[111 | 348.41] loss=2.22 avg=2.53\n",
            "[112 | 351.43] loss=2.33 avg=2.53\n",
            "[113 | 354.45] loss=2.39 avg=2.52\n",
            "[114 | 357.45] loss=2.65 avg=2.53\n",
            "[115 | 360.47] loss=2.30 avg=2.52\n",
            "[116 | 363.47] loss=2.37 avg=2.52\n",
            "[117 | 366.48] loss=2.48 avg=2.52\n",
            "[118 | 369.49] loss=2.10 avg=2.51\n",
            "[119 | 372.52] loss=2.35 avg=2.51\n",
            "[120 | 375.54] loss=2.47 avg=2.51\n",
            "[121 | 378.55] loss=2.89 avg=2.52\n",
            "[122 | 381.55] loss=2.25 avg=2.51\n",
            "[123 | 384.58] loss=2.61 avg=2.51\n",
            "[124 | 387.59] loss=2.73 avg=2.52\n",
            "[125 | 390.60] loss=2.34 avg=2.51\n",
            "[126 | 393.61] loss=2.50 avg=2.51\n",
            "[127 | 396.63] loss=2.29 avg=2.51\n",
            "[128 | 399.65] loss=2.19 avg=2.51\n",
            "[129 | 402.66] loss=2.75 avg=2.51\n",
            "[130 | 405.67] loss=2.49 avg=2.51\n",
            "[131 | 408.68] loss=2.89 avg=2.52\n",
            "[132 | 411.69] loss=2.44 avg=2.51\n",
            "[133 | 414.71] loss=2.78 avg=2.52\n",
            "[134 | 417.72] loss=2.95 avg=2.52\n",
            "[135 | 420.74] loss=2.95 avg=2.53\n",
            "[136 | 423.75] loss=2.56 avg=2.53\n",
            "[137 | 426.79] loss=2.26 avg=2.53\n",
            "[138 | 429.80] loss=2.61 avg=2.53\n",
            "[139 | 432.83] loss=2.30 avg=2.52\n",
            "[140 | 435.86] loss=3.12 avg=2.53\n",
            "[141 | 438.89] loss=2.21 avg=2.53\n",
            "[142 | 441.92] loss=2.62 avg=2.53\n",
            "[143 | 444.95] loss=2.60 avg=2.53\n",
            "[144 | 447.98] loss=2.51 avg=2.53\n",
            "[145 | 451.00] loss=1.83 avg=2.52\n",
            "[146 | 454.04] loss=2.79 avg=2.52\n",
            "[147 | 457.09] loss=2.30 avg=2.52\n",
            "[148 | 460.13] loss=2.19 avg=2.52\n",
            "[149 | 463.17] loss=3.04 avg=2.52\n",
            "[150 | 466.20] loss=2.44 avg=2.52\n",
            "[151 | 469.24] loss=2.77 avg=2.53\n",
            "[152 | 472.27] loss=2.50 avg=2.53\n",
            "[153 | 475.31] loss=2.40 avg=2.52\n",
            "[154 | 478.35] loss=2.58 avg=2.52\n",
            "[155 | 481.38] loss=2.40 avg=2.52\n",
            "[156 | 484.41] loss=2.76 avg=2.53\n",
            "[157 | 487.46] loss=2.63 avg=2.53\n",
            "[158 | 490.48] loss=2.63 avg=2.53\n",
            "[159 | 493.51] loss=2.50 avg=2.53\n",
            "[160 | 496.55] loss=2.70 avg=2.53\n",
            "[161 | 499.59] loss=2.21 avg=2.53\n",
            "[162 | 502.62] loss=1.85 avg=2.52\n",
            "[163 | 505.64] loss=2.82 avg=2.52\n",
            "[164 | 508.68] loss=2.40 avg=2.52\n",
            "[165 | 511.71] loss=2.88 avg=2.52\n",
            "[166 | 514.74] loss=2.17 avg=2.52\n",
            "[167 | 517.78] loss=2.85 avg=2.52\n",
            "[168 | 520.82] loss=2.92 avg=2.53\n",
            "[169 | 523.86] loss=2.33 avg=2.53\n",
            "[170 | 526.89] loss=2.21 avg=2.52\n",
            "[171 | 529.93] loss=2.47 avg=2.52\n",
            "[172 | 532.96] loss=2.77 avg=2.53\n",
            "[173 | 536.01] loss=2.69 avg=2.53\n",
            "[174 | 539.05] loss=2.30 avg=2.52\n",
            "[175 | 542.08] loss=2.60 avg=2.53\n",
            "[176 | 545.12] loss=2.36 avg=2.52\n",
            "[177 | 548.15] loss=3.05 avg=2.53\n",
            "[178 | 551.19] loss=2.60 avg=2.53\n",
            "[179 | 554.22] loss=2.51 avg=2.53\n",
            "[180 | 557.26] loss=2.39 avg=2.53\n",
            "[181 | 560.30] loss=2.43 avg=2.53\n",
            "[182 | 563.33] loss=1.91 avg=2.52\n",
            "[183 | 566.36] loss=2.35 avg=2.52\n",
            "[184 | 569.40] loss=2.44 avg=2.52\n",
            "[185 | 572.44] loss=2.70 avg=2.52\n",
            "[186 | 575.47] loss=2.64 avg=2.52\n",
            "[187 | 578.49] loss=2.56 avg=2.52\n",
            "[188 | 581.53] loss=2.27 avg=2.52\n",
            "[189 | 584.57] loss=2.44 avg=2.52\n",
            "[190 | 587.59] loss=2.67 avg=2.52\n",
            "[191 | 590.62] loss=2.37 avg=2.52\n",
            "[192 | 593.66] loss=2.79 avg=2.52\n",
            "[193 | 596.69] loss=2.60 avg=2.52\n",
            "[194 | 599.73] loss=2.59 avg=2.52\n",
            "[195 | 602.76] loss=2.42 avg=2.52\n",
            "[196 | 605.80] loss=2.09 avg=2.52\n",
            "[197 | 608.82] loss=2.26 avg=2.51\n",
            "[198 | 611.84] loss=2.29 avg=2.51\n",
            "[199 | 614.87] loss=2.21 avg=2.51\n",
            "[200 | 617.90] loss=2.40 avg=2.51\n",
            "[201 | 620.94] loss=3.11 avg=2.51\n",
            "[202 | 623.98] loss=2.31 avg=2.51\n",
            "[203 | 627.01] loss=2.32 avg=2.51\n",
            "[204 | 630.03] loss=2.32 avg=2.51\n",
            "[205 | 633.06] loss=2.67 avg=2.51\n",
            "[206 | 636.10] loss=2.24 avg=2.50\n",
            "[207 | 639.13] loss=2.02 avg=2.50\n",
            "[208 | 642.15] loss=2.73 avg=2.50\n",
            "[209 | 645.18] loss=2.52 avg=2.50\n",
            "[210 | 648.21] loss=2.11 avg=2.50\n",
            "[211 | 651.23] loss=2.54 avg=2.50\n",
            "[212 | 654.26] loss=1.78 avg=2.49\n",
            "[213 | 657.29] loss=2.53 avg=2.49\n",
            "[214 | 660.33] loss=2.73 avg=2.49\n",
            "[215 | 663.37] loss=3.33 avg=2.50\n",
            "[216 | 666.39] loss=2.10 avg=2.50\n",
            "[217 | 669.43] loss=2.71 avg=2.50\n",
            "[218 | 672.47] loss=1.91 avg=2.49\n",
            "[219 | 675.52] loss=2.36 avg=2.49\n",
            "[220 | 678.55] loss=2.14 avg=2.49\n",
            "[221 | 681.57] loss=2.29 avg=2.49\n",
            "[222 | 684.61] loss=2.51 avg=2.49\n",
            "[223 | 687.65] loss=1.94 avg=2.48\n",
            "[224 | 690.69] loss=2.52 avg=2.48\n",
            "[225 | 693.72] loss=2.86 avg=2.48\n",
            "[226 | 696.75] loss=2.19 avg=2.48\n",
            "[227 | 699.78] loss=2.11 avg=2.48\n",
            "[228 | 702.81] loss=2.81 avg=2.48\n",
            "[229 | 705.85] loss=2.35 avg=2.48\n",
            "[230 | 708.90] loss=2.20 avg=2.48\n",
            "[231 | 711.93] loss=2.46 avg=2.48\n",
            "[232 | 714.96] loss=2.64 avg=2.48\n",
            "[233 | 717.99] loss=2.49 avg=2.48\n",
            "[234 | 721.03] loss=2.55 avg=2.48\n",
            "[235 | 724.06] loss=2.39 avg=2.48\n",
            "[236 | 727.09] loss=2.16 avg=2.47\n",
            "[237 | 730.11] loss=2.06 avg=2.47\n",
            "[238 | 733.15] loss=2.31 avg=2.47\n",
            "[239 | 736.18] loss=2.52 avg=2.47\n",
            "[240 | 739.23] loss=2.41 avg=2.47\n",
            "[241 | 742.26] loss=2.81 avg=2.47\n",
            "[242 | 745.28] loss=1.93 avg=2.47\n",
            "[243 | 748.31] loss=2.22 avg=2.46\n",
            "[244 | 751.35] loss=2.55 avg=2.46\n",
            "[245 | 754.38] loss=2.40 avg=2.46\n",
            "[246 | 757.42] loss=2.48 avg=2.46\n",
            "[247 | 760.46] loss=2.72 avg=2.47\n",
            "[248 | 763.49] loss=2.22 avg=2.46\n",
            "[249 | 766.52] loss=2.43 avg=2.46\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "..... (I'm not sure how I should phrase it; it needs to start with a question mark or a dash.)\n",
            "(129572060) theking: no it's probably the wrong thing to say\n",
            "(129572060) theking: you said:\n",
            "(129572060) theking: can't you do that?\n",
            "(129572060) theking: it's not a keyword for the search engines.\n",
            "(129572420) theking: you can go to that website and say the term and it will search\n",
            "(129572480) theking: if I search for the term 'puppies' in it the search won't show up\n",
            "(129572480) theking: if I google 'taco' for 'puppies' i can see them\n",
            "(129572480) theking: when searching for a keyword it will search for the term\n",
            "(129572480) theking: i get the same results\n",
            "(129572480) theking: i can't type it correctly (it doesn't seem to work\n",
            "(1295723840) theking: i try to add 'puppies' to a search for 'taco' but it doesn't work\n",
            "(129572440) paul_: that's the reason i couldn't read it: you're not in your native language.\n",
            "(129572440) theking: yeah. the problem is that you can't type to say 'taco'\n",
            "(129572440) theking: it doesn't seem to work\n",
            "(1295724640) theking: i can't click the link.\n",
            "(1295725660) theking: you'll have to google for it\n",
            "\n",
            "\n",
            "(132181280) pep: are the diferences of linux distros different between windows and mac...how can i install linux from ubuntu?\n",
            "(1321812340) shaun: ubuntu is available as a windows install, linux from ubuntu is not\n",
            "(132181460) shaun: linux is also available for linux-gnome from a separate installer, you do not have to install linux first.\n",
            "(1321815280) shaun: i have no way to install linux from ubuntu, because i have not installed linux before on my localhost...it was done on windows and the linux was installed.\n",
            "(132181460) shaun: and when i install linux on my localhost, it will tell me my distro: ubuntu, linux from ubuntu.\n",
            "\n",
            "\n",
            "(137240340) sarto: I can't find the url to download the live cd.\n",
            "(137240340) sarto: My ISP wont allow me to download the live cd\n",
            "(137240400) hhgthunder: what do you mean\n",
            "(137240400) sarto: I can't find the url to download the live cd.\n",
            "(137240400) sarto: My ISP wont allow me to download the live cd\n",
            "(137240460) sarto: I can't find the url to download the live cd.\n",
            "(137240540) sarto: My ISP wont allow me to download the live cd\n",
            "(137240540) sarto: i can't find the url to download the live cd.\n",
            "(137240540) sarto: my ISP wont allow me to download the live cd.\n",
            "\n",
            "\n",
            "(1140231240) tlsh: how do i delete the root cd? or its cdimage-root\n",
            "(114023130) sarto: You do it in grub or grub-efi\n",
            "(114023130) sarto: I will see when to check the system...\n",
            "(114023150) tlsh: can i read the root.img file\n",
            "(1140231540) sarto: I have a good idea here. You should have the root cd, but when using the gdm interface, do you see the file? How do you find it?\n",
            "(1140231640) sarto: I do not know how\n",
            "(1140231700) tlsh: i tried to reinstall from the live cd but it failed\n",
            "(1140231800) sarto: You don't. If you do it in a terminal, do you have a new root? Why not?\n",
            "(1140231980) sarto: but after installing, do you start at boot or at /?   \n",
            "(1140231980) sarto: no error\n",
            "(1140232000) sarto: so, why not start. (boot, at boot? is that not possible?)\n",
            "(1140239100) zeroman\n",
            "\n",
            "[250 | 810.09] loss=3.12 avg=2.47\n",
            "[251 | 813.11] loss=2.63 avg=2.47\n",
            "[252 | 816.13] loss=2.90 avg=2.48\n",
            "[253 | 819.15] loss=2.38 avg=2.48\n",
            "[254 | 822.16] loss=2.51 avg=2.48\n",
            "[255 | 825.18] loss=2.25 avg=2.47\n",
            "[256 | 828.20] loss=2.40 avg=2.47\n",
            "[257 | 831.21] loss=2.80 avg=2.48\n",
            "[258 | 834.24] loss=2.98 avg=2.48\n",
            "[259 | 837.27] loss=2.50 avg=2.48\n",
            "[260 | 840.29] loss=2.62 avg=2.48\n",
            "[261 | 843.32] loss=2.33 avg=2.48\n",
            "[262 | 846.35] loss=2.46 avg=2.48\n",
            "[263 | 849.39] loss=2.12 avg=2.48\n",
            "[264 | 852.42] loss=2.88 avg=2.48\n",
            "[265 | 855.44] loss=2.13 avg=2.48\n",
            "[266 | 858.48] loss=2.76 avg=2.48\n",
            "[267 | 861.51] loss=2.79 avg=2.48\n",
            "[268 | 864.54] loss=2.74 avg=2.49\n",
            "[269 | 867.57] loss=2.28 avg=2.49\n",
            "[270 | 870.59] loss=2.26 avg=2.48\n",
            "[271 | 873.61] loss=2.73 avg=2.49\n",
            "[272 | 876.64] loss=1.98 avg=2.48\n",
            "[273 | 879.67] loss=2.70 avg=2.48\n",
            "[274 | 882.70] loss=2.53 avg=2.48\n",
            "[275 | 885.73] loss=2.56 avg=2.48\n",
            "[276 | 888.76] loss=3.00 avg=2.49\n",
            "[277 | 891.79] loss=2.93 avg=2.49\n",
            "[278 | 894.81] loss=2.74 avg=2.50\n",
            "[279 | 897.85] loss=2.77 avg=2.50\n",
            "[280 | 900.88] loss=2.19 avg=2.50\n",
            "[281 | 903.93] loss=2.25 avg=2.49\n",
            "[282 | 906.96] loss=2.67 avg=2.50\n",
            "[283 | 909.99] loss=2.54 avg=2.50\n",
            "[284 | 913.02] loss=2.33 avg=2.49\n",
            "[285 | 916.05] loss=2.80 avg=2.50\n",
            "[286 | 919.08] loss=2.86 avg=2.50\n",
            "[287 | 922.13] loss=2.40 avg=2.50\n",
            "[288 | 925.16] loss=2.53 avg=2.50\n",
            "[289 | 928.20] loss=2.44 avg=2.50\n",
            "[290 | 931.23] loss=2.85 avg=2.50\n",
            "[291 | 934.27] loss=2.31 avg=2.50\n",
            "[292 | 937.32] loss=1.83 avg=2.49\n",
            "[293 | 940.35] loss=2.88 avg=2.50\n",
            "[294 | 943.38] loss=2.92 avg=2.50\n",
            "[295 | 946.42] loss=2.60 avg=2.50\n",
            "[296 | 949.46] loss=2.36 avg=2.50\n",
            "[297 | 952.50] loss=2.35 avg=2.50\n",
            "[298 | 955.54] loss=2.50 avg=2.50\n",
            "[299 | 958.56] loss=2.18 avg=2.50\n",
            "[300 | 961.59] loss=2.45 avg=2.50\n",
            "[301 | 964.63] loss=2.07 avg=2.49\n",
            "[302 | 967.65] loss=2.82 avg=2.50\n",
            "[303 | 970.70] loss=2.12 avg=2.49\n",
            "[304 | 973.73] loss=1.70 avg=2.48\n",
            "[305 | 976.76] loss=2.30 avg=2.48\n",
            "[306 | 979.80] loss=2.24 avg=2.48\n",
            "[307 | 982.83] loss=2.15 avg=2.48\n",
            "[308 | 985.86] loss=2.91 avg=2.48\n",
            "[309 | 988.88] loss=2.41 avg=2.48\n",
            "[310 | 991.92] loss=2.34 avg=2.48\n",
            "[311 | 994.96] loss=2.22 avg=2.48\n",
            "[312 | 997.98] loss=2.64 avg=2.48\n",
            "[313 | 1001.01] loss=2.29 avg=2.48\n",
            "[314 | 1004.04] loss=2.02 avg=2.47\n",
            "[315 | 1007.06] loss=3.01 avg=2.48\n",
            "[316 | 1010.10] loss=2.40 avg=2.48\n",
            "[317 | 1013.14] loss=2.57 avg=2.48\n",
            "[318 | 1016.18] loss=2.53 avg=2.48\n",
            "[319 | 1019.21] loss=2.38 avg=2.48\n",
            "[320 | 1022.22] loss=2.16 avg=2.47\n",
            "[321 | 1025.24] loss=2.94 avg=2.48\n",
            "[322 | 1028.26] loss=2.43 avg=2.48\n",
            "[323 | 1031.27] loss=2.78 avg=2.48\n",
            "[324 | 1034.29] loss=2.56 avg=2.48\n",
            "[325 | 1037.30] loss=2.52 avg=2.48\n",
            "[326 | 1040.30] loss=2.45 avg=2.48\n",
            "[327 | 1043.31] loss=2.28 avg=2.48\n",
            "[328 | 1046.33] loss=2.60 avg=2.48\n",
            "[329 | 1049.37] loss=2.55 avg=2.48\n",
            "[330 | 1052.39] loss=2.92 avg=2.49\n",
            "[331 | 1055.43] loss=2.43 avg=2.49\n",
            "[332 | 1058.44] loss=1.98 avg=2.48\n",
            "[333 | 1061.47] loss=2.82 avg=2.48\n",
            "[334 | 1064.49] loss=1.83 avg=2.48\n",
            "[335 | 1067.51] loss=2.22 avg=2.47\n",
            "[336 | 1070.52] loss=2.36 avg=2.47\n",
            "[337 | 1073.53] loss=2.93 avg=2.48\n",
            "[338 | 1076.55] loss=2.13 avg=2.47\n",
            "[339 | 1079.57] loss=2.33 avg=2.47\n",
            "[340 | 1082.61] loss=2.34 avg=2.47\n",
            "[341 | 1085.64] loss=2.38 avg=2.47\n",
            "[342 | 1088.66] loss=2.17 avg=2.47\n",
            "[343 | 1091.68] loss=2.81 avg=2.47\n",
            "[344 | 1094.72] loss=2.51 avg=2.47\n",
            "[345 | 1097.75] loss=2.35 avg=2.47\n",
            "[346 | 1100.76] loss=2.72 avg=2.47\n",
            "[347 | 1103.77] loss=1.95 avg=2.47\n",
            "[348 | 1106.78] loss=2.51 avg=2.47\n",
            "[349 | 1109.79] loss=2.68 avg=2.47\n",
            "[350 | 1112.80] loss=2.18 avg=2.47\n",
            "[351 | 1115.80] loss=2.41 avg=2.47\n",
            "[352 | 1118.80] loss=2.00 avg=2.46\n",
            "[353 | 1121.80] loss=2.31 avg=2.46\n",
            "[354 | 1124.81] loss=2.76 avg=2.46\n",
            "[355 | 1127.85] loss=2.26 avg=2.46\n",
            "[356 | 1130.88] loss=2.58 avg=2.46\n",
            "[357 | 1133.89] loss=2.61 avg=2.46\n",
            "[358 | 1136.92] loss=2.31 avg=2.46\n",
            "[359 | 1139.95] loss=2.39 avg=2.46\n",
            "[360 | 1142.98] loss=2.22 avg=2.46\n",
            "[361 | 1146.02] loss=1.95 avg=2.45\n",
            "[362 | 1149.04] loss=2.37 avg=2.45\n",
            "[363 | 1152.05] loss=2.28 avg=2.45\n",
            "[364 | 1155.06] loss=2.89 avg=2.46\n",
            "[365 | 1158.07] loss=2.63 avg=2.46\n",
            "[366 | 1161.08] loss=2.62 avg=2.46\n",
            "[367 | 1164.10] loss=2.34 avg=2.46\n",
            "[368 | 1167.11] loss=2.41 avg=2.46\n",
            "[369 | 1170.12] loss=2.21 avg=2.45\n",
            "[370 | 1173.14] loss=3.01 avg=2.46\n",
            "[371 | 1176.17] loss=2.22 avg=2.46\n",
            "[372 | 1179.19] loss=2.52 avg=2.46\n",
            "[373 | 1182.20] loss=2.30 avg=2.46\n",
            "[374 | 1185.21] loss=2.84 avg=2.46\n",
            "[375 | 1188.23] loss=2.86 avg=2.46\n",
            "[376 | 1191.26] loss=2.59 avg=2.47\n",
            "[377 | 1194.30] loss=2.43 avg=2.47\n",
            "[378 | 1197.33] loss=2.86 avg=2.47\n",
            "[379 | 1200.35] loss=2.26 avg=2.47\n",
            "[380 | 1203.36] loss=2.29 avg=2.47\n",
            "[381 | 1206.39] loss=2.49 avg=2.47\n",
            "[382 | 1209.43] loss=2.92 avg=2.47\n",
            "[383 | 1212.45] loss=2.21 avg=2.47\n",
            "[384 | 1215.49] loss=3.03 avg=2.47\n",
            "[385 | 1218.52] loss=2.38 avg=2.47\n",
            "[386 | 1221.54] loss=3.05 avg=2.48\n",
            "[387 | 1224.55] loss=2.52 avg=2.48\n",
            "[388 | 1227.57] loss=2.84 avg=2.48\n",
            "[389 | 1230.61] loss=2.75 avg=2.49\n",
            "[390 | 1233.64] loss=2.63 avg=2.49\n",
            "[391 | 1236.68] loss=2.78 avg=2.49\n",
            "[392 | 1239.72] loss=2.21 avg=2.49\n",
            "[393 | 1242.76] loss=2.35 avg=2.49\n",
            "[394 | 1245.79] loss=2.71 avg=2.49\n",
            "[395 | 1248.82] loss=2.42 avg=2.49\n",
            "[396 | 1251.85] loss=2.07 avg=2.48\n",
            "[397 | 1254.88] loss=2.30 avg=2.48\n",
            "[398 | 1257.92] loss=2.60 avg=2.48\n",
            "[399 | 1260.95] loss=2.53 avg=2.48\n",
            "[400 | 1263.98] loss=2.53 avg=2.48\n",
            "[401 | 1267.00] loss=2.11 avg=2.48\n",
            "[402 | 1270.01] loss=2.53 avg=2.48\n",
            "[403 | 1273.05] loss=2.76 avg=2.48\n",
            "[404 | 1276.08] loss=2.80 avg=2.49\n",
            "[405 | 1279.11] loss=2.12 avg=2.48\n",
            "[406 | 1282.15] loss=2.11 avg=2.48\n",
            "[407 | 1285.17] loss=2.21 avg=2.48\n",
            "[408 | 1288.20] loss=2.44 avg=2.48\n",
            "[409 | 1291.21] loss=2.34 avg=2.47\n",
            "[410 | 1294.23] loss=2.32 avg=2.47\n",
            "[411 | 1297.25] loss=2.56 avg=2.47\n",
            "[412 | 1300.29] loss=2.74 avg=2.48\n",
            "[413 | 1303.31] loss=2.89 avg=2.48\n",
            "[414 | 1306.33] loss=2.50 avg=2.48\n",
            "[415 | 1309.36] loss=1.91 avg=2.47\n",
            "[416 | 1312.40] loss=2.16 avg=2.47\n",
            "[417 | 1315.43] loss=2.57 avg=2.47\n",
            "[418 | 1318.45] loss=2.11 avg=2.47\n",
            "[419 | 1321.46] loss=2.07 avg=2.46\n",
            "[420 | 1324.47] loss=2.39 avg=2.46\n",
            "[421 | 1327.49] loss=2.09 avg=2.46\n",
            "[422 | 1330.53] loss=2.62 avg=2.46\n",
            "[423 | 1333.55] loss=2.52 avg=2.46\n",
            "[424 | 1336.58] loss=2.48 avg=2.46\n",
            "[425 | 1339.60] loss=2.87 avg=2.47\n",
            "[426 | 1342.64] loss=2.85 avg=2.47\n",
            "[427 | 1345.67] loss=2.08 avg=2.47\n",
            "[428 | 1348.69] loss=2.39 avg=2.47\n",
            "[429 | 1351.72] loss=2.48 avg=2.47\n",
            "[430 | 1354.75] loss=2.45 avg=2.47\n",
            "[431 | 1357.79] loss=2.43 avg=2.47\n",
            "[432 | 1360.82] loss=2.48 avg=2.47\n",
            "[433 | 1363.84] loss=2.56 avg=2.47\n",
            "[434 | 1366.87] loss=2.70 avg=2.47\n",
            "[435 | 1369.91] loss=2.68 avg=2.47\n",
            "[436 | 1372.94] loss=2.41 avg=2.47\n",
            "[437 | 1375.96] loss=2.32 avg=2.47\n",
            "[438 | 1378.99] loss=2.86 avg=2.47\n",
            "[439 | 1382.03] loss=2.59 avg=2.47\n",
            "[440 | 1385.07] loss=2.74 avg=2.48\n",
            "[441 | 1388.10] loss=2.37 avg=2.48\n",
            "[442 | 1391.13] loss=2.30 avg=2.47\n",
            "[443 | 1394.16] loss=2.27 avg=2.47\n",
            "[444 | 1397.20] loss=2.07 avg=2.47\n",
            "[445 | 1400.24] loss=2.51 avg=2.47\n",
            "[446 | 1403.26] loss=2.35 avg=2.47\n",
            "[447 | 1406.30] loss=2.33 avg=2.47\n",
            "[448 | 1409.34] loss=2.58 avg=2.47\n",
            "[449 | 1412.38] loss=2.46 avg=2.47\n",
            "[450 | 1415.40] loss=2.39 avg=2.47\n",
            "[451 | 1418.43] loss=2.65 avg=2.47\n",
            "[452 | 1421.46] loss=2.54 avg=2.47\n",
            "[453 | 1424.50] loss=2.36 avg=2.47\n",
            "[454 | 1427.54] loss=2.14 avg=2.46\n",
            "[455 | 1430.57] loss=1.96 avg=2.46\n",
            "[456 | 1433.60] loss=2.78 avg=2.46\n",
            "[457 | 1436.62] loss=2.69 avg=2.46\n",
            "[458 | 1439.65] loss=2.72 avg=2.47\n",
            "[459 | 1442.67] loss=2.07 avg=2.46\n",
            "[460 | 1445.70] loss=1.82 avg=2.46\n",
            "[461 | 1448.74] loss=2.29 avg=2.45\n",
            "[462 | 1451.78] loss=2.90 avg=2.46\n",
            "[463 | 1454.81] loss=2.38 avg=2.46\n",
            "[464 | 1457.84] loss=2.64 avg=2.46\n",
            "[465 | 1460.86] loss=2.46 avg=2.46\n",
            "[466 | 1463.89] loss=2.68 avg=2.46\n",
            "[467 | 1466.93] loss=2.12 avg=2.46\n",
            "[468 | 1469.94] loss=2.58 avg=2.46\n",
            "[469 | 1472.97] loss=2.92 avg=2.46\n",
            "[470 | 1475.99] loss=2.50 avg=2.47\n",
            "[471 | 1479.02] loss=2.42 avg=2.46\n",
            "[472 | 1482.03] loss=2.46 avg=2.46\n",
            "[473 | 1485.06] loss=2.01 avg=2.46\n",
            "[474 | 1488.09] loss=2.07 avg=2.46\n",
            "[475 | 1491.10] loss=2.15 avg=2.45\n",
            "[476 | 1494.14] loss=2.95 avg=2.46\n",
            "[477 | 1497.16] loss=2.23 avg=2.46\n",
            "[478 | 1500.19] loss=2.24 avg=2.45\n",
            "[479 | 1503.21] loss=2.71 avg=2.46\n",
            "[480 | 1506.23] loss=2.04 avg=2.45\n",
            "[481 | 1509.24] loss=2.29 avg=2.45\n",
            "[482 | 1512.27] loss=2.52 avg=2.45\n",
            "[483 | 1515.30] loss=2.18 avg=2.45\n",
            "[484 | 1518.33] loss=2.80 avg=2.45\n",
            "[485 | 1521.37] loss=2.30 avg=2.45\n",
            "[486 | 1524.40] loss=2.26 avg=2.45\n",
            "[487 | 1527.45] loss=2.42 avg=2.45\n",
            "[488 | 1530.50] loss=2.49 avg=2.45\n",
            "[489 | 1533.55] loss=2.67 avg=2.45\n",
            "[490 | 1536.60] loss=2.37 avg=2.45\n",
            "[491 | 1539.65] loss=2.16 avg=2.45\n",
            "[492 | 1542.69] loss=2.33 avg=2.45\n",
            "[493 | 1545.74] loss=2.23 avg=2.44\n",
            "[494 | 1548.79] loss=2.23 avg=2.44\n",
            "[495 | 1551.84] loss=2.66 avg=2.44\n",
            "[496 | 1554.89] loss=2.52 avg=2.44\n",
            "[497 | 1557.94] loss=2.54 avg=2.45\n",
            "[498 | 1560.98] loss=2.01 avg=2.44\n",
            "[499 | 1564.02] loss=2.63 avg=2.44\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " hate...but i would rather not read a book that takes too long to start...if you are at home doing nothing, then im not here\n",
            "(1258695080) _Szabo: :)\n",
            "(1255955040) _Szabo: i wish they would only have the right to read...i would buy the book at a store\n",
            "(1255955040) _Szabo: i feel guilty asking for a question i do not even know the answer to, but i don't want to ruin it...\n",
            "\n",
            "\n",
            "(1293058000) bdub:  http://mibbit.com/download/ubuntu-9-10.04-amd64x64-i386.dmg\n",
            "(12930580020) ddub: http://mibbit.com/download/ubuntu-9-10.04-amd64x64.iso.iso\n",
            "(12930580080) ddub:  http://mibbit.com/download/ubuntu-9-10.04-i386.iso.iso\n",
            "(12930586080) ddub:  http://mibbit.com/download/ubuntu-9-10.04-x86_64-i386.iso.iso\n",
            "(12930586080) ddub:  http://mibbit.com/download/ubuntu-9-10.04-x86_64-vista-amd64.iso.iso\n",
            "(1293058680) ddub:  http://mibbit.com/Downloads/ubuntu-9-10.04-i386-a386.iso.iso\n",
            "(1293058780)\n",
            "(1293057140) ddub: what is the ubuntu image on the link?\n",
            "(1293057140) ddub: i have just installed ubuntu on my computer\n",
            "(1293057300) ddub: you say its not bootable, but what about the system.\n",
            "(1293057300) ddub: when i boot you say that no image is presented, what should i do\n",
            "(1293057300) ddub: i think i've found the problem\n",
            "(1293057360) ddub: sorry i dont know the solution\n",
            "(1293057480) ddub: i used flash and it works and is still a bootable image\n",
            "(1293057480) ddub: but the problem is the system, im not sure if it has to boot, but then i cant run flash...\n",
            "(1293057480) dg: what kernel do you have?\n",
            "(1293057480) ddub: i just went to the command line\n",
            "(1293057480) ddub: i used it wich showed in the last two weeks\n",
            "(1293057240) ddub: i used it wich showed in the last two weeks\n",
            "(1293057360) dg: what is it on the links?\n",
            "\n",
            "\n",
            "(1123183620) mjdcd: what is the difference of kde and kde-w\n",
            "(1123183620) mjdcd: !arch | mjdcd\n",
            "(1123183620) mjdcd: !kde | mjdcd\n",
            "(1123183620) bt: i think kde was the main target here, but i dunno\n",
            "(1123183620) bt: yes thats one reason why I like kde more\n",
            "(1123183680) bt: kde has got better features...so thats my answer\n",
            "(1123183740) bt: you did a google search for \"how to switch windows to kde\" and found it.\n",
            "(1123183740) bt: i am not sure where it was so much as in your post...\n",
            "(1123183800) bt: i had to run 'sudo cp -p3 /dev/partition /swap' to get it back\n",
            "(1123183800) bt: yes, i made one mistake\n",
            "(1123183800) bob.\n",
            "(1123183800) bt: how?\n",
            "\n",
            "\n",
            "(1310980120) rdw1: anyone know how to mount it?\n",
            "(1310980120) n0x: how to mount it ?\n",
            "(1310980120) rdw1: its really complicated\n",
            "(1310980120) rdw1: mount    / /mnt (mounting it is all about the files on your drive (others of us know that is not the same as the windows drive).\n",
            "(1310980120) rdw1: mount   / /swap\n",
            "(1310980120) n0x:\n",
            "\n",
            "[500 | 1603.57] loss=2.93 avg=2.45\n",
            "[501 | 1606.62] loss=2.26 avg=2.45\n",
            "[502 | 1609.66] loss=2.31 avg=2.44\n",
            "[503 | 1612.71] loss=2.60 avg=2.45\n",
            "[504 | 1615.76] loss=2.22 avg=2.44\n",
            "[505 | 1618.79] loss=2.27 avg=2.44\n",
            "[506 | 1621.85] loss=2.16 avg=2.44\n",
            "[507 | 1624.90] loss=2.80 avg=2.44\n",
            "[508 | 1627.95] loss=2.41 avg=2.44\n",
            "[509 | 1631.00] loss=2.45 avg=2.44\n",
            "[510 | 1634.05] loss=2.47 avg=2.44\n",
            "[511 | 1637.09] loss=2.41 avg=2.44\n",
            "[512 | 1640.14] loss=2.41 avg=2.44\n",
            "[513 | 1643.19] loss=2.42 avg=2.44\n",
            "[514 | 1646.24] loss=1.98 avg=2.44\n",
            "[515 | 1649.28] loss=2.59 avg=2.44\n",
            "[516 | 1652.34] loss=2.44 avg=2.44\n",
            "[517 | 1655.39] loss=2.30 avg=2.44\n",
            "[518 | 1658.43] loss=1.99 avg=2.43\n",
            "[519 | 1661.49] loss=2.31 avg=2.43\n",
            "[520 | 1664.53] loss=1.91 avg=2.43\n",
            "[521 | 1667.58] loss=2.24 avg=2.42\n",
            "[522 | 1670.63] loss=2.68 avg=2.43\n",
            "[523 | 1673.68] loss=2.21 avg=2.42\n",
            "[524 | 1676.74] loss=2.59 avg=2.43\n",
            "[525 | 1679.79] loss=2.60 avg=2.43\n",
            "[526 | 1682.83] loss=2.85 avg=2.43\n",
            "[527 | 1685.88] loss=2.83 avg=2.44\n",
            "[528 | 1688.93] loss=2.19 avg=2.43\n",
            "[529 | 1691.98] loss=2.07 avg=2.43\n",
            "[530 | 1695.03] loss=2.49 avg=2.43\n",
            "[531 | 1698.08] loss=3.01 avg=2.44\n",
            "[532 | 1701.12] loss=2.27 avg=2.44\n",
            "[533 | 1704.18] loss=2.80 avg=2.44\n",
            "[534 | 1707.22] loss=2.39 avg=2.44\n",
            "[535 | 1710.27] loss=2.38 avg=2.44\n",
            "[536 | 1713.31] loss=2.63 avg=2.44\n",
            "[537 | 1716.36] loss=2.56 avg=2.44\n",
            "[538 | 1719.40] loss=2.86 avg=2.45\n",
            "[539 | 1722.43] loss=2.12 avg=2.44\n",
            "[540 | 1725.48] loss=2.41 avg=2.44\n",
            "[541 | 1728.54] loss=2.30 avg=2.44\n",
            "[542 | 1731.58] loss=2.41 avg=2.44\n",
            "[543 | 1734.62] loss=1.88 avg=2.43\n",
            "[544 | 1737.67] loss=2.08 avg=2.43\n",
            "[545 | 1740.73] loss=2.14 avg=2.43\n",
            "[546 | 1743.77] loss=2.77 avg=2.43\n",
            "[547 | 1746.82] loss=2.53 avg=2.43\n",
            "[548 | 1749.86] loss=2.21 avg=2.43\n",
            "[549 | 1752.90] loss=2.41 avg=2.43\n",
            "[550 | 1755.96] loss=2.45 avg=2.43\n",
            "[551 | 1759.00] loss=2.43 avg=2.43\n",
            "[552 | 1762.06] loss=1.99 avg=2.43\n",
            "[553 | 1765.10] loss=2.37 avg=2.42\n",
            "[554 | 1768.14] loss=2.79 avg=2.43\n",
            "[555 | 1771.19] loss=2.57 avg=2.43\n",
            "[556 | 1774.26] loss=2.34 avg=2.43\n",
            "[557 | 1777.31] loss=2.26 avg=2.43\n",
            "[558 | 1780.36] loss=2.05 avg=2.42\n",
            "[559 | 1783.40] loss=2.30 avg=2.42\n",
            "[560 | 1786.45] loss=1.94 avg=2.42\n",
            "[561 | 1789.50] loss=2.60 avg=2.42\n",
            "[562 | 1792.55] loss=2.28 avg=2.42\n",
            "[563 | 1795.60] loss=2.52 avg=2.42\n",
            "[564 | 1798.64] loss=2.28 avg=2.42\n",
            "[565 | 1801.69] loss=2.80 avg=2.42\n",
            "[566 | 1804.74] loss=2.26 avg=2.42\n",
            "[567 | 1807.80] loss=2.04 avg=2.42\n",
            "[568 | 1810.86] loss=2.47 avg=2.42\n",
            "[569 | 1813.90] loss=2.36 avg=2.42\n",
            "[570 | 1816.96] loss=2.71 avg=2.42\n",
            "[571 | 1820.00] loss=2.31 avg=2.42\n",
            "[572 | 1823.05] loss=2.37 avg=2.42\n",
            "[573 | 1826.11] loss=2.47 avg=2.42\n",
            "[574 | 1829.16] loss=2.59 avg=2.42\n",
            "[575 | 1832.21] loss=2.22 avg=2.42\n",
            "[576 | 1835.25] loss=1.89 avg=2.41\n",
            "[577 | 1838.30] loss=2.23 avg=2.41\n",
            "[578 | 1841.36] loss=2.67 avg=2.41\n",
            "[579 | 1844.41] loss=2.46 avg=2.41\n",
            "[580 | 1847.45] loss=2.23 avg=2.41\n",
            "[581 | 1850.50] loss=2.89 avg=2.42\n",
            "[582 | 1853.55] loss=2.25 avg=2.41\n",
            "[583 | 1856.60] loss=2.27 avg=2.41\n",
            "[584 | 1859.65] loss=2.41 avg=2.41\n",
            "[585 | 1862.69] loss=2.41 avg=2.41\n",
            "[586 | 1865.75] loss=2.05 avg=2.41\n",
            "[587 | 1868.80] loss=2.09 avg=2.41\n",
            "[588 | 1871.85] loss=2.38 avg=2.41\n",
            "[589 | 1874.91] loss=2.53 avg=2.41\n",
            "[590 | 1877.96] loss=2.44 avg=2.41\n",
            "[591 | 1881.00] loss=2.36 avg=2.41\n",
            "[592 | 1884.06] loss=2.26 avg=2.41\n",
            "[593 | 1887.10] loss=2.39 avg=2.41\n",
            "[594 | 1890.15] loss=2.71 avg=2.41\n",
            "[595 | 1893.21] loss=2.06 avg=2.40\n",
            "[596 | 1896.25] loss=2.42 avg=2.41\n",
            "[597 | 1899.30] loss=2.54 avg=2.41\n",
            "[598 | 1902.36] loss=2.36 avg=2.41\n",
            "[599 | 1905.40] loss=2.26 avg=2.40\n",
            "[600 | 1908.44] loss=2.28 avg=2.40\n",
            "[601 | 1911.50] loss=2.39 avg=2.40\n",
            "[602 | 1914.56] loss=2.18 avg=2.40\n",
            "[603 | 1917.62] loss=2.92 avg=2.41\n",
            "[604 | 1920.66] loss=2.30 avg=2.40\n",
            "[605 | 1923.71] loss=2.76 avg=2.41\n",
            "[606 | 1926.76] loss=1.93 avg=2.40\n",
            "[607 | 1929.80] loss=2.59 avg=2.41\n",
            "[608 | 1932.85] loss=2.86 avg=2.41\n",
            "[609 | 1935.91] loss=2.59 avg=2.41\n",
            "[610 | 1938.95] loss=3.11 avg=2.42\n",
            "[611 | 1941.99] loss=2.12 avg=2.42\n",
            "[612 | 1945.03] loss=2.45 avg=2.42\n",
            "[613 | 1948.07] loss=2.60 avg=2.42\n",
            "[614 | 1951.12] loss=3.10 avg=2.42\n",
            "[615 | 1954.17] loss=2.38 avg=2.42\n",
            "[616 | 1957.21] loss=2.48 avg=2.42\n",
            "[617 | 1960.26] loss=2.06 avg=2.42\n",
            "[618 | 1963.30] loss=2.74 avg=2.42\n",
            "[619 | 1966.35] loss=1.81 avg=2.42\n",
            "[620 | 1969.41] loss=2.39 avg=2.42\n",
            "[621 | 1972.44] loss=2.51 avg=2.42\n",
            "[622 | 1975.50] loss=2.94 avg=2.42\n",
            "[623 | 1978.55] loss=2.12 avg=2.42\n",
            "[624 | 1981.60] loss=2.37 avg=2.42\n",
            "[625 | 1984.64] loss=2.10 avg=2.42\n",
            "[626 | 1987.70] loss=2.33 avg=2.42\n",
            "[627 | 1990.75] loss=2.30 avg=2.42\n",
            "[628 | 1993.79] loss=2.36 avg=2.41\n",
            "[629 | 1996.84] loss=2.34 avg=2.41\n",
            "[630 | 1999.90] loss=2.46 avg=2.41\n",
            "[631 | 2002.94] loss=2.63 avg=2.42\n",
            "[632 | 2005.99] loss=2.57 avg=2.42\n",
            "[633 | 2009.05] loss=2.08 avg=2.41\n",
            "[634 | 2012.09] loss=2.46 avg=2.42\n",
            "[635 | 2015.13] loss=2.28 avg=2.41\n",
            "[636 | 2018.18] loss=2.58 avg=2.42\n",
            "[637 | 2021.23] loss=2.07 avg=2.41\n",
            "[638 | 2024.28] loss=2.76 avg=2.42\n",
            "[639 | 2027.33] loss=2.46 avg=2.42\n",
            "[640 | 2030.37] loss=2.13 avg=2.41\n",
            "[641 | 2033.42] loss=2.36 avg=2.41\n",
            "[642 | 2036.47] loss=2.23 avg=2.41\n",
            "[643 | 2039.50] loss=2.80 avg=2.41\n",
            "[644 | 2042.54] loss=2.71 avg=2.42\n",
            "[645 | 2045.59] loss=2.60 avg=2.42\n",
            "[646 | 2048.62] loss=2.04 avg=2.42\n",
            "[647 | 2051.65] loss=2.55 avg=2.42\n",
            "[648 | 2054.69] loss=1.99 avg=2.41\n",
            "[649 | 2057.73] loss=2.21 avg=2.41\n",
            "[650 | 2060.77] loss=2.81 avg=2.41\n",
            "[651 | 2063.81] loss=2.50 avg=2.42\n",
            "[652 | 2066.84] loss=2.27 avg=2.41\n",
            "[653 | 2069.88] loss=2.44 avg=2.41\n",
            "[654 | 2072.93] loss=2.78 avg=2.42\n",
            "[655 | 2075.97] loss=2.57 avg=2.42\n",
            "[656 | 2079.01] loss=2.21 avg=2.42\n",
            "[657 | 2082.06] loss=2.25 avg=2.42\n",
            "[658 | 2085.12] loss=2.29 avg=2.41\n",
            "[659 | 2088.15] loss=2.29 avg=2.41\n",
            "[660 | 2091.18] loss=2.25 avg=2.41\n",
            "[661 | 2094.21] loss=2.42 avg=2.41\n",
            "[662 | 2097.25] loss=2.43 avg=2.41\n",
            "[663 | 2100.28] loss=2.41 avg=2.41\n",
            "[664 | 2103.30] loss=2.40 avg=2.41\n",
            "[665 | 2106.35] loss=2.96 avg=2.42\n",
            "[666 | 2109.38] loss=2.00 avg=2.41\n",
            "[667 | 2112.43] loss=2.62 avg=2.41\n",
            "[668 | 2115.47] loss=2.06 avg=2.41\n",
            "[669 | 2118.51] loss=2.41 avg=2.41\n",
            "[670 | 2121.55] loss=2.68 avg=2.41\n",
            "[671 | 2124.59] loss=2.37 avg=2.41\n",
            "[672 | 2127.62] loss=2.55 avg=2.41\n",
            "[673 | 2130.67] loss=2.18 avg=2.41\n",
            "[674 | 2133.71] loss=2.83 avg=2.42\n",
            "[675 | 2136.74] loss=2.19 avg=2.41\n",
            "[676 | 2139.78] loss=2.84 avg=2.42\n",
            "[677 | 2142.83] loss=2.14 avg=2.42\n",
            "[678 | 2145.88] loss=2.95 avg=2.42\n",
            "[679 | 2148.92] loss=2.78 avg=2.42\n",
            "[680 | 2151.97] loss=2.65 avg=2.43\n",
            "[681 | 2155.01] loss=2.74 avg=2.43\n",
            "[682 | 2158.06] loss=2.44 avg=2.43\n",
            "[683 | 2161.11] loss=1.98 avg=2.43\n",
            "[684 | 2164.15] loss=2.25 avg=2.42\n",
            "[685 | 2167.20] loss=2.30 avg=2.42\n",
            "[686 | 2170.25] loss=2.74 avg=2.43\n",
            "[687 | 2173.31] loss=2.73 avg=2.43\n",
            "[688 | 2176.36] loss=2.57 avg=2.43\n",
            "[689 | 2179.41] loss=2.64 avg=2.43\n",
            "[690 | 2182.46] loss=2.63 avg=2.43\n",
            "[691 | 2185.50] loss=2.20 avg=2.43\n",
            "[692 | 2188.55] loss=2.73 avg=2.44\n",
            "[693 | 2191.59] loss=2.31 avg=2.43\n",
            "[694 | 2194.63] loss=2.24 avg=2.43\n",
            "[695 | 2197.68] loss=2.38 avg=2.43\n",
            "[696 | 2200.72] loss=2.35 avg=2.43\n",
            "[697 | 2203.77] loss=2.32 avg=2.43\n",
            "[698 | 2206.83] loss=2.49 avg=2.43\n",
            "[699 | 2209.86] loss=1.72 avg=2.42\n",
            "[700 | 2212.90] loss=2.90 avg=2.43\n",
            "[701 | 2215.94] loss=2.35 avg=2.43\n",
            "[702 | 2218.99] loss=2.75 avg=2.43\n",
            "[703 | 2222.04] loss=1.96 avg=2.43\n",
            "[704 | 2225.08] loss=3.02 avg=2.43\n",
            "[705 | 2228.21] loss=2.60 avg=2.43\n",
            "[706 | 2231.25] loss=2.22 avg=2.43\n",
            "[707 | 2234.27] loss=2.90 avg=2.44\n",
            "[708 | 2237.32] loss=2.36 avg=2.44\n",
            "[709 | 2240.35] loss=2.17 avg=2.43\n",
            "[710 | 2243.41] loss=2.48 avg=2.43\n",
            "[711 | 2246.44] loss=1.85 avg=2.43\n",
            "[712 | 2249.48] loss=2.30 avg=2.43\n",
            "[713 | 2252.51] loss=2.82 avg=2.43\n",
            "[714 | 2255.55] loss=2.61 avg=2.43\n",
            "[715 | 2258.58] loss=2.32 avg=2.43\n",
            "[716 | 2261.59] loss=2.27 avg=2.43\n",
            "[717 | 2264.61] loss=2.25 avg=2.43\n",
            "[718 | 2267.64] loss=2.91 avg=2.43\n",
            "[719 | 2270.66] loss=2.97 avg=2.44\n",
            "[720 | 2273.69] loss=2.86 avg=2.44\n",
            "[721 | 2276.71] loss=2.60 avg=2.44\n",
            "[722 | 2279.73] loss=2.31 avg=2.44\n",
            "[723 | 2282.76] loss=2.10 avg=2.44\n",
            "[724 | 2285.80] loss=2.25 avg=2.44\n",
            "[725 | 2288.84] loss=3.06 avg=2.44\n",
            "[726 | 2291.88] loss=2.31 avg=2.44\n",
            "[727 | 2294.92] loss=2.23 avg=2.44\n",
            "[728 | 2297.95] loss=2.41 avg=2.44\n",
            "[729 | 2300.99] loss=2.60 avg=2.44\n",
            "[730 | 2304.02] loss=3.15 avg=2.45\n",
            "[731 | 2307.04] loss=2.10 avg=2.44\n",
            "[732 | 2310.08] loss=2.86 avg=2.45\n",
            "[733 | 2313.12] loss=2.54 avg=2.45\n",
            "[734 | 2316.16] loss=2.12 avg=2.45\n",
            "[735 | 2319.20] loss=2.46 avg=2.45\n",
            "[736 | 2322.23] loss=2.48 avg=2.45\n",
            "[737 | 2325.28] loss=2.74 avg=2.45\n",
            "[738 | 2328.32] loss=2.79 avg=2.45\n",
            "[739 | 2331.37] loss=2.44 avg=2.45\n",
            "[740 | 2334.44] loss=2.63 avg=2.45\n",
            "[741 | 2337.50] loss=2.41 avg=2.45\n",
            "[742 | 2340.55] loss=2.97 avg=2.46\n",
            "[743 | 2343.59] loss=2.34 avg=2.46\n",
            "[744 | 2346.62] loss=2.64 avg=2.46\n",
            "[745 | 2349.65] loss=2.59 avg=2.46\n",
            "[746 | 2352.65] loss=2.79 avg=2.46\n",
            "[747 | 2355.67] loss=2.38 avg=2.46\n",
            "[748 | 2358.69] loss=2.17 avg=2.46\n",
            "[749 | 2361.71] loss=2.42 avg=2.46\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "500180) h1st: well for me it's still the easiest way I can get my gparted to do this.\n",
            "(1194868640) swt: gparted was just a great tool. I think gparted is what I'd like to have in my life.\n",
            "(1186159880) swt: you can't have two of the same thing.\n",
            "(1186159940) h1st: if something does happen while running gparted, it won't allow me to get it back.\n",
            "(1186161000) swt: but there is another gparted that was much easier and could run faster, gparted-partition-2\n",
            "(1146967140) h1st: and it's a gparted thing\n",
            "(1146967140) swt: and also gparted-3\n",
            "(1146967140) swt: i see\n",
            "(1146967140) swt: gparted is a gparted thing\n",
            "(1146967200) h1st: yes it is. But gparted-partition-2 is better, with its ext3-size, which is in linux-systems.. but in Windows I just don't use it for any more than 4 TB.\n",
            "(1146967300) swt: which one I can run?\n",
            "(1146967300) swt: gparted is in my home directory\n",
            "(1146967360) h1st: ah, i forgot to write it, the file name isn't as long.\n",
            "(1146967440) swt: if you don't have a partition as root, then partitioned, it shouldn't have been used, since as the name suggests, it would be the partition that would be partitioned after the gparted would be partitioned.\n",
            "(1146967500) h1st: but you can find out if it's the right path, on gparted-partition you could see which it will be then\n",
            "(1146967500) swt: it won't be there, but if it's in your home folder, then it's probably it's the gparted partition.\n",
            "(1186166900) swt: but gparted-3 will be automatically partitioned after the gparted will be partitioned, so it's the same, but you don't have a partition as root?\n",
            "(1186166960) h1st: no, but if you are installing gparted-partition-2, then it's probably partitioned for the gparted-partition-1 partition, but I didn't know if i would have the right path, I could have it partitioned as a partition on the gparted-partition-2 partition, but then it would be there to run gparted-partition-3\n",
            "\n",
            "\n",
            "(1097333760) D1kM: do you have a GUI to start a service that runs it's own processes like ncurses, for example httpd.conf?\n",
            "(1097333760) D1kM: ok, no i didn't mean for all of them to run, i only ran a system with them in it: httpd /etc/systemd/init.d\n",
            "\n",
            "\n",
            "(1281716480) chriso: why is there not an option in 'gnome-settings-panel-icon-manager' to add the icons?\n",
            "(1281716480) chriso: !cursoricon\n",
            "(1281716480) chriso: !gnome\n",
            "(1281716780) chriso: !gnome-settings-panel-icon-manager\n",
            "(1281716780) chriso: !gnome -cursor\n",
            "(1281716840) chriso: and, that's not in gnome-settings-panel. And then, they dont use the ctrl+alt+del buttons!\n",
            "(1281716840) geebles: the mouse has no buttons on it anymore so I dont think your mouse is in gnome-settings-panel...\n",
            "\n",
            "\n",
            "(1215802200) h00k: !help\n",
            "(1215802200) vrz: what version of ubuntu do you use?\n",
            "(1215802200) vrz:  I would guess 5.10\n",
            "(121580220) vrz: maybe install it from synaptic, so you dont need synaptic :)\n",
            "\n",
            "\n",
            "(1232088240) dm_e: i've updated my system and i'm finding the following errors when i try to install a package/extension. Any ideas?\n",
            "(1232088300) dm_e: is there a menu item to change a program from /tmp?\n",
            "(1232088300)\n",
            "\n",
            "[750 | 2401.53] loss=2.24 avg=2.46\n",
            "[751 | 2404.51] loss=2.30 avg=2.46\n",
            "[752 | 2407.50] loss=2.48 avg=2.46\n",
            "[753 | 2410.50] loss=1.90 avg=2.45\n",
            "[754 | 2413.52] loss=2.69 avg=2.45\n",
            "[755 | 2416.54] loss=2.93 avg=2.46\n",
            "[756 | 2419.56] loss=2.06 avg=2.45\n",
            "[757 | 2422.56] loss=2.42 avg=2.45\n",
            "[758 | 2425.58] loss=2.32 avg=2.45\n",
            "[759 | 2428.57] loss=2.33 avg=2.45\n",
            "[760 | 2431.61] loss=2.06 avg=2.45\n",
            "[761 | 2434.63] loss=2.16 avg=2.44\n",
            "[762 | 2437.65] loss=2.92 avg=2.45\n",
            "[763 | 2440.67] loss=2.52 avg=2.45\n",
            "[764 | 2443.69] loss=2.38 avg=2.45\n",
            "[765 | 2446.71] loss=2.85 avg=2.45\n",
            "[766 | 2449.74] loss=2.60 avg=2.45\n",
            "[767 | 2452.76] loss=2.32 avg=2.45\n",
            "[768 | 2455.78] loss=2.16 avg=2.45\n",
            "[769 | 2458.79] loss=2.25 avg=2.45\n",
            "[770 | 2461.80] loss=2.12 avg=2.45\n",
            "[771 | 2464.81] loss=2.64 avg=2.45\n",
            "[772 | 2467.83] loss=2.65 avg=2.45\n",
            "[773 | 2470.85] loss=2.77 avg=2.45\n",
            "[774 | 2473.87] loss=3.09 avg=2.46\n",
            "[775 | 2476.89] loss=1.98 avg=2.45\n",
            "[776 | 2479.91] loss=1.73 avg=2.45\n",
            "[777 | 2482.92] loss=2.04 avg=2.44\n",
            "[778 | 2485.94] loss=2.06 avg=2.44\n",
            "[779 | 2488.95] loss=2.90 avg=2.44\n",
            "[780 | 2491.95] loss=2.40 avg=2.44\n",
            "[781 | 2494.97] loss=2.47 avg=2.44\n",
            "[782 | 2497.99] loss=1.90 avg=2.44\n",
            "[783 | 2501.01] loss=2.27 avg=2.44\n",
            "[784 | 2504.04] loss=2.31 avg=2.43\n",
            "[785 | 2507.05] loss=2.26 avg=2.43\n",
            "[786 | 2510.07] loss=2.32 avg=2.43\n",
            "[787 | 2513.09] loss=1.92 avg=2.43\n",
            "[788 | 2516.11] loss=2.20 avg=2.42\n",
            "[789 | 2519.12] loss=2.36 avg=2.42\n",
            "[790 | 2522.14] loss=2.47 avg=2.42\n",
            "[791 | 2525.16] loss=2.37 avg=2.42\n",
            "[792 | 2528.15] loss=2.35 avg=2.42\n",
            "[793 | 2531.12] loss=2.86 avg=2.43\n",
            "[794 | 2534.10] loss=2.80 avg=2.43\n",
            "[795 | 2537.08] loss=1.60 avg=2.42\n",
            "[796 | 2540.06] loss=2.19 avg=2.42\n",
            "[797 | 2543.03] loss=2.33 avg=2.42\n",
            "[798 | 2546.03] loss=2.66 avg=2.42\n",
            "[799 | 2549.06] loss=2.86 avg=2.43\n",
            "[800 | 2552.07] loss=2.03 avg=2.42\n",
            "[801 | 2555.09] loss=2.61 avg=2.42\n",
            "[802 | 2558.11] loss=2.77 avg=2.43\n",
            "[803 | 2561.13] loss=2.07 avg=2.42\n",
            "[804 | 2564.14] loss=2.13 avg=2.42\n",
            "[805 | 2567.15] loss=2.11 avg=2.42\n",
            "[806 | 2570.17] loss=1.90 avg=2.41\n",
            "[807 | 2573.17] loss=2.67 avg=2.42\n",
            "[808 | 2576.20] loss=2.40 avg=2.42\n",
            "[809 | 2579.21] loss=2.01 avg=2.41\n",
            "[810 | 2582.20] loss=2.89 avg=2.42\n",
            "[811 | 2585.22] loss=2.32 avg=2.41\n",
            "[812 | 2588.24] loss=1.58 avg=2.41\n",
            "[813 | 2591.26] loss=2.29 avg=2.41\n",
            "[814 | 2594.28] loss=1.63 avg=2.40\n",
            "[815 | 2597.30] loss=2.83 avg=2.40\n",
            "[816 | 2600.31] loss=2.30 avg=2.40\n",
            "[817 | 2603.33] loss=2.70 avg=2.40\n",
            "[818 | 2606.34] loss=2.27 avg=2.40\n",
            "[819 | 2609.36] loss=2.61 avg=2.40\n",
            "[820 | 2612.37] loss=2.47 avg=2.41\n",
            "[821 | 2615.39] loss=2.41 avg=2.41\n",
            "[822 | 2618.40] loss=2.33 avg=2.40\n",
            "[823 | 2621.41] loss=2.55 avg=2.41\n",
            "[824 | 2624.42] loss=2.80 avg=2.41\n",
            "[825 | 2627.44] loss=2.23 avg=2.41\n",
            "[826 | 2630.45] loss=3.01 avg=2.41\n",
            "[827 | 2633.47] loss=2.26 avg=2.41\n",
            "[828 | 2636.48] loss=2.36 avg=2.41\n",
            "[829 | 2639.50] loss=2.45 avg=2.41\n",
            "[830 | 2642.51] loss=2.88 avg=2.42\n",
            "[831 | 2645.52] loss=2.41 avg=2.42\n",
            "[832 | 2648.54] loss=2.46 avg=2.42\n",
            "[833 | 2651.56] loss=2.58 avg=2.42\n",
            "[834 | 2654.57] loss=2.36 avg=2.42\n",
            "[835 | 2657.58] loss=2.52 avg=2.42\n",
            "[836 | 2660.59] loss=2.39 avg=2.42\n",
            "[837 | 2663.61] loss=2.18 avg=2.42\n",
            "[838 | 2666.62] loss=2.08 avg=2.41\n",
            "[839 | 2669.63] loss=2.59 avg=2.42\n",
            "[840 | 2672.65] loss=2.41 avg=2.42\n",
            "[841 | 2675.66] loss=2.54 avg=2.42\n",
            "[842 | 2678.67] loss=2.24 avg=2.41\n",
            "[843 | 2681.69] loss=2.46 avg=2.42\n",
            "[844 | 2684.70] loss=2.17 avg=2.41\n",
            "[845 | 2687.71] loss=2.67 avg=2.42\n",
            "[846 | 2690.73] loss=2.24 avg=2.41\n",
            "[847 | 2693.74] loss=2.62 avg=2.42\n",
            "[848 | 2696.76] loss=2.17 avg=2.41\n",
            "[849 | 2699.78] loss=2.42 avg=2.41\n",
            "[850 | 2702.79] loss=2.98 avg=2.42\n",
            "[851 | 2705.81] loss=2.35 avg=2.42\n",
            "[852 | 2708.83] loss=2.09 avg=2.41\n",
            "[853 | 2711.85] loss=2.60 avg=2.42\n",
            "[854 | 2714.88] loss=2.33 avg=2.42\n",
            "[855 | 2717.90] loss=2.29 avg=2.41\n",
            "[856 | 2720.90] loss=2.26 avg=2.41\n",
            "[857 | 2723.92] loss=2.68 avg=2.42\n",
            "[858 | 2726.95] loss=2.86 avg=2.42\n",
            "[859 | 2729.97] loss=2.35 avg=2.42\n",
            "[860 | 2732.99] loss=2.36 avg=2.42\n",
            "[861 | 2736.00] loss=2.24 avg=2.42\n",
            "[862 | 2739.02] loss=2.15 avg=2.41\n",
            "[863 | 2742.04] loss=2.57 avg=2.42\n",
            "[864 | 2745.08] loss=2.42 avg=2.42\n",
            "[865 | 2748.09] loss=2.53 avg=2.42\n",
            "[866 | 2751.10] loss=2.83 avg=2.42\n",
            "[867 | 2754.12] loss=2.69 avg=2.42\n",
            "[868 | 2757.15] loss=1.70 avg=2.42\n",
            "[869 | 2760.17] loss=2.59 avg=2.42\n",
            "[870 | 2763.17] loss=2.40 avg=2.42\n",
            "[871 | 2766.18] loss=2.20 avg=2.42\n",
            "[872 | 2769.19] loss=2.42 avg=2.42\n",
            "[873 | 2772.19] loss=2.57 avg=2.42\n",
            "[874 | 2775.19] loss=3.08 avg=2.42\n",
            "[875 | 2778.19] loss=2.11 avg=2.42\n",
            "[876 | 2781.20] loss=2.49 avg=2.42\n",
            "[877 | 2784.19] loss=2.57 avg=2.42\n",
            "[878 | 2787.21] loss=2.50 avg=2.42\n",
            "[879 | 2790.24] loss=2.51 avg=2.42\n",
            "[880 | 2793.27] loss=1.87 avg=2.42\n",
            "[881 | 2796.27] loss=2.03 avg=2.42\n",
            "[882 | 2799.28] loss=2.34 avg=2.41\n",
            "[883 | 2802.28] loss=2.90 avg=2.42\n",
            "[884 | 2805.28] loss=2.20 avg=2.42\n",
            "[885 | 2808.30] loss=1.77 avg=2.41\n",
            "[886 | 2811.31] loss=2.60 avg=2.41\n",
            "[887 | 2814.31] loss=2.29 avg=2.41\n",
            "[888 | 2817.31] loss=2.63 avg=2.41\n",
            "[889 | 2820.33] loss=1.97 avg=2.41\n",
            "[890 | 2823.34] loss=2.16 avg=2.41\n",
            "[891 | 2826.35] loss=2.63 avg=2.41\n",
            "[892 | 2829.36] loss=2.32 avg=2.41\n",
            "[893 | 2832.37] loss=2.58 avg=2.41\n",
            "[894 | 2835.39] loss=1.89 avg=2.40\n",
            "[895 | 2838.40] loss=2.32 avg=2.40\n",
            "[896 | 2841.42] loss=2.17 avg=2.40\n",
            "[897 | 2844.43] loss=2.43 avg=2.40\n",
            "[898 | 2847.45] loss=2.05 avg=2.40\n",
            "[899 | 2850.48] loss=2.14 avg=2.40\n",
            "[900 | 2853.51] loss=2.27 avg=2.39\n",
            "[901 | 2856.54] loss=2.65 avg=2.40\n",
            "[902 | 2859.56] loss=1.88 avg=2.39\n",
            "[903 | 2862.59] loss=2.14 avg=2.39\n",
            "[904 | 2865.62] loss=2.58 avg=2.39\n",
            "[905 | 2868.65] loss=2.64 avg=2.39\n",
            "[906 | 2871.67] loss=2.03 avg=2.39\n",
            "[907 | 2874.69] loss=2.43 avg=2.39\n",
            "[908 | 2877.69] loss=2.23 avg=2.39\n",
            "[909 | 2880.71] loss=2.16 avg=2.39\n",
            "[910 | 2883.73] loss=1.77 avg=2.38\n",
            "[911 | 2886.74] loss=2.80 avg=2.38\n",
            "[912 | 2889.75] loss=2.42 avg=2.38\n",
            "[913 | 2892.77] loss=2.76 avg=2.39\n",
            "[914 | 2895.79] loss=2.61 avg=2.39\n",
            "[915 | 2898.80] loss=2.45 avg=2.39\n",
            "[916 | 2901.82] loss=1.85 avg=2.39\n",
            "[917 | 2904.83] loss=2.38 avg=2.39\n",
            "[918 | 2907.84] loss=3.04 avg=2.39\n",
            "[919 | 2910.87] loss=2.79 avg=2.40\n",
            "[920 | 2913.89] loss=2.58 avg=2.40\n",
            "[921 | 2916.92] loss=2.40 avg=2.40\n",
            "[922 | 2919.95] loss=2.03 avg=2.39\n",
            "[923 | 2922.97] loss=2.52 avg=2.40\n",
            "[924 | 2925.99] loss=2.78 avg=2.40\n",
            "[925 | 2929.00] loss=2.28 avg=2.40\n",
            "[926 | 2932.02] loss=2.23 avg=2.40\n",
            "[927 | 2935.03] loss=2.62 avg=2.40\n",
            "[928 | 2938.03] loss=2.50 avg=2.40\n",
            "[929 | 2941.04] loss=2.64 avg=2.40\n",
            "[930 | 2944.03] loss=2.23 avg=2.40\n",
            "[931 | 2947.02] loss=2.18 avg=2.40\n",
            "[932 | 2950.05] loss=1.91 avg=2.39\n",
            "[933 | 2953.07] loss=2.01 avg=2.39\n",
            "[934 | 2956.08] loss=2.64 avg=2.39\n",
            "[935 | 2959.09] loss=2.16 avg=2.39\n",
            "[936 | 2962.12] loss=2.36 avg=2.39\n",
            "[937 | 2965.15] loss=2.08 avg=2.39\n",
            "[938 | 2968.18] loss=2.71 avg=2.39\n",
            "[939 | 2971.19] loss=2.15 avg=2.39\n",
            "[940 | 2974.20] loss=2.02 avg=2.38\n",
            "[941 | 2977.22] loss=2.79 avg=2.39\n",
            "[942 | 2980.23] loss=2.35 avg=2.39\n",
            "[943 | 2983.24] loss=2.35 avg=2.39\n",
            "[944 | 2986.27] loss=2.44 avg=2.39\n",
            "[945 | 2989.29] loss=2.25 avg=2.39\n",
            "[946 | 2992.32] loss=2.57 avg=2.39\n",
            "[947 | 2995.34] loss=2.86 avg=2.39\n",
            "[948 | 2998.37] loss=2.65 avg=2.40\n",
            "[949 | 3001.38] loss=2.15 avg=2.39\n",
            "[950 | 3004.40] loss=2.57 avg=2.39\n",
            "[951 | 3007.42] loss=2.11 avg=2.39\n",
            "[952 | 3010.43] loss=2.38 avg=2.39\n",
            "[953 | 3013.44] loss=2.45 avg=2.39\n",
            "[954 | 3016.46] loss=1.91 avg=2.39\n",
            "[955 | 3019.48] loss=2.88 avg=2.39\n",
            "[956 | 3022.50] loss=2.36 avg=2.39\n",
            "[957 | 3025.50] loss=2.59 avg=2.39\n",
            "[958 | 3028.51] loss=2.76 avg=2.40\n",
            "[959 | 3031.54] loss=2.66 avg=2.40\n",
            "[960 | 3034.57] loss=2.67 avg=2.40\n",
            "[961 | 3037.60] loss=2.25 avg=2.40\n",
            "[962 | 3040.62] loss=2.68 avg=2.40\n",
            "[963 | 3043.64] loss=2.22 avg=2.40\n",
            "[964 | 3046.65] loss=2.62 avg=2.40\n",
            "[965 | 3049.67] loss=2.31 avg=2.40\n",
            "[966 | 3052.68] loss=2.02 avg=2.40\n",
            "[967 | 3055.70] loss=2.36 avg=2.40\n",
            "[968 | 3058.74] loss=2.59 avg=2.40\n",
            "[969 | 3061.76] loss=2.31 avg=2.40\n",
            "[970 | 3064.78] loss=2.95 avg=2.41\n",
            "[971 | 3067.80] loss=2.15 avg=2.40\n",
            "[972 | 3070.80] loss=2.28 avg=2.40\n",
            "[973 | 3073.80] loss=2.71 avg=2.41\n",
            "[974 | 3076.82] loss=2.27 avg=2.40\n",
            "[975 | 3079.84] loss=1.86 avg=2.40\n",
            "[976 | 3082.86] loss=2.44 avg=2.40\n",
            "[977 | 3085.88] loss=1.75 avg=2.39\n",
            "[978 | 3088.91] loss=2.28 avg=2.39\n",
            "[979 | 3091.91] loss=2.13 avg=2.39\n",
            "[980 | 3094.92] loss=2.34 avg=2.39\n",
            "[981 | 3097.93] loss=2.13 avg=2.39\n",
            "[982 | 3100.95] loss=2.73 avg=2.39\n",
            "[983 | 3103.96] loss=2.29 avg=2.39\n",
            "[984 | 3106.97] loss=2.53 avg=2.39\n",
            "[985 | 3109.99] loss=2.19 avg=2.39\n",
            "[986 | 3113.00] loss=2.30 avg=2.39\n",
            "[987 | 3116.03] loss=2.42 avg=2.39\n",
            "[988 | 3119.06] loss=2.35 avg=2.39\n",
            "[989 | 3122.08] loss=2.57 avg=2.39\n",
            "[990 | 3125.10] loss=2.30 avg=2.39\n",
            "[991 | 3128.12] loss=2.06 avg=2.38\n",
            "[992 | 3131.14] loss=2.64 avg=2.39\n",
            "[993 | 3134.16] loss=2.88 avg=2.39\n",
            "[994 | 3137.20] loss=2.67 avg=2.39\n",
            "[995 | 3140.22] loss=2.12 avg=2.39\n",
            "[996 | 3143.24] loss=2.53 avg=2.39\n",
            "[997 | 3146.25] loss=2.43 avg=2.39\n",
            "[998 | 3149.27] loss=2.55 avg=2.39\n",
            "[999 | 3152.28] loss=1.97 avg=2.39\n",
            "Saving checkpoint/run1/model-1000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "165:  I did it\n",
            "(1246142960) Juhazan:  I'll be honest with you : I am an extremely inexperienced ubuntu user. I know little about ubuntu, only that from what I looked through here, you'd be right :P\n",
            "(1246142960) Juhazan:  the best thing to do is install and try it, if there isnt anything wrong,  try to reinstall the old version so you'd be in a place where you can get help :p\n",
            "(1246143100) Juhazan:  if you are like me, you are a little scared :p\n",
            "(1246143100) Juhazan:  my apologies !\n",
            "\n",
            "\n",
            "(1266952680) c3de: hi :) any idea about how to use my network card in kubuntu?  its a netgear network card\n",
            "(1266952740) c3de:  I am having issue with network connection in kubuntu\n",
            "(1266952740) c3de:  I am having issue with network connection in kubuntu\n",
            "(1266952800) c3de:  I can use my netgear network card\n",
            "(1266952800) ompaul: /s\n",
            "(1266952800) ompaul: or what\n",
            "(1266952800) ompaul: http://wiki.ubuntu.com/NetworkAdapter\n",
            "(1266953800) ompaul: http://wiki.ubuntu.com/NetworkAdapter\n",
            "(1266953860) ompaul: http://wiki.ubuntu.com/NetworkAdapter (if you google I'll give you the link)\n",
            "(1266953920) c3de:  i need a way to connect my network card  to network\n",
            "(1266953920) c3de:  i need a way to connect my network card  to network\n",
            "(1266953980) ompaul: http://wiki.ubuntu.com/NetworkAdapter\n",
            "(1266953980) ompaul: http://wiki.ubuntu.com/NetworkAdapter (if you google I'll give you the link)\n",
            "(1266953980) ompaul: http://wiki.ubuntu.com/NetworkAdapter (if you google I'll give you the link)\n",
            "\n",
            "\n",
            "(1324061180) h1st1d: can someone help with iptables? i have a bunch of iptables stuffs i need to set up. the last question is, what exactly is the iptables thing? so i have one iptables firewall and one iptables gateway.\n",
            "(1324061200) h1st1d: you can create iptables and iptables rules\n",
            "(1324061260) h1st1d: iptables=nat\n",
            "(1324061320) h1st1d: nat is not on iptables\n",
            "(1324061440) h1st1d: is there a way to get iptables on ubuntu? my iptables firewall firewall isnt set up properly\n",
            "(1324061500) h1st1d: ive already found it in the firewall\n",
            "(1324061560) h1st1d: yes iptables\n",
            "(1324061560) h1st1d: im trying to get a proxy iptables, but i need to have access to the server\n",
            "(1324061620) h1st1d: ive installed ipv4nat and iptables\n",
            "\n",
            "\n",
            "(1226331640) gimmethek: I cant install kubuntu on my ubuntu\n",
            "(1226331700) gimmethek: i don't see any files\n",
            "(1226331700) gimmethek: !kubuntu\n",
            "(1226331700) gimmethek: kubuntu is the only way\n",
            "(1226331760) gimmethek: please?\n",
            "(1226331760) gimmethek: my kubuntu install\n",
            "(1226331800) gimmethek: kubuntu is hard to get\n",
            "(1226331820) gimmethek: how to get kubuntu please?\n",
            "(1226331880) gimmethek: kubuntu has some problems with the network\n",
            "(1226331880) gimmethek: !kubuntu\n",
            "(1226331880) gimmethek: thanks for help\n",
            "\n",
            "\n",
            "(1127833200) zarqin: Hi, can somebody help me remove one program and the others have it installed\n",
            "(1127833200) zarqin: from kde\n",
            "(11278\n",
            "\n",
            "[1000 | 3204.24] loss=2.42 avg=2.39\n",
            "[1001 | 3207.25] loss=2.52 avg=2.39\n",
            "[1002 | 3210.28] loss=2.69 avg=2.40\n",
            "[1003 | 3213.28] loss=2.47 avg=2.40\n",
            "[1004 | 3216.29] loss=2.45 avg=2.40\n",
            "[1005 | 3219.30] loss=2.59 avg=2.40\n",
            "[1006 | 3222.31] loss=2.25 avg=2.40\n",
            "[1007 | 3225.32] loss=2.51 avg=2.40\n",
            "[1008 | 3228.33] loss=2.27 avg=2.40\n",
            "[1009 | 3231.34] loss=2.13 avg=2.39\n",
            "[1010 | 3234.36] loss=2.18 avg=2.39\n",
            "[1011 | 3237.37] loss=2.65 avg=2.39\n",
            "[1012 | 3240.38] loss=2.42 avg=2.39\n",
            "[1013 | 3243.40] loss=2.63 avg=2.40\n",
            "[1014 | 3246.41] loss=2.68 avg=2.40\n",
            "[1015 | 3249.42] loss=2.15 avg=2.40\n",
            "[1016 | 3252.44] loss=2.41 avg=2.40\n",
            "[1017 | 3255.46] loss=2.94 avg=2.40\n",
            "[1018 | 3258.47] loss=3.05 avg=2.41\n",
            "[1019 | 3261.48] loss=2.11 avg=2.41\n",
            "[1020 | 3264.49] loss=2.70 avg=2.41\n",
            "[1021 | 3267.51] loss=3.04 avg=2.42\n",
            "[1022 | 3270.51] loss=2.37 avg=2.42\n",
            "[1023 | 3273.53] loss=1.51 avg=2.41\n",
            "[1024 | 3276.56] loss=2.15 avg=2.40\n",
            "[1025 | 3279.59] loss=1.95 avg=2.40\n",
            "[1026 | 3282.62] loss=2.59 avg=2.40\n",
            "[1027 | 3285.64] loss=2.66 avg=2.40\n",
            "[1028 | 3288.66] loss=2.90 avg=2.41\n",
            "[1029 | 3291.68] loss=2.74 avg=2.41\n",
            "[1030 | 3294.72] loss=2.47 avg=2.41\n",
            "[1031 | 3297.75] loss=2.83 avg=2.42\n",
            "[1032 | 3300.77] loss=2.40 avg=2.42\n",
            "[1033 | 3303.78] loss=2.23 avg=2.41\n",
            "[1034 | 3306.79] loss=2.04 avg=2.41\n",
            "[1035 | 3309.81] loss=2.88 avg=2.42\n",
            "[1036 | 3312.82] loss=2.45 avg=2.42\n",
            "[1037 | 3315.82] loss=2.09 avg=2.41\n",
            "[1038 | 3318.82] loss=2.39 avg=2.41\n",
            "[1039 | 3321.85] loss=2.70 avg=2.42\n",
            "[1040 | 3324.86] loss=2.33 avg=2.41\n",
            "[1041 | 3327.89] loss=2.37 avg=2.41\n",
            "[1042 | 3330.89] loss=2.29 avg=2.41\n",
            "[1043 | 3333.92] loss=2.21 avg=2.41\n",
            "[1044 | 3336.99] loss=2.54 avg=2.41\n",
            "[1045 | 3340.01] loss=2.47 avg=2.41\n",
            "[1046 | 3343.01] loss=2.27 avg=2.41\n",
            "[1047 | 3346.04] loss=2.70 avg=2.41\n",
            "[1048 | 3349.05] loss=2.28 avg=2.41\n",
            "[1049 | 3352.05] loss=2.07 avg=2.41\n",
            "[1050 | 3355.07] loss=2.22 avg=2.41\n",
            "[1051 | 3358.09] loss=2.49 avg=2.41\n",
            "[1052 | 3361.11] loss=1.82 avg=2.40\n",
            "[1053 | 3364.11] loss=1.79 avg=2.40\n",
            "[1054 | 3367.13] loss=2.67 avg=2.40\n",
            "[1055 | 3370.13] loss=2.30 avg=2.40\n",
            "[1056 | 3373.14] loss=1.95 avg=2.39\n",
            "[1057 | 3376.16] loss=2.45 avg=2.39\n",
            "[1058 | 3379.17] loss=2.08 avg=2.39\n",
            "[1059 | 3382.19] loss=2.45 avg=2.39\n",
            "[1060 | 3385.21] loss=1.90 avg=2.39\n",
            "[1061 | 3388.22] loss=1.95 avg=2.38\n",
            "[1062 | 3391.25] loss=1.95 avg=2.38\n",
            "[1063 | 3394.26] loss=2.11 avg=2.38\n",
            "[1064 | 3397.28] loss=2.09 avg=2.37\n",
            "[1065 | 3400.32] loss=2.28 avg=2.37\n",
            "[1066 | 3403.33] loss=2.12 avg=2.37\n",
            "[1067 | 3406.35] loss=2.15 avg=2.37\n",
            "[1068 | 3409.37] loss=2.64 avg=2.37\n",
            "[1069 | 3412.39] loss=2.96 avg=2.38\n",
            "[1070 | 3415.44] loss=2.81 avg=2.38\n",
            "[1071 | 3418.45] loss=2.26 avg=2.38\n",
            "[1072 | 3421.47] loss=2.36 avg=2.38\n",
            "[1073 | 3424.49] loss=2.16 avg=2.38\n",
            "[1074 | 3427.50] loss=2.20 avg=2.37\n",
            "[1075 | 3430.51] loss=1.86 avg=2.37\n",
            "[1076 | 3433.54] loss=2.79 avg=2.37\n",
            "[1077 | 3436.56] loss=2.29 avg=2.37\n",
            "[1078 | 3439.59] loss=2.39 avg=2.37\n",
            "[1079 | 3442.62] loss=2.22 avg=2.37\n",
            "[1080 | 3445.64] loss=2.63 avg=2.37\n",
            "[1081 | 3448.65] loss=2.24 avg=2.37\n",
            "[1082 | 3451.67] loss=2.27 avg=2.37\n",
            "[1083 | 3454.68] loss=2.49 avg=2.37\n",
            "[1084 | 3457.70] loss=2.35 avg=2.37\n",
            "[1085 | 3460.71] loss=2.10 avg=2.37\n",
            "[1086 | 3463.73] loss=2.19 avg=2.37\n",
            "[1087 | 3466.74] loss=2.93 avg=2.37\n",
            "[1088 | 3469.75] loss=2.29 avg=2.37\n",
            "[1089 | 3472.76] loss=2.52 avg=2.37\n",
            "[1090 | 3475.78] loss=2.62 avg=2.38\n",
            "[1091 | 3478.80] loss=2.44 avg=2.38\n",
            "[1092 | 3481.83] loss=2.71 avg=2.38\n",
            "[1093 | 3484.85] loss=2.73 avg=2.38\n",
            "[1094 | 3487.88] loss=2.23 avg=2.38\n",
            "[1095 | 3490.90] loss=2.26 avg=2.38\n",
            "[1096 | 3493.92] loss=2.54 avg=2.38\n",
            "[1097 | 3496.93] loss=2.29 avg=2.38\n",
            "[1098 | 3499.95] loss=1.95 avg=2.38\n",
            "[1099 | 3502.97] loss=2.11 avg=2.38\n",
            "[1100 | 3505.97] loss=2.90 avg=2.38\n",
            "[1101 | 3508.98] loss=2.29 avg=2.38\n",
            "[1102 | 3512.00] loss=2.67 avg=2.38\n",
            "[1103 | 3515.01] loss=2.32 avg=2.38\n",
            "[1104 | 3518.03] loss=2.24 avg=2.38\n",
            "[1105 | 3521.06] loss=1.63 avg=2.37\n",
            "[1106 | 3524.07] loss=2.49 avg=2.37\n",
            "[1107 | 3527.10] loss=2.62 avg=2.38\n",
            "[1108 | 3530.12] loss=2.07 avg=2.37\n",
            "[1109 | 3533.15] loss=2.55 avg=2.38\n",
            "[1110 | 3536.17] loss=2.31 avg=2.37\n",
            "[1111 | 3539.17] loss=2.76 avg=2.38\n",
            "[1112 | 3542.19] loss=2.03 avg=2.37\n",
            "[1113 | 3545.20] loss=2.54 avg=2.38\n",
            "[1114 | 3548.21] loss=2.21 avg=2.37\n",
            "[1115 | 3551.23] loss=2.29 avg=2.37\n",
            "[1116 | 3554.24] loss=2.25 avg=2.37\n",
            "[1117 | 3557.26] loss=2.68 avg=2.38\n",
            "[1118 | 3560.26] loss=2.28 avg=2.37\n",
            "[1119 | 3563.25] loss=1.92 avg=2.37\n",
            "[1120 | 3566.26] loss=2.77 avg=2.37\n",
            "[1121 | 3569.30] loss=2.28 avg=2.37\n",
            "[1122 | 3572.32] loss=3.05 avg=2.38\n",
            "[1123 | 3575.35] loss=2.44 avg=2.38\n",
            "[1124 | 3578.36] loss=1.98 avg=2.38\n",
            "[1125 | 3581.37] loss=2.09 avg=2.37\n",
            "[1126 | 3584.39] loss=2.12 avg=2.37\n",
            "[1127 | 3587.40] loss=2.39 avg=2.37\n",
            "[1128 | 3590.42] loss=2.20 avg=2.37\n",
            "[1129 | 3593.44] loss=2.78 avg=2.37\n",
            "[1130 | 3596.44] loss=2.74 avg=2.38\n",
            "[1131 | 3599.46] loss=2.39 avg=2.38\n",
            "[1132 | 3602.47] loss=2.79 avg=2.38\n",
            "[1133 | 3605.48] loss=2.42 avg=2.38\n",
            "[1134 | 3608.49] loss=2.54 avg=2.38\n",
            "[1135 | 3611.51] loss=1.80 avg=2.38\n",
            "[1136 | 3614.52] loss=2.15 avg=2.38\n",
            "[1137 | 3617.54] loss=2.78 avg=2.38\n",
            "[1138 | 3620.55] loss=2.83 avg=2.38\n",
            "[1139 | 3623.57] loss=2.48 avg=2.39\n",
            "[1140 | 3626.58] loss=2.53 avg=2.39\n",
            "[1141 | 3629.60] loss=2.49 avg=2.39\n",
            "[1142 | 3632.60] loss=2.19 avg=2.39\n",
            "[1143 | 3635.61] loss=2.49 avg=2.39\n",
            "[1144 | 3638.62] loss=2.16 avg=2.38\n",
            "[1145 | 3641.63] loss=2.09 avg=2.38\n",
            "[1146 | 3644.63] loss=2.42 avg=2.38\n",
            "[1147 | 3647.63] loss=2.44 avg=2.38\n",
            "[1148 | 3650.64] loss=2.39 avg=2.38\n",
            "[1149 | 3653.61] loss=2.10 avg=2.38\n",
            "[1150 | 3656.62] loss=2.32 avg=2.38\n",
            "[1151 | 3659.61] loss=2.14 avg=2.38\n",
            "[1152 | 3662.61] loss=2.39 avg=2.38\n",
            "[1153 | 3665.59] loss=2.15 avg=2.37\n",
            "[1154 | 3668.57] loss=2.87 avg=2.38\n",
            "[1155 | 3671.59] loss=2.76 avg=2.38\n",
            "[1156 | 3674.59] loss=2.53 avg=2.38\n",
            "[1157 | 3677.60] loss=2.46 avg=2.39\n",
            "[1158 | 3680.62] loss=2.43 avg=2.39\n",
            "[1159 | 3683.62] loss=2.47 avg=2.39\n",
            "[1160 | 3686.63] loss=2.49 avg=2.39\n",
            "[1161 | 3689.62] loss=1.93 avg=2.38\n",
            "[1162 | 3692.59] loss=2.46 avg=2.38\n",
            "[1163 | 3695.58] loss=2.08 avg=2.38\n",
            "[1164 | 3698.58] loss=2.66 avg=2.38\n",
            "[1165 | 3701.57] loss=2.33 avg=2.38\n",
            "[1166 | 3704.56] loss=2.52 avg=2.38\n",
            "[1167 | 3707.54] loss=1.92 avg=2.38\n",
            "[1168 | 3710.54] loss=3.20 avg=2.39\n",
            "[1169 | 3713.51] loss=2.04 avg=2.38\n",
            "[1170 | 3716.52] loss=2.45 avg=2.39\n",
            "[1171 | 3719.54] loss=2.21 avg=2.38\n",
            "[1172 | 3722.57] loss=2.30 avg=2.38\n",
            "[1173 | 3725.60] loss=2.18 avg=2.38\n",
            "[1174 | 3728.61] loss=2.35 avg=2.38\n",
            "[1175 | 3731.64] loss=2.84 avg=2.39\n",
            "[1176 | 3734.67] loss=2.20 avg=2.38\n",
            "[1177 | 3737.70] loss=1.75 avg=2.38\n",
            "[1178 | 3740.72] loss=2.79 avg=2.38\n",
            "[1179 | 3743.74] loss=2.61 avg=2.38\n",
            "[1180 | 3746.75] loss=2.49 avg=2.38\n",
            "[1181 | 3749.76] loss=2.15 avg=2.38\n",
            "[1182 | 3752.79] loss=2.23 avg=2.38\n",
            "[1183 | 3755.80] loss=2.28 avg=2.38\n",
            "[1184 | 3758.81] loss=2.73 avg=2.38\n",
            "[1185 | 3761.83] loss=2.62 avg=2.39\n",
            "[1186 | 3764.84] loss=2.46 avg=2.39\n",
            "[1187 | 3767.87] loss=2.28 avg=2.39\n",
            "[1188 | 3770.89] loss=2.23 avg=2.38\n",
            "[1189 | 3773.92] loss=2.44 avg=2.38\n",
            "[1190 | 3776.95] loss=2.28 avg=2.38\n",
            "[1191 | 3779.97] loss=2.43 avg=2.38\n",
            "[1192 | 3783.00] loss=2.83 avg=2.39\n",
            "[1193 | 3786.03] loss=2.71 avg=2.39\n",
            "[1194 | 3789.06] loss=2.64 avg=2.39\n",
            "[1195 | 3792.09] loss=2.40 avg=2.39\n",
            "[1196 | 3795.10] loss=2.14 avg=2.39\n",
            "[1197 | 3798.11] loss=2.24 avg=2.39\n",
            "[1198 | 3801.12] loss=2.60 avg=2.39\n",
            "[1199 | 3804.14] loss=2.38 avg=2.39\n",
            "[1200 | 3807.16] loss=1.98 avg=2.39\n",
            "[1201 | 3810.18] loss=1.84 avg=2.38\n",
            "[1202 | 3813.20] loss=2.67 avg=2.38\n",
            "[1203 | 3816.21] loss=2.14 avg=2.38\n",
            "[1204 | 3819.23] loss=2.14 avg=2.38\n",
            "[1205 | 3822.24] loss=2.10 avg=2.38\n",
            "[1206 | 3825.28] loss=2.37 avg=2.38\n",
            "[1207 | 3828.29] loss=2.19 avg=2.38\n",
            "[1208 | 3831.33] loss=2.57 avg=2.38\n",
            "[1209 | 3834.36] loss=2.44 avg=2.38\n",
            "[1210 | 3837.38] loss=1.98 avg=2.37\n",
            "[1211 | 3840.39] loss=1.71 avg=2.37\n",
            "[1212 | 3843.40] loss=2.21 avg=2.37\n",
            "[1213 | 3846.42] loss=2.20 avg=2.36\n",
            "[1214 | 3849.44] loss=2.28 avg=2.36\n",
            "[1215 | 3852.47] loss=2.01 avg=2.36\n",
            "[1216 | 3855.49] loss=2.31 avg=2.36\n",
            "[1217 | 3858.51] loss=2.08 avg=2.36\n",
            "[1218 | 3861.53] loss=2.75 avg=2.36\n",
            "[1219 | 3864.55] loss=2.73 avg=2.36\n",
            "[1220 | 3867.58] loss=2.73 avg=2.37\n",
            "[1221 | 3870.59] loss=2.39 avg=2.37\n",
            "[1222 | 3873.61] loss=2.28 avg=2.37\n",
            "[1223 | 3876.63] loss=2.72 avg=2.37\n",
            "[1224 | 3879.66] loss=2.28 avg=2.37\n",
            "[1225 | 3882.68] loss=2.40 avg=2.37\n",
            "[1226 | 3885.70] loss=2.15 avg=2.37\n",
            "[1227 | 3888.72] loss=2.88 avg=2.37\n",
            "[1228 | 3891.73] loss=2.44 avg=2.37\n",
            "[1229 | 3894.74] loss=2.17 avg=2.37\n",
            "[1230 | 3897.76] loss=2.49 avg=2.37\n",
            "[1231 | 3900.77] loss=2.02 avg=2.37\n",
            "[1232 | 3903.78] loss=2.22 avg=2.37\n",
            "[1233 | 3906.79] loss=2.41 avg=2.37\n",
            "[1234 | 3909.81] loss=1.96 avg=2.36\n",
            "[1235 | 3912.83] loss=2.77 avg=2.37\n",
            "[1236 | 3915.85] loss=2.93 avg=2.37\n",
            "[1237 | 3918.85] loss=2.26 avg=2.37\n",
            "[1238 | 3921.87] loss=2.16 avg=2.37\n",
            "[1239 | 3924.88] loss=2.12 avg=2.37\n",
            "[1240 | 3927.90] loss=2.51 avg=2.37\n",
            "[1241 | 3930.91] loss=2.42 avg=2.37\n",
            "[1242 | 3933.93] loss=1.96 avg=2.37\n",
            "[1243 | 3936.95] loss=2.75 avg=2.37\n",
            "[1244 | 3939.95] loss=2.44 avg=2.37\n",
            "[1245 | 3942.95] loss=2.29 avg=2.37\n",
            "[1246 | 3945.97] loss=2.39 avg=2.37\n",
            "[1247 | 3948.99] loss=2.07 avg=2.37\n",
            "[1248 | 3952.01] loss=2.10 avg=2.36\n",
            "[1249 | 3955.03] loss=1.95 avg=2.36\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "75) jonathanb: what package? and what's it's about?\n",
            "(1210662440) fredycat: the ubuntu-desktop package?\n",
            "(1210662440) jonathanb: ah, good point.   if you're already running it on a different system, it will find it when its loaded and load the files, even when the laptop, or another machine, isn't online (that would let you set things up without having to reboot)\n",
            "(1210662560) jonathanb: okay.  you get an error in the terminal saying 'can't load the module: File(1).java is not found'\n",
            "(1210662560) jonathanb: not in the terminal\n",
            "(1210662560) jonathanb: what's that about?\n",
            "(1210662560) fredycat: the 'can't load the module' is a bug we have to report, you know\n",
            "\n",
            "\n",
            "(1245707920) j_fantastic_hobo: how do I change the name of some directory?\n",
            "(1245707980) sf: in terminal: lname   for the path\n",
            "(1245708040) j_fantastic_hobo: it's the actual path\n",
            "(1245705100) sf: look at /etc/fstab\n",
            "(1245705160) j_fantastic_hobo: it says it's /var/tmp\n",
            "(1245705160) sf: it's not a path\n",
            "(1245705160) j_fantastic_hobo: you also said this is a /dev/sdb1\n",
            "(1245704240) j_fantastic_hobo: and the /proc file says the /dev/sdb1/dev is /var/lib/fstab\n",
            "(1245704240) sf: i think it means the '/var/tmp' partition\n",
            "(1245704240) sf: not /var\n",
            "(1245704400) sf: do any one do...\n",
            "(1245704400) sf: do they even use ubuntu?\n",
            "(1245704460) sf: are you connected to the internet?\n",
            "(1245704520) sf: i think it means you are connected to the net. not the internet\n",
            "(1245704780) sf: so let's say you want to change from /var/lib to /var/lib/...\n",
            "(1245704780) sf: then there might be an option to change that\n",
            "(1245704780) sf:   you can change the file\n",
            "(1245704840) sf: if you get that error, you should have to change that option\n",
            "(1245704840) sf: just do ls -ld /var/lib/fstab\n",
            "(1245704460) sf: sorry i don't have ubuntu on here :(\n",
            "(1245704580) sf: we don't really care what's in fstab.\n",
            "(1245704640) sf: i think the option is in /etc/fstab\n",
            "(1245704700) sf: you don't have to delete anything from that file.\n",
            "(1245704840) sf: ls -l /var/lib/fstab  would tell you about that\n",
            "(1245704920) sf: i would guess your filesystem isnt /var/lib/fstab\n",
            "(1245704980) sf: look at /var/lib/fstab\n",
            "(1245705040) sf: we don't really do that.\n",
            "\n",
            "\n",
            "(1171577140) aj: can somebody help me please\n",
            "(1171577920) aj: any one?\n",
            "(1171577220) aj: i want to install to my system with ubuntu but i cannot find the packages\n",
            "(1171577220) aj: can somebody help, i don't know\n",
            "(1171577220) aj: i dont have my files in there, what do i do from my system?\n",
            "(1171577220) aj: any one?\n",
            "(1171577280) aj: just an hour or two ago, i used to have files in the ubuntu folder too\n",
            "(1171577280) tonyyork: i suggest you use nero if you're installing a new install on the ubuntu live cd\n",
            "(1171577340) tonyyork: if you do however use nero, it should support you well\n",
            "(1171577400) tonyyork: we are not sure what the package\n",
            "\n",
            "[1250 | 3995.14] loss=2.10 avg=2.36\n",
            "[1251 | 3998.16] loss=2.45 avg=2.36\n",
            "[1252 | 4001.17] loss=2.48 avg=2.36\n",
            "[1253 | 4004.18] loss=2.20 avg=2.36\n",
            "[1254 | 4007.19] loss=2.39 avg=2.36\n",
            "[1255 | 4010.20] loss=2.17 avg=2.36\n",
            "[1256 | 4013.21] loss=2.30 avg=2.36\n",
            "[1257 | 4016.23] loss=2.63 avg=2.36\n",
            "[1258 | 4019.25] loss=2.24 avg=2.36\n",
            "[1259 | 4022.26] loss=2.01 avg=2.35\n",
            "[1260 | 4025.27] loss=2.35 avg=2.35\n",
            "[1261 | 4028.29] loss=2.17 avg=2.35\n",
            "[1262 | 4031.31] loss=2.46 avg=2.35\n",
            "[1263 | 4034.33] loss=2.54 avg=2.35\n",
            "[1264 | 4037.35] loss=2.83 avg=2.36\n",
            "[1265 | 4040.37] loss=2.28 avg=2.36\n",
            "[1266 | 4043.38] loss=2.14 avg=2.36\n",
            "[1267 | 4046.39] loss=2.89 avg=2.36\n",
            "[1268 | 4049.41] loss=2.30 avg=2.36\n",
            "[1269 | 4052.44] loss=2.37 avg=2.36\n",
            "[1270 | 4055.47] loss=2.71 avg=2.36\n",
            "[1271 | 4058.49] loss=2.24 avg=2.36\n",
            "[1272 | 4061.50] loss=2.72 avg=2.37\n",
            "[1273 | 4064.52] loss=3.01 avg=2.37\n",
            "[1274 | 4067.53] loss=2.20 avg=2.37\n",
            "[1275 | 4070.55] loss=2.35 avg=2.37\n",
            "[1276 | 4073.56] loss=2.26 avg=2.37\n",
            "[1277 | 4076.57] loss=1.76 avg=2.36\n",
            "[1278 | 4079.59] loss=2.09 avg=2.36\n",
            "[1279 | 4082.59] loss=2.56 avg=2.36\n",
            "[1280 | 4085.61] loss=2.13 avg=2.36\n",
            "[1281 | 4088.63] loss=2.27 avg=2.36\n",
            "[1282 | 4091.65] loss=2.82 avg=2.36\n",
            "[1283 | 4094.66] loss=2.20 avg=2.36\n",
            "[1284 | 4097.67] loss=1.81 avg=2.36\n",
            "[1285 | 4100.68] loss=2.33 avg=2.36\n",
            "[1286 | 4103.70] loss=2.32 avg=2.36\n",
            "[1287 | 4106.70] loss=2.06 avg=2.35\n",
            "[1288 | 4109.72] loss=2.27 avg=2.35\n",
            "[1289 | 4112.74] loss=2.62 avg=2.36\n",
            "[1290 | 4115.75] loss=2.43 avg=2.36\n",
            "[1291 | 4118.79] loss=2.41 avg=2.36\n",
            "[1292 | 4121.81] loss=2.45 avg=2.36\n",
            "[1293 | 4124.84] loss=2.10 avg=2.36\n",
            "[1294 | 4127.86] loss=2.23 avg=2.35\n",
            "[1295 | 4130.89] loss=2.42 avg=2.35\n",
            "[1296 | 4133.92] loss=2.07 avg=2.35\n",
            "[1297 | 4136.95] loss=2.35 avg=2.35\n",
            "[1298 | 4139.97] loss=2.60 avg=2.35\n",
            "[1299 | 4142.99] loss=2.31 avg=2.35\n",
            "[1300 | 4146.01] loss=2.14 avg=2.35\n",
            "[1301 | 4149.04] loss=2.50 avg=2.35\n",
            "[1302 | 4152.08] loss=2.03 avg=2.35\n",
            "[1303 | 4155.12] loss=2.36 avg=2.35\n",
            "[1304 | 4158.15] loss=2.32 avg=2.35\n",
            "[1305 | 4161.18] loss=2.51 avg=2.35\n",
            "[1306 | 4164.21] loss=2.56 avg=2.35\n",
            "[1307 | 4167.23] loss=2.69 avg=2.36\n",
            "[1308 | 4170.25] loss=2.35 avg=2.36\n",
            "[1309 | 4173.30] loss=2.40 avg=2.36\n",
            "[1310 | 4176.33] loss=2.35 avg=2.36\n",
            "[1311 | 4179.35] loss=2.41 avg=2.36\n",
            "[1312 | 4182.39] loss=2.59 avg=2.36\n",
            "[1313 | 4185.41] loss=2.46 avg=2.36\n",
            "[1314 | 4188.43] loss=2.86 avg=2.37\n",
            "[1315 | 4191.46] loss=2.38 avg=2.37\n",
            "[1316 | 4194.49] loss=2.28 avg=2.37\n",
            "[1317 | 4197.52] loss=2.05 avg=2.36\n",
            "[1318 | 4200.55] loss=2.19 avg=2.36\n",
            "[1319 | 4203.56] loss=2.47 avg=2.36\n",
            "[1320 | 4206.59] loss=2.83 avg=2.37\n",
            "[1321 | 4209.62] loss=2.51 avg=2.37\n",
            "[1322 | 4212.64] loss=2.59 avg=2.37\n",
            "[1323 | 4215.66] loss=2.01 avg=2.37\n",
            "[1324 | 4218.69] loss=1.87 avg=2.36\n",
            "[1325 | 4221.70] loss=2.03 avg=2.36\n",
            "[1326 | 4224.73] loss=2.08 avg=2.36\n",
            "[1327 | 4227.75] loss=2.91 avg=2.36\n",
            "[1328 | 4230.79] loss=2.28 avg=2.36\n",
            "[1329 | 4233.81] loss=2.15 avg=2.36\n",
            "[1330 | 4236.83] loss=2.69 avg=2.36\n",
            "[1331 | 4239.84] loss=2.56 avg=2.36\n",
            "[1332 | 4242.86] loss=2.39 avg=2.36\n",
            "[1333 | 4245.87] loss=2.92 avg=2.37\n",
            "[1334 | 4248.88] loss=2.04 avg=2.37\n",
            "[1335 | 4251.90] loss=2.39 avg=2.37\n",
            "[1336 | 4254.91] loss=2.30 avg=2.37\n",
            "[1337 | 4257.93] loss=2.26 avg=2.36\n",
            "[1338 | 4260.95] loss=2.31 avg=2.36\n",
            "[1339 | 4263.96] loss=2.95 avg=2.37\n",
            "[1340 | 4266.97] loss=2.38 avg=2.37\n",
            "[1341 | 4269.99] loss=2.50 avg=2.37\n",
            "[1342 | 4273.00] loss=2.34 avg=2.37\n",
            "[1343 | 4276.03] loss=2.06 avg=2.37\n",
            "[1344 | 4279.06] loss=2.25 avg=2.37\n",
            "[1345 | 4282.07] loss=2.32 avg=2.37\n",
            "[1346 | 4285.10] loss=2.12 avg=2.36\n",
            "[1347 | 4288.12] loss=2.33 avg=2.36\n",
            "[1348 | 4291.13] loss=2.31 avg=2.36\n",
            "[1349 | 4294.14] loss=1.99 avg=2.36\n",
            "[1350 | 4297.16] loss=2.02 avg=2.36\n",
            "[1351 | 4300.19] loss=2.21 avg=2.35\n",
            "[1352 | 4303.23] loss=2.20 avg=2.35\n",
            "[1353 | 4306.25] loss=2.52 avg=2.35\n",
            "[1354 | 4309.27] loss=2.15 avg=2.35\n",
            "[1355 | 4312.30] loss=1.92 avg=2.35\n",
            "[1356 | 4315.33] loss=2.71 avg=2.35\n",
            "[1357 | 4318.35] loss=2.53 avg=2.35\n",
            "[1358 | 4321.37] loss=2.50 avg=2.35\n",
            "[1359 | 4324.39] loss=1.74 avg=2.35\n",
            "[1360 | 4327.41] loss=2.99 avg=2.36\n",
            "[1361 | 4330.46] loss=2.30 avg=2.35\n",
            "[1362 | 4333.47] loss=2.56 avg=2.36\n",
            "[1363 | 4336.49] loss=2.02 avg=2.35\n",
            "[1364 | 4339.51] loss=2.11 avg=2.35\n",
            "[1365 | 4342.52] loss=1.91 avg=2.35\n",
            "[1366 | 4345.54] loss=2.77 avg=2.35\n",
            "[1367 | 4348.55] loss=3.00 avg=2.36\n",
            "[1368 | 4351.57] loss=2.46 avg=2.36\n",
            "[1369 | 4354.58] loss=2.62 avg=2.36\n",
            "[1370 | 4357.61] loss=2.88 avg=2.37\n",
            "[1371 | 4360.63] loss=2.35 avg=2.37\n",
            "[1372 | 4363.65] loss=2.15 avg=2.36\n",
            "[1373 | 4366.67] loss=2.58 avg=2.37\n",
            "[1374 | 4369.68] loss=2.47 avg=2.37\n",
            "[1375 | 4372.71] loss=2.15 avg=2.36\n",
            "[1376 | 4375.74] loss=2.61 avg=2.37\n",
            "[1377 | 4378.77] loss=2.56 avg=2.37\n",
            "[1378 | 4381.78] loss=2.75 avg=2.37\n",
            "[1379 | 4384.81] loss=2.67 avg=2.38\n",
            "[1380 | 4387.83] loss=2.69 avg=2.38\n",
            "[1381 | 4390.85] loss=2.64 avg=2.38\n",
            "[1382 | 4393.86] loss=2.22 avg=2.38\n",
            "[1383 | 4396.88] loss=2.53 avg=2.38\n",
            "[1384 | 4399.89] loss=2.37 avg=2.38\n",
            "[1385 | 4402.90] loss=2.11 avg=2.38\n",
            "[1386 | 4405.91] loss=2.66 avg=2.38\n",
            "[1387 | 4408.92] loss=2.49 avg=2.38\n",
            "[1388 | 4411.95] loss=2.41 avg=2.38\n",
            "[1389 | 4414.99] loss=2.21 avg=2.38\n",
            "[1390 | 4418.01] loss=2.20 avg=2.38\n",
            "[1391 | 4421.02] loss=2.22 avg=2.38\n",
            "[1392 | 4424.06] loss=2.32 avg=2.38\n",
            "[1393 | 4427.09] loss=1.83 avg=2.37\n",
            "[1394 | 4430.12] loss=1.84 avg=2.37\n",
            "[1395 | 4433.14] loss=2.06 avg=2.36\n",
            "[1396 | 4436.16] loss=2.19 avg=2.36\n",
            "[1397 | 4439.17] loss=2.43 avg=2.36\n",
            "[1398 | 4442.19] loss=2.57 avg=2.36\n",
            "[1399 | 4445.20] loss=2.79 avg=2.37\n",
            "[1400 | 4448.22] loss=2.44 avg=2.37\n",
            "[1401 | 4451.22] loss=2.40 avg=2.37\n",
            "[1402 | 4454.24] loss=2.38 avg=2.37\n",
            "[1403 | 4457.25] loss=2.80 avg=2.37\n",
            "[1404 | 4460.27] loss=2.37 avg=2.37\n",
            "[1405 | 4463.28] loss=2.58 avg=2.38\n",
            "[1406 | 4466.31] loss=2.27 avg=2.38\n",
            "[1407 | 4469.32] loss=2.02 avg=2.37\n",
            "[1408 | 4472.33] loss=1.99 avg=2.37\n",
            "[1409 | 4475.34] loss=2.38 avg=2.37\n",
            "[1410 | 4478.35] loss=2.13 avg=2.37\n",
            "[1411 | 4481.38] loss=2.01 avg=2.36\n",
            "[1412 | 4484.41] loss=2.93 avg=2.37\n",
            "[1413 | 4487.43] loss=2.08 avg=2.36\n",
            "[1414 | 4490.45] loss=2.61 avg=2.37\n",
            "[1415 | 4493.48] loss=2.28 avg=2.37\n",
            "[1416 | 4496.50] loss=2.30 avg=2.37\n",
            "[1417 | 4499.51] loss=2.15 avg=2.36\n",
            "[1418 | 4502.53] loss=2.36 avg=2.36\n",
            "[1419 | 4505.54] loss=2.27 avg=2.36\n",
            "[1420 | 4508.56] loss=2.47 avg=2.36\n",
            "[1421 | 4511.58] loss=2.68 avg=2.37\n",
            "[1422 | 4514.61] loss=2.18 avg=2.36\n",
            "[1423 | 4517.64] loss=2.34 avg=2.36\n",
            "[1424 | 4520.65] loss=2.10 avg=2.36\n",
            "[1425 | 4523.66] loss=2.60 avg=2.36\n",
            "[1426 | 4526.67] loss=2.41 avg=2.36\n",
            "[1427 | 4529.69] loss=2.21 avg=2.36\n",
            "[1428 | 4532.71] loss=1.97 avg=2.36\n",
            "[1429 | 4535.73] loss=2.32 avg=2.36\n",
            "[1430 | 4538.74] loss=2.30 avg=2.36\n",
            "[1431 | 4541.75] loss=1.88 avg=2.35\n",
            "[1432 | 4544.77] loss=1.97 avg=2.35\n",
            "[1433 | 4547.78] loss=2.49 avg=2.35\n",
            "[1434 | 4550.80] loss=1.95 avg=2.35\n",
            "[1435 | 4553.83] loss=2.72 avg=2.35\n",
            "[1436 | 4556.85] loss=2.16 avg=2.35\n",
            "[1437 | 4559.84] loss=2.24 avg=2.35\n",
            "[1438 | 4562.84] loss=2.29 avg=2.35\n",
            "[1439 | 4565.84] loss=2.86 avg=2.35\n",
            "[1440 | 4568.85] loss=2.04 avg=2.35\n",
            "[1441 | 4571.86] loss=2.36 avg=2.35\n",
            "[1442 | 4574.85] loss=2.80 avg=2.35\n",
            "[1443 | 4577.87] loss=2.30 avg=2.35\n",
            "[1444 | 4580.87] loss=2.77 avg=2.36\n",
            "[1445 | 4583.89] loss=2.68 avg=2.36\n",
            "[1446 | 4586.91] loss=2.42 avg=2.36\n",
            "[1447 | 4589.93] loss=2.75 avg=2.37\n",
            "[1448 | 4592.96] loss=2.23 avg=2.36\n",
            "[1449 | 4596.00] loss=2.34 avg=2.36\n",
            "[1450 | 4599.02] loss=2.52 avg=2.37\n",
            "[1451 | 4602.03] loss=2.38 avg=2.37\n",
            "[1452 | 4605.06] loss=2.13 avg=2.36\n",
            "[1453 | 4608.09] loss=2.15 avg=2.36\n",
            "[1454 | 4611.13] loss=2.69 avg=2.36\n",
            "[1455 | 4614.18] loss=2.13 avg=2.36\n",
            "[1456 | 4617.22] loss=2.33 avg=2.36\n",
            "[1457 | 4620.26] loss=1.82 avg=2.36\n",
            "[1458 | 4623.32] loss=2.31 avg=2.36\n",
            "[1459 | 4626.38] loss=2.61 avg=2.36\n",
            "[1460 | 4629.42] loss=2.20 avg=2.36\n",
            "[1461 | 4632.48] loss=2.22 avg=2.36\n",
            "[1462 | 4635.53] loss=2.90 avg=2.36\n",
            "[1463 | 4638.59] loss=2.29 avg=2.36\n",
            "[1464 | 4641.62] loss=2.28 avg=2.36\n",
            "[1465 | 4644.67] loss=2.22 avg=2.36\n",
            "[1466 | 4647.73] loss=2.48 avg=2.36\n",
            "[1467 | 4650.77] loss=2.73 avg=2.36\n",
            "[1468 | 4653.82] loss=2.76 avg=2.37\n",
            "[1469 | 4656.87] loss=2.30 avg=2.37\n",
            "[1470 | 4659.91] loss=2.43 avg=2.37\n",
            "[1471 | 4662.97] loss=2.09 avg=2.36\n",
            "[1472 | 4666.00] loss=2.27 avg=2.36\n",
            "[1473 | 4669.05] loss=1.97 avg=2.36\n",
            "[1474 | 4672.08] loss=2.28 avg=2.36\n",
            "[1475 | 4675.11] loss=2.42 avg=2.36\n",
            "[1476 | 4678.15] loss=2.30 avg=2.36\n",
            "[1477 | 4681.19] loss=2.27 avg=2.36\n",
            "[1478 | 4684.23] loss=2.93 avg=2.36\n",
            "[1479 | 4687.26] loss=2.11 avg=2.36\n",
            "[1480 | 4690.32] loss=2.86 avg=2.37\n",
            "[1481 | 4693.37] loss=2.50 avg=2.37\n",
            "[1482 | 4696.43] loss=2.52 avg=2.37\n",
            "[1483 | 4699.47] loss=2.56 avg=2.37\n",
            "[1484 | 4702.51] loss=2.71 avg=2.37\n",
            "[1485 | 4705.55] loss=2.62 avg=2.38\n",
            "[1486 | 4708.59] loss=2.75 avg=2.38\n",
            "[1487 | 4711.62] loss=2.09 avg=2.38\n",
            "[1488 | 4714.66] loss=2.68 avg=2.38\n",
            "[1489 | 4717.71] loss=2.43 avg=2.38\n",
            "[1490 | 4720.75] loss=2.27 avg=2.38\n",
            "[1491 | 4723.82] loss=2.80 avg=2.38\n",
            "[1492 | 4726.87] loss=1.94 avg=2.38\n",
            "[1493 | 4729.91] loss=2.41 avg=2.38\n",
            "[1494 | 4732.94] loss=2.78 avg=2.38\n",
            "[1495 | 4735.99] loss=2.08 avg=2.38\n",
            "[1496 | 4739.04] loss=2.91 avg=2.39\n",
            "[1497 | 4742.08] loss=2.80 avg=2.39\n",
            "[1498 | 4745.12] loss=2.00 avg=2.39\n",
            "[1499 | 4748.16] loss=2.78 avg=2.39\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " i and the others.\n",
            "(1210740120) LjL:  i know i dont have to take any more time if i want...\n",
            "(1210740180) LjL: but in order to make the work more comfortable i need to put more effort in some cases and i need to start work sooner. in that case the other guys need to have access to more time\n",
            "(1210740180) LjL: i know i've had my share of problems with ubuntu\n",
            "(1210950240) Dr_Willis:   if i were to be at work today i'm not at work tomorrow.   i'm in the hospital.  i've been in a hospital for 4 days.  im not getting any help yet.\n",
            "(1210950240) LjL: if i had gone to a doctor today\n",
            "(1210950360) LjL: my problem is not with the server itself, it has to do with setting up the IP\n",
            "(1210950360) LjL: since i am on ubuntu i was getting some error messages when i attempted to connect to the server.\n",
            "(1210950420) LjL:  because i am now sitting here and i can help others if i so choose\n",
            "(1210950420) LjL: i will have to give up for the next 20 minutes\n",
            "(1210950480) LjL: i want the servers connected\n",
            "(1210950940) ikonia: thats the problem but you don't need to know anything about it then\n",
            "(1210950940) LjL: yes i have been trying the same thing without success\n",
            "(1210950940) LjL: it was the same solution for a few days but i had to set up the IP every time\n",
            "(1210950940) LjL: but i don't want to be a moron again\n",
            "(1210950940) LjL: is there anyway to run a command in a console? i have never used any tool for this purpose.\n",
            "(1210950940) LjL: i have no computer to write it out in\n",
            "(1210950940) LjL: which i did\n",
            "(1210950940) LjL: but the script was too long to type on there\n",
            "(1210950940) LjL: or i don't know the right thing\n",
            "(1210950940) ikonia: i wouldnt want to write a whole script\n",
            "(1210950940) ikonia: is there some way for you to run a gui command in the terminal (via keyboard shortcuts?) rather than manually writing the rest of yourself to type it out?\n",
            "(1210950940) ikonia: but i don't want to lose you on time again\n",
            "(1210950940) ikonia: i am going to give you the exact same solution\n",
            "(1210950940) LjL:  i am going to have to give you the exact same solution\n",
            "(1210950940) LjL: i wanted to know where the problem came from?\n",
            "(1210950940) ikonia: i don't know but the solution was the wrong way for a few days\n",
            "(1210950940) LjL:  the solution is the wrong way for a few days\n",
            "(1210950940) LjL: but i cannot tell you if there are any solutions\n",
            "(1210950940) LjL: it would be nice if you said yes to this one and i don't have to leave again for another day\n",
            "(1210950940) LjL: and i'm not going to let that happen again\n",
            "(1210950940) LjL: because i have a very large problem and i don't want to go to a hospital today\n",
            "(1217921060) LjL: but i will give you the same solution if you are available in a moment\n",
            "(1217921060) ikonia: i will try in the next couple of days\n",
            "(1217921060) LjL: but the problem came too late for me\n",
            "(1217921060) LjL:  and i think i could have done it any number of times without even getting tired\n",
            "(1217921060) LjL: what is this issue? i had this problem with ubuntu and for a good 5 days when i did some small things i could not connect my phone to the server or whatever.\n",
            "(1217921060) ikonia: im at the end of my work day now\n",
            "(1217921060) LjL:  maybe you should try another windows machine for a better one, or another ubuntu machine and see if this is the best one, i understand the difficulties\n",
            "\n",
            "[1500 | 4787.73] loss=2.49 avg=2.39\n",
            "Saving checkpoint/run1/model-1501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXgqHg9mlMVQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7b2a1c07-6357-4898-8146-970743f14ac6"
      },
      "source": [
        "!PYTHONPATH=src ./train.py --dataset ubuntu-npz/ --sample_every=250 --learning_rate 0.0001 --stop_after 3501 --model_name 345M"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:73: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:76: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-10-12 18:28:24.248411: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-10-12 18:28:24.248724: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x17d5480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-12 18:28:24.248761: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-10-12 18:28:24.259384: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-10-12 18:28:24.357657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 18:28:24.358861: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6f7d180 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-12 18:28:24.358896: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-10-12 18:28:24.359128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 18:28:24.360216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-12 18:28:24.365550: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-12 18:28:24.387836: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-12 18:28:24.398895: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-12 18:28:24.409560: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-12 18:28:24.427988: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-12 18:28:24.439841: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-12 18:28:24.459486: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-12 18:28:24.459660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 18:28:24.460482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 18:28:24.461130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-12 18:28:24.461203: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-12 18:28:24.462757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-10-12 18:28:24.462792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-10-12 18:28:24.462811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-10-12 18:28:24.463034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 18:28:24.463785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 18:28:24.464557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From ./train.py:77: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From ./train.py:100: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:113: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:89: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:120: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:122: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:125: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:129: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "Loading checkpoint checkpoint/run1/model-1501\n",
            "Loading dataset...\n",
            "100% 8/8 [00:08<00:00,  1.05s/it]\n",
            "dataset has 233929304 tokens\n",
            "Training...\n",
            "2019-10-12 18:29:30.979806: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1502 | 14.34] loss=2.89 avg=2.89\n",
            "[1503 | 17.39] loss=2.26 avg=2.57\n",
            "[1504 | 20.43] loss=2.25 avg=2.46\n",
            "[1505 | 23.49] loss=2.16 avg=2.39\n",
            "[1506 | 26.54] loss=2.84 avg=2.48\n",
            "[1507 | 29.60] loss=2.41 avg=2.47\n",
            "[1508 | 32.64] loss=2.52 avg=2.47\n",
            "[1509 | 35.70] loss=2.41 avg=2.47\n",
            "[1510 | 38.75] loss=2.44 avg=2.46\n",
            "[1511 | 41.80] loss=2.64 avg=2.48\n",
            "[1512 | 44.88] loss=2.21 avg=2.46\n",
            "[1513 | 47.94] loss=2.46 avg=2.46\n",
            "[1514 | 51.00] loss=2.46 avg=2.46\n",
            "[1515 | 54.09] loss=1.99 avg=2.42\n",
            "[1516 | 57.17] loss=2.22 avg=2.41\n",
            "[1517 | 60.26] loss=2.37 avg=2.40\n",
            "[1518 | 63.34] loss=2.47 avg=2.41\n",
            "[1519 | 66.41] loss=2.41 avg=2.41\n",
            "[1520 | 69.50] loss=2.23 avg=2.40\n",
            "[1521 | 72.56] loss=2.81 avg=2.42\n",
            "[1522 | 75.64] loss=2.60 avg=2.43\n",
            "[1523 | 78.72] loss=2.36 avg=2.43\n",
            "[1524 | 81.82] loss=2.77 avg=2.44\n",
            "[1525 | 84.93] loss=2.95 avg=2.47\n",
            "[1526 | 88.03] loss=2.17 avg=2.45\n",
            "[1527 | 91.11] loss=2.34 avg=2.45\n",
            "[1528 | 94.20] loss=2.38 avg=2.45\n",
            "[1529 | 97.30] loss=2.48 avg=2.45\n",
            "[1530 | 100.39] loss=2.57 avg=2.45\n",
            "[1531 | 103.48] loss=2.33 avg=2.45\n",
            "[1532 | 106.57] loss=2.21 avg=2.44\n",
            "[1533 | 109.63] loss=2.75 avg=2.45\n",
            "[1534 | 112.73] loss=2.10 avg=2.44\n",
            "[1535 | 115.83] loss=2.52 avg=2.44\n",
            "[1536 | 118.93] loss=2.77 avg=2.45\n",
            "[1537 | 122.01] loss=2.56 avg=2.46\n",
            "[1538 | 125.10] loss=2.50 avg=2.46\n",
            "[1539 | 128.17] loss=2.45 avg=2.46\n",
            "[1540 | 131.26] loss=1.85 avg=2.44\n",
            "[1541 | 134.36] loss=2.35 avg=2.44\n",
            "[1542 | 137.44] loss=2.94 avg=2.45\n",
            "[1543 | 140.52] loss=2.88 avg=2.46\n",
            "[1544 | 143.61] loss=2.02 avg=2.45\n",
            "[1545 | 146.68] loss=2.66 avg=2.46\n",
            "[1546 | 149.76] loss=2.65 avg=2.46\n",
            "[1547 | 152.84] loss=2.62 avg=2.47\n",
            "[1548 | 155.92] loss=2.68 avg=2.47\n",
            "[1549 | 159.01] loss=2.90 avg=2.48\n",
            "[1550 | 162.11] loss=2.50 avg=2.48\n",
            "[1551 | 165.21] loss=2.10 avg=2.47\n",
            "[1552 | 168.31] loss=2.32 avg=2.47\n",
            "[1553 | 171.40] loss=2.21 avg=2.46\n",
            "[1554 | 174.49] loss=2.26 avg=2.46\n",
            "[1555 | 177.58] loss=2.69 avg=2.46\n",
            "[1556 | 180.66] loss=2.07 avg=2.45\n",
            "[1557 | 183.75] loss=2.35 avg=2.45\n",
            "[1558 | 186.84] loss=2.31 avg=2.45\n",
            "[1559 | 189.92] loss=2.33 avg=2.45\n",
            "[1560 | 193.02] loss=2.34 avg=2.44\n",
            "[1561 | 196.12] loss=2.74 avg=2.45\n",
            "[1562 | 199.20] loss=2.57 avg=2.45\n",
            "[1563 | 202.31] loss=2.38 avg=2.45\n",
            "[1564 | 205.39] loss=2.26 avg=2.45\n",
            "[1565 | 208.46] loss=2.74 avg=2.45\n",
            "[1566 | 211.54] loss=2.39 avg=2.45\n",
            "[1567 | 214.61] loss=2.30 avg=2.45\n",
            "[1568 | 217.68] loss=2.41 avg=2.45\n",
            "[1569 | 220.76] loss=2.39 avg=2.45\n",
            "[1570 | 223.81] loss=2.16 avg=2.44\n",
            "[1571 | 226.88] loss=2.96 avg=2.45\n",
            "[1572 | 229.93] loss=2.38 avg=2.45\n",
            "[1573 | 232.99] loss=2.85 avg=2.46\n",
            "[1574 | 236.05] loss=2.21 avg=2.45\n",
            "[1575 | 239.11] loss=1.94 avg=2.44\n",
            "[1576 | 242.17] loss=2.22 avg=2.44\n",
            "[1577 | 245.22] loss=2.75 avg=2.44\n",
            "[1578 | 248.26] loss=2.33 avg=2.44\n",
            "[1579 | 251.33] loss=2.32 avg=2.44\n",
            "[1580 | 254.37] loss=1.78 avg=2.43\n",
            "[1581 | 257.42] loss=2.69 avg=2.43\n",
            "[1582 | 260.47] loss=2.79 avg=2.44\n",
            "[1583 | 263.53] loss=2.53 avg=2.44\n",
            "[1584 | 266.56] loss=2.43 avg=2.44\n",
            "[1585 | 269.61] loss=2.48 avg=2.44\n",
            "[1586 | 272.64] loss=2.66 avg=2.44\n",
            "[1587 | 275.70] loss=2.62 avg=2.45\n",
            "[1588 | 278.76] loss=2.20 avg=2.44\n",
            "[1589 | 281.80] loss=2.81 avg=2.45\n",
            "[1590 | 284.86] loss=2.24 avg=2.45\n",
            "[1591 | 287.90] loss=2.85 avg=2.45\n",
            "[1592 | 290.94] loss=3.63 avg=2.47\n",
            "[1593 | 293.99] loss=2.30 avg=2.47\n",
            "[1594 | 297.05] loss=2.29 avg=2.47\n",
            "[1595 | 300.11] loss=2.54 avg=2.47\n",
            "[1596 | 303.17] loss=2.29 avg=2.47\n",
            "[1597 | 306.23] loss=1.94 avg=2.46\n",
            "[1598 | 309.27] loss=2.44 avg=2.46\n",
            "[1599 | 312.32] loss=2.24 avg=2.45\n",
            "[1600 | 315.37] loss=2.16 avg=2.45\n",
            "[1601 | 318.42] loss=1.87 avg=2.44\n",
            "[1602 | 321.46] loss=2.46 avg=2.44\n",
            "[1603 | 324.50] loss=2.27 avg=2.44\n",
            "[1604 | 327.56] loss=3.01 avg=2.45\n",
            "[1605 | 330.60] loss=2.09 avg=2.44\n",
            "[1606 | 333.64] loss=2.27 avg=2.44\n",
            "[1607 | 336.68] loss=1.82 avg=2.43\n",
            "[1608 | 339.70] loss=2.85 avg=2.43\n",
            "[1609 | 342.75] loss=2.39 avg=2.43\n",
            "[1610 | 345.79] loss=2.43 avg=2.43\n",
            "[1611 | 348.84] loss=2.51 avg=2.44\n",
            "[1612 | 351.87] loss=2.38 avg=2.43\n",
            "[1613 | 354.92] loss=1.89 avg=2.43\n",
            "[1614 | 357.97] loss=2.47 avg=2.43\n",
            "[1615 | 361.00] loss=1.80 avg=2.42\n",
            "[1616 | 364.02] loss=2.53 avg=2.42\n",
            "[1617 | 367.05] loss=2.78 avg=2.42\n",
            "[1618 | 370.09] loss=2.02 avg=2.42\n",
            "[1619 | 373.13] loss=2.58 avg=2.42\n",
            "[1620 | 376.17] loss=2.72 avg=2.43\n",
            "[1621 | 379.20] loss=2.19 avg=2.42\n",
            "[1622 | 382.24] loss=2.83 avg=2.43\n",
            "[1623 | 385.26] loss=2.45 avg=2.43\n",
            "[1624 | 388.32] loss=2.19 avg=2.42\n",
            "[1625 | 391.38] loss=2.87 avg=2.43\n",
            "[1626 | 394.41] loss=2.48 avg=2.43\n",
            "[1627 | 397.46] loss=2.10 avg=2.43\n",
            "[1628 | 400.51] loss=2.31 avg=2.43\n",
            "[1629 | 403.55] loss=2.65 avg=2.43\n",
            "[1630 | 406.61] loss=2.21 avg=2.43\n",
            "[1631 | 409.67] loss=2.03 avg=2.42\n",
            "[1632 | 412.73] loss=2.30 avg=2.42\n",
            "[1633 | 415.78] loss=2.25 avg=2.42\n",
            "[1634 | 418.85] loss=2.44 avg=2.42\n",
            "[1635 | 421.90] loss=2.49 avg=2.42\n",
            "[1636 | 424.94] loss=2.25 avg=2.42\n",
            "[1637 | 428.01] loss=2.66 avg=2.42\n",
            "[1638 | 431.06] loss=2.44 avg=2.42\n",
            "[1639 | 434.11] loss=2.52 avg=2.42\n",
            "[1640 | 437.17] loss=2.41 avg=2.42\n",
            "[1641 | 440.20] loss=2.53 avg=2.42\n",
            "[1642 | 443.26] loss=2.94 avg=2.43\n",
            "[1643 | 446.30] loss=2.61 avg=2.43\n",
            "[1644 | 449.33] loss=2.01 avg=2.43\n",
            "[1645 | 452.37] loss=2.54 avg=2.43\n",
            "[1646 | 455.43] loss=2.38 avg=2.43\n",
            "[1647 | 458.48] loss=2.37 avg=2.43\n",
            "[1648 | 461.53] loss=2.26 avg=2.42\n",
            "[1649 | 464.59] loss=2.20 avg=2.42\n",
            "[1650 | 467.65] loss=2.27 avg=2.42\n",
            "[1651 | 470.71] loss=2.31 avg=2.42\n",
            "[1652 | 473.77] loss=1.65 avg=2.41\n",
            "[1653 | 476.82] loss=2.76 avg=2.41\n",
            "[1654 | 479.89] loss=2.61 avg=2.41\n",
            "[1655 | 482.95] loss=2.51 avg=2.42\n",
            "[1656 | 485.99] loss=2.26 avg=2.41\n",
            "[1657 | 489.05] loss=2.44 avg=2.41\n",
            "[1658 | 492.10] loss=2.29 avg=2.41\n",
            "[1659 | 495.16] loss=1.88 avg=2.41\n",
            "[1660 | 498.21] loss=2.98 avg=2.41\n",
            "[1661 | 501.25] loss=2.28 avg=2.41\n",
            "[1662 | 504.30] loss=2.30 avg=2.41\n",
            "[1663 | 507.34] loss=2.20 avg=2.41\n",
            "[1664 | 510.40] loss=2.22 avg=2.40\n",
            "[1665 | 513.44] loss=2.54 avg=2.41\n",
            "[1666 | 516.49] loss=2.43 avg=2.41\n",
            "[1667 | 519.53] loss=2.71 avg=2.41\n",
            "[1668 | 522.55] loss=2.60 avg=2.41\n",
            "[1669 | 525.59] loss=2.55 avg=2.41\n",
            "[1670 | 528.65] loss=2.69 avg=2.42\n",
            "[1671 | 531.70] loss=2.78 avg=2.42\n",
            "[1672 | 534.75] loss=2.19 avg=2.42\n",
            "[1673 | 537.78] loss=2.31 avg=2.42\n",
            "[1674 | 540.83] loss=2.50 avg=2.42\n",
            "[1675 | 543.87] loss=2.90 avg=2.42\n",
            "[1676 | 546.91] loss=2.87 avg=2.43\n",
            "[1677 | 549.96] loss=2.86 avg=2.44\n",
            "[1678 | 553.00] loss=2.52 avg=2.44\n",
            "[1679 | 556.05] loss=2.68 avg=2.44\n",
            "[1680 | 559.09] loss=2.04 avg=2.43\n",
            "[1681 | 562.13] loss=2.37 avg=2.43\n",
            "[1682 | 565.17] loss=2.89 avg=2.44\n",
            "[1683 | 568.21] loss=2.63 avg=2.44\n",
            "[1684 | 571.25] loss=2.35 avg=2.44\n",
            "[1685 | 574.27] loss=2.38 avg=2.44\n",
            "[1686 | 577.31] loss=2.27 avg=2.44\n",
            "[1687 | 580.37] loss=2.31 avg=2.44\n",
            "[1688 | 583.42] loss=2.22 avg=2.43\n",
            "[1689 | 586.47] loss=2.28 avg=2.43\n",
            "[1690 | 589.51] loss=2.09 avg=2.43\n",
            "[1691 | 592.55] loss=1.93 avg=2.42\n",
            "[1692 | 595.61] loss=1.96 avg=2.42\n",
            "[1693 | 598.68] loss=2.49 avg=2.42\n",
            "[1694 | 601.73] loss=1.91 avg=2.41\n",
            "[1695 | 604.76] loss=2.21 avg=2.41\n",
            "[1696 | 607.79] loss=1.95 avg=2.40\n",
            "[1697 | 610.82] loss=2.38 avg=2.40\n",
            "[1698 | 613.84] loss=2.24 avg=2.40\n",
            "[1699 | 616.88] loss=2.49 avg=2.40\n",
            "[1700 | 619.91] loss=2.85 avg=2.41\n",
            "[1701 | 622.96] loss=2.96 avg=2.41\n",
            "[1702 | 626.02] loss=2.77 avg=2.42\n",
            "[1703 | 629.07] loss=2.45 avg=2.42\n",
            "[1704 | 632.10] loss=2.59 avg=2.42\n",
            "[1705 | 635.13] loss=2.17 avg=2.42\n",
            "[1706 | 638.18] loss=2.36 avg=2.42\n",
            "[1707 | 641.22] loss=2.42 avg=2.42\n",
            "[1708 | 644.26] loss=2.21 avg=2.41\n",
            "[1709 | 647.31] loss=2.55 avg=2.42\n",
            "[1710 | 650.35] loss=2.44 avg=2.42\n",
            "[1711 | 653.39] loss=2.31 avg=2.42\n",
            "[1712 | 656.44] loss=2.31 avg=2.41\n",
            "[1713 | 659.49] loss=2.41 avg=2.41\n",
            "[1714 | 662.54] loss=2.86 avg=2.42\n",
            "[1715 | 665.58] loss=2.26 avg=2.42\n",
            "[1716 | 668.62] loss=2.19 avg=2.41\n",
            "[1717 | 671.67] loss=2.36 avg=2.41\n",
            "[1718 | 674.71] loss=2.26 avg=2.41\n",
            "[1719 | 677.78] loss=2.22 avg=2.41\n",
            "[1720 | 680.81] loss=2.52 avg=2.41\n",
            "[1721 | 683.86] loss=2.21 avg=2.41\n",
            "[1722 | 686.92] loss=2.33 avg=2.41\n",
            "[1723 | 689.97] loss=2.34 avg=2.41\n",
            "[1724 | 693.01] loss=2.56 avg=2.41\n",
            "[1725 | 696.07] loss=2.32 avg=2.41\n",
            "[1726 | 699.12] loss=2.03 avg=2.40\n",
            "[1727 | 702.15] loss=2.20 avg=2.40\n",
            "[1728 | 705.21] loss=2.82 avg=2.41\n",
            "[1729 | 708.23] loss=2.54 avg=2.41\n",
            "[1730 | 711.27] loss=2.43 avg=2.41\n",
            "[1731 | 714.31] loss=1.88 avg=2.40\n",
            "[1732 | 717.36] loss=2.29 avg=2.40\n",
            "[1733 | 720.39] loss=2.09 avg=2.40\n",
            "[1734 | 723.42] loss=2.66 avg=2.40\n",
            "[1735 | 726.44] loss=2.61 avg=2.40\n",
            "[1736 | 729.48] loss=2.15 avg=2.40\n",
            "[1737 | 732.51] loss=2.91 avg=2.41\n",
            "[1738 | 735.55] loss=2.54 avg=2.41\n",
            "[1739 | 738.57] loss=2.41 avg=2.41\n",
            "[1740 | 741.59] loss=2.31 avg=2.41\n",
            "[1741 | 744.63] loss=1.90 avg=2.40\n",
            "[1742 | 747.70] loss=2.04 avg=2.40\n",
            "[1743 | 750.75] loss=2.39 avg=2.40\n",
            "[1744 | 753.79] loss=2.36 avg=2.40\n",
            "[1745 | 756.85] loss=2.33 avg=2.40\n",
            "[1746 | 759.90] loss=2.24 avg=2.39\n",
            "[1747 | 762.95] loss=1.94 avg=2.39\n",
            "[1748 | 766.00] loss=2.49 avg=2.39\n",
            "[1749 | 769.05] loss=2.47 avg=2.39\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " some problems, for some reason i cant get the kernel 3.0 to load anymore. i tried it in an earlier kernel version, with no result.\n",
            "(1222488360) fiesty: i didn't change the boot parameter, i used a special-key combination\n",
            "(1222488360) jrib: the problem is with the kernel... if you have rebooted with it, you should be fine\n",
            "(1222489080) grawlix: any suggestion that i can get on fixing it? :)\n",
            "\n",
            "\n",
            "(1262666120) n3nd3n: Hey guys. Is there a way to get all the files in an archive to appear in our gnome panel?\n",
            "(1262666220) vox9:  do you believe the icons are important that can get misplaced after changes?\n",
            "(1262666280) n3nd3n: Not that I can see. :)\n",
            "(1262666400) vox9:  that would be like removing the gnome theme\n",
            "(1262666800) n3nd3n: I just want to add them to my panel. :)\n",
            "(1262656600) vox9:  so, it doesnt matter if the files are in the right place, but i dont see the way it could be possible but that there cant be a way to get the gnome logo off of the panel. :/\n",
            "(1262666740) n3nd3n: I don't know what I'm doing with the files...\n",
            "(1262667700) vox9:  so, how can I get the gnome logo off the panel?     ahh, okayy?\n",
            "(1262667820) vox9:     or just add the files to a folder in the panel\n",
            "(1262658080) n3nd3n: I'm looking into all the options...\n",
            "(1262667920) n3nd3n: I don't want to mess with the gnome panel so much...\n",
            "(1262579440) n3nd3n: I'm running a web server...\n",
            "(1278088380) vox9:       not sure\n",
            "(1278088500) vox9:  i guess you want to remove it\n",
            "(1278088500) vox9:                                                                                                   ubuntu  linux  ubuntu  ubuntu  ubuntu  ubuntu  ubuntu\n",
            "(1278088760) vox9:                                                                                            greek\n",
            "(1278088720) n3nd3n: I think there is.\n",
            "(1278088720) avatarskull: in which folder?\n",
            "(1278088780) hmw:  install a package in /etc/apt/sources.list\n",
            "(1278088800) hmw:  then read a few lines...        but most of its                                                                                                                                                 Ubuntu  ubuntu  ubuntu\n",
            "(1278088960) hmw: what are you looking to do?            ?                                      \n",
            "\n",
            "[1750 | 811.94] loss=2.16 avg=2.39\n",
            "[1751 | 814.97] loss=2.03 avg=2.38\n",
            "[1752 | 818.02] loss=2.75 avg=2.39\n",
            "[1753 | 821.07] loss=2.85 avg=2.39\n",
            "[1754 | 824.11] loss=2.26 avg=2.39\n",
            "[1755 | 827.13] loss=2.45 avg=2.39\n",
            "[1756 | 830.18] loss=2.92 avg=2.40\n",
            "[1757 | 833.21] loss=2.72 avg=2.40\n",
            "[1758 | 836.25] loss=2.77 avg=2.41\n",
            "[1759 | 839.28] loss=2.20 avg=2.40\n",
            "[1760 | 842.33] loss=2.12 avg=2.40\n",
            "[1761 | 845.39] loss=2.13 avg=2.40\n",
            "[1762 | 848.44] loss=2.22 avg=2.40\n",
            "[1763 | 851.47] loss=2.36 avg=2.40\n",
            "[1764 | 854.52] loss=2.08 avg=2.39\n",
            "[1765 | 857.57] loss=2.73 avg=2.40\n",
            "[1766 | 860.71] loss=2.06 avg=2.39\n",
            "[1767 | 863.78] loss=2.03 avg=2.39\n",
            "[1768 | 866.81] loss=2.09 avg=2.38\n",
            "[1769 | 869.87] loss=2.34 avg=2.38\n",
            "[1770 | 872.94] loss=2.09 avg=2.38\n",
            "[1771 | 875.99] loss=2.73 avg=2.38\n",
            "[1772 | 879.06] loss=2.71 avg=2.39\n",
            "[1773 | 882.11] loss=2.05 avg=2.38\n",
            "[1774 | 885.18] loss=2.10 avg=2.38\n",
            "[1775 | 888.23] loss=2.85 avg=2.39\n",
            "[1776 | 891.28] loss=2.34 avg=2.39\n",
            "[1777 | 894.33] loss=2.20 avg=2.38\n",
            "[1778 | 897.39] loss=1.97 avg=2.38\n",
            "[1779 | 900.42] loss=2.50 avg=2.38\n",
            "[1780 | 903.48] loss=2.16 avg=2.38\n",
            "[1781 | 906.52] loss=2.43 avg=2.38\n",
            "[1782 | 909.57] loss=1.90 avg=2.37\n",
            "[1783 | 912.63] loss=2.27 avg=2.37\n",
            "[1784 | 915.69] loss=2.25 avg=2.37\n",
            "[1785 | 918.72] loss=2.30 avg=2.37\n",
            "[1786 | 921.77] loss=2.36 avg=2.37\n",
            "[1787 | 924.82] loss=2.35 avg=2.37\n",
            "[1788 | 927.85] loss=2.26 avg=2.37\n",
            "[1789 | 930.92] loss=2.72 avg=2.37\n",
            "[1790 | 933.94] loss=1.93 avg=2.37\n",
            "[1791 | 936.99] loss=2.59 avg=2.37\n",
            "[1792 | 940.05] loss=2.00 avg=2.37\n",
            "[1793 | 943.10] loss=2.07 avg=2.36\n",
            "[1794 | 946.16] loss=2.31 avg=2.36\n",
            "[1795 | 949.20] loss=2.23 avg=2.36\n",
            "[1796 | 952.21] loss=2.21 avg=2.36\n",
            "[1797 | 955.26] loss=2.55 avg=2.36\n",
            "[1798 | 958.29] loss=2.76 avg=2.37\n",
            "[1799 | 961.33] loss=1.89 avg=2.36\n",
            "[1800 | 964.38] loss=2.30 avg=2.36\n",
            "[1801 | 967.44] loss=2.63 avg=2.36\n",
            "[1802 | 970.48] loss=1.87 avg=2.36\n",
            "[1803 | 973.51] loss=2.51 avg=2.36\n",
            "[1804 | 976.56] loss=2.84 avg=2.36\n",
            "[1805 | 979.60] loss=2.14 avg=2.36\n",
            "[1806 | 982.62] loss=2.05 avg=2.36\n",
            "[1807 | 985.65] loss=2.26 avg=2.36\n",
            "[1808 | 988.68] loss=2.11 avg=2.36\n",
            "[1809 | 991.73] loss=2.12 avg=2.35\n",
            "[1810 | 994.75] loss=2.32 avg=2.35\n",
            "[1811 | 997.78] loss=1.88 avg=2.35\n",
            "[1812 | 1000.82] loss=2.02 avg=2.34\n",
            "[1813 | 1003.87] loss=2.54 avg=2.35\n",
            "[1814 | 1006.92] loss=2.42 avg=2.35\n",
            "[1815 | 1009.98] loss=2.20 avg=2.35\n",
            "[1816 | 1013.02] loss=1.84 avg=2.34\n",
            "[1817 | 1016.06] loss=2.33 avg=2.34\n",
            "[1818 | 1019.09] loss=2.20 avg=2.34\n",
            "[1819 | 1022.13] loss=2.74 avg=2.34\n",
            "[1820 | 1025.17] loss=2.07 avg=2.34\n",
            "[1821 | 1028.22] loss=2.28 avg=2.34\n",
            "[1822 | 1031.27] loss=2.03 avg=2.34\n",
            "[1823 | 1034.32] loss=2.32 avg=2.34\n",
            "[1824 | 1037.37] loss=2.02 avg=2.33\n",
            "[1825 | 1040.41] loss=3.01 avg=2.34\n",
            "[1826 | 1043.45] loss=1.68 avg=2.33\n",
            "[1827 | 1046.50] loss=2.28 avg=2.33\n",
            "[1828 | 1049.54] loss=2.14 avg=2.33\n",
            "[1829 | 1052.59] loss=2.41 avg=2.33\n",
            "[1830 | 1055.63] loss=2.36 avg=2.33\n",
            "[1831 | 1058.69] loss=1.83 avg=2.33\n",
            "[1832 | 1061.72] loss=2.27 avg=2.33\n",
            "[1833 | 1064.78] loss=2.53 avg=2.33\n",
            "[1834 | 1067.81] loss=2.30 avg=2.33\n",
            "[1835 | 1070.87] loss=2.11 avg=2.33\n",
            "[1836 | 1073.94] loss=2.76 avg=2.33\n",
            "[1837 | 1076.99] loss=2.35 avg=2.33\n",
            "[1838 | 1080.03] loss=2.24 avg=2.33\n",
            "[1839 | 1083.09] loss=2.64 avg=2.33\n",
            "[1840 | 1086.14] loss=2.68 avg=2.34\n",
            "[1841 | 1089.18] loss=2.14 avg=2.33\n",
            "[1842 | 1092.22] loss=2.16 avg=2.33\n",
            "[1843 | 1095.28] loss=2.10 avg=2.33\n",
            "[1844 | 1098.33] loss=2.25 avg=2.33\n",
            "[1845 | 1101.37] loss=2.42 avg=2.33\n",
            "[1846 | 1104.42] loss=2.02 avg=2.33\n",
            "[1847 | 1107.47] loss=2.74 avg=2.33\n",
            "[1848 | 1110.51] loss=2.27 avg=2.33\n",
            "[1849 | 1113.56] loss=2.07 avg=2.33\n",
            "[1850 | 1116.63] loss=2.51 avg=2.33\n",
            "[1851 | 1119.69] loss=2.18 avg=2.33\n",
            "[1852 | 1122.74] loss=2.33 avg=2.33\n",
            "[1853 | 1125.78] loss=2.30 avg=2.33\n",
            "[1854 | 1128.83] loss=2.47 avg=2.33\n",
            "[1855 | 1131.87] loss=2.17 avg=2.33\n",
            "[1856 | 1134.91] loss=2.10 avg=2.33\n",
            "[1857 | 1137.97] loss=2.82 avg=2.33\n",
            "[1858 | 1141.02] loss=2.29 avg=2.33\n",
            "[1859 | 1144.08] loss=1.89 avg=2.33\n",
            "[1860 | 1147.12] loss=2.03 avg=2.32\n",
            "[1861 | 1150.16] loss=2.45 avg=2.32\n",
            "[1862 | 1153.20] loss=2.57 avg=2.33\n",
            "[1863 | 1156.26] loss=2.31 avg=2.33\n",
            "[1864 | 1159.29] loss=1.92 avg=2.32\n",
            "[1865 | 1162.34] loss=2.24 avg=2.32\n",
            "[1866 | 1165.40] loss=2.08 avg=2.32\n",
            "[1867 | 1168.46] loss=2.06 avg=2.32\n",
            "[1868 | 1171.50] loss=2.08 avg=2.31\n",
            "[1869 | 1174.55] loss=2.28 avg=2.31\n",
            "[1870 | 1177.60] loss=2.30 avg=2.31\n",
            "[1871 | 1180.64] loss=2.53 avg=2.32\n",
            "[1872 | 1183.70] loss=2.29 avg=2.31\n",
            "[1873 | 1186.76] loss=2.28 avg=2.31\n",
            "[1874 | 1189.81] loss=2.27 avg=2.31\n",
            "[1875 | 1192.85] loss=2.26 avg=2.31\n",
            "[1876 | 1195.90] loss=2.40 avg=2.31\n",
            "[1877 | 1198.95] loss=2.57 avg=2.32\n",
            "[1878 | 1202.01] loss=2.50 avg=2.32\n",
            "[1879 | 1205.05] loss=2.23 avg=2.32\n",
            "[1880 | 1208.09] loss=2.25 avg=2.32\n",
            "[1881 | 1211.14] loss=2.00 avg=2.31\n",
            "[1882 | 1214.20] loss=2.51 avg=2.32\n",
            "[1883 | 1217.26] loss=2.15 avg=2.31\n",
            "[1884 | 1220.32] loss=2.46 avg=2.32\n",
            "[1885 | 1223.35] loss=2.07 avg=2.31\n",
            "[1886 | 1226.41] loss=2.45 avg=2.31\n",
            "[1887 | 1229.47] loss=2.68 avg=2.32\n",
            "[1888 | 1232.53] loss=2.01 avg=2.32\n",
            "[1889 | 1235.59] loss=2.85 avg=2.32\n",
            "[1890 | 1238.62] loss=2.35 avg=2.32\n",
            "[1891 | 1241.65] loss=2.47 avg=2.32\n",
            "[1892 | 1244.70] loss=2.47 avg=2.32\n",
            "[1893 | 1247.74] loss=2.80 avg=2.33\n",
            "[1894 | 1250.80] loss=2.52 avg=2.33\n",
            "[1895 | 1253.86] loss=2.42 avg=2.33\n",
            "[1896 | 1256.93] loss=2.43 avg=2.33\n",
            "[1897 | 1259.96] loss=2.26 avg=2.33\n",
            "[1898 | 1263.01] loss=2.86 avg=2.34\n",
            "[1899 | 1266.07] loss=2.56 avg=2.34\n",
            "[1900 | 1269.11] loss=2.10 avg=2.34\n",
            "[1901 | 1272.16] loss=1.96 avg=2.33\n",
            "[1902 | 1275.22] loss=2.13 avg=2.33\n",
            "[1903 | 1278.26] loss=2.07 avg=2.33\n",
            "[1904 | 1281.32] loss=2.34 avg=2.33\n",
            "[1905 | 1284.37] loss=2.06 avg=2.33\n",
            "[1906 | 1287.42] loss=2.33 avg=2.33\n",
            "[1907 | 1290.45] loss=2.32 avg=2.33\n",
            "[1908 | 1293.52] loss=2.70 avg=2.33\n",
            "[1909 | 1296.58] loss=2.54 avg=2.33\n",
            "[1910 | 1299.65] loss=2.46 avg=2.33\n",
            "[1911 | 1302.71] loss=2.34 avg=2.33\n",
            "[1912 | 1305.77] loss=2.15 avg=2.33\n",
            "[1913 | 1308.84] loss=2.05 avg=2.33\n",
            "[1914 | 1311.88] loss=2.42 avg=2.33\n",
            "[1915 | 1314.94] loss=2.70 avg=2.33\n",
            "[1916 | 1318.01] loss=2.10 avg=2.33\n",
            "[1917 | 1321.06] loss=2.88 avg=2.34\n",
            "[1918 | 1324.10] loss=2.07 avg=2.33\n",
            "[1919 | 1327.16] loss=2.66 avg=2.34\n",
            "[1920 | 1330.24] loss=2.40 avg=2.34\n",
            "[1921 | 1333.30] loss=2.91 avg=2.34\n",
            "[1922 | 1336.36] loss=2.71 avg=2.35\n",
            "[1923 | 1339.41] loss=1.35 avg=2.34\n",
            "[1924 | 1342.47] loss=2.53 avg=2.34\n",
            "[1925 | 1345.54] loss=2.23 avg=2.34\n",
            "[1926 | 1348.59] loss=2.35 avg=2.34\n",
            "[1927 | 1351.65] loss=2.60 avg=2.34\n",
            "[1928 | 1354.71] loss=3.48 avg=2.35\n",
            "[1929 | 1357.77] loss=2.30 avg=2.35\n",
            "[1930 | 1360.84] loss=2.29 avg=2.35\n",
            "[1931 | 1363.88] loss=2.34 avg=2.35\n",
            "[1932 | 1366.94] loss=2.57 avg=2.35\n",
            "[1933 | 1369.99] loss=2.87 avg=2.36\n",
            "[1934 | 1373.04] loss=2.52 avg=2.36\n",
            "[1935 | 1376.08] loss=2.13 avg=2.36\n",
            "[1936 | 1379.14] loss=2.38 avg=2.36\n",
            "[1937 | 1382.20] loss=2.44 avg=2.36\n",
            "[1938 | 1385.25] loss=2.78 avg=2.36\n",
            "[1939 | 1388.28] loss=2.36 avg=2.36\n",
            "[1940 | 1391.34] loss=2.68 avg=2.37\n",
            "[1941 | 1394.39] loss=2.73 avg=2.37\n",
            "[1942 | 1397.46] loss=2.23 avg=2.37\n",
            "[1943 | 1400.50] loss=2.12 avg=2.37\n",
            "[1944 | 1403.55] loss=2.06 avg=2.36\n",
            "[1945 | 1406.61] loss=2.41 avg=2.36\n",
            "[1946 | 1409.67] loss=2.38 avg=2.36\n",
            "[1947 | 1412.75] loss=2.59 avg=2.37\n",
            "[1948 | 1415.81] loss=1.99 avg=2.36\n",
            "[1949 | 1418.87] loss=2.10 avg=2.36\n",
            "[1950 | 1421.95] loss=2.53 avg=2.36\n",
            "[1951 | 1425.01] loss=2.01 avg=2.36\n",
            "[1952 | 1428.09] loss=2.26 avg=2.36\n",
            "[1953 | 1431.18] loss=2.06 avg=2.35\n",
            "[1954 | 1434.25] loss=2.36 avg=2.35\n",
            "[1955 | 1437.33] loss=2.07 avg=2.35\n",
            "[1956 | 1440.43] loss=2.24 avg=2.35\n",
            "[1957 | 1443.51] loss=2.39 avg=2.35\n",
            "[1958 | 1446.61] loss=2.47 avg=2.35\n",
            "[1959 | 1449.70] loss=2.30 avg=2.35\n",
            "[1960 | 1452.81] loss=2.12 avg=2.35\n",
            "[1961 | 1455.93] loss=2.80 avg=2.35\n",
            "[1962 | 1459.05] loss=1.94 avg=2.35\n",
            "[1963 | 1462.17] loss=2.61 avg=2.35\n",
            "[1964 | 1465.29] loss=2.41 avg=2.35\n",
            "[1965 | 1468.39] loss=2.87 avg=2.36\n",
            "[1966 | 1471.49] loss=1.99 avg=2.35\n",
            "[1967 | 1474.59] loss=2.20 avg=2.35\n",
            "[1968 | 1477.69] loss=2.09 avg=2.35\n",
            "[1969 | 1480.80] loss=2.77 avg=2.35\n",
            "[1970 | 1483.91] loss=2.56 avg=2.36\n",
            "[1971 | 1487.02] loss=2.37 avg=2.36\n",
            "[1972 | 1490.15] loss=2.37 avg=2.36\n",
            "[1973 | 1493.25] loss=2.52 avg=2.36\n",
            "[1974 | 1496.37] loss=1.93 avg=2.35\n",
            "[1975 | 1499.46] loss=2.42 avg=2.35\n",
            "[1976 | 1502.55] loss=2.46 avg=2.36\n",
            "[1977 | 1505.63] loss=2.34 avg=2.36\n",
            "[1978 | 1508.71] loss=2.04 avg=2.35\n",
            "[1979 | 1511.81] loss=2.85 avg=2.36\n",
            "[1980 | 1514.91] loss=2.69 avg=2.36\n",
            "[1981 | 1518.00] loss=2.28 avg=2.36\n",
            "[1982 | 1521.06] loss=2.63 avg=2.36\n",
            "[1983 | 1524.15] loss=2.27 avg=2.36\n",
            "[1984 | 1527.23] loss=2.12 avg=2.36\n",
            "[1985 | 1530.35] loss=2.34 avg=2.36\n",
            "[1986 | 1533.46] loss=2.20 avg=2.36\n",
            "[1987 | 1536.56] loss=1.86 avg=2.35\n",
            "[1988 | 1539.64] loss=2.29 avg=2.35\n",
            "[1989 | 1542.71] loss=2.48 avg=2.35\n",
            "[1990 | 1545.80] loss=2.83 avg=2.36\n",
            "[1991 | 1548.94] loss=1.91 avg=2.35\n",
            "[1992 | 1552.07] loss=2.07 avg=2.35\n",
            "[1993 | 1555.22] loss=2.37 avg=2.35\n",
            "[1994 | 1558.35] loss=2.10 avg=2.35\n",
            "[1995 | 1561.49] loss=2.47 avg=2.35\n",
            "[1996 | 1564.64] loss=2.22 avg=2.35\n",
            "[1997 | 1567.77] loss=2.25 avg=2.35\n",
            "[1998 | 1570.89] loss=2.65 avg=2.35\n",
            "[1999 | 1574.04] loss=1.68 avg=2.34\n",
            "Saving checkpoint/run1/model-2000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ") it wont even go to the main menu it just starts with booting up a bootable ubuntu cd\n",
            "(1245115540) n3r0: well... this is ubuntu... I havve a 7.04 cd... I have the 7.04 cd as well.\n",
            "(1245115600) fccf: what are you trying to do?\n",
            "(1245115660) n3r0: you dont have enough disk space to install live cds\n",
            "(1245115780) fccf: no, you need to install it\n",
            "(1245115840) n3r0: do you have a 7.04 cd... as you say... what can you do to you?\n",
            "(1245115900) fccf: yes... try a cd burn, try to install on a live cd... or if it doesnt work in grub... you may have too much space to make it work\n",
            "(1245116420) n3r0: you know anyhow, if not a 7.04 cd then i would say it is a 8.04 cd... which it does\n",
            "(1245116480) fccf: not a live cd. thats no live cd... i cant see that what u say but your pc booting from a live cd. is that live cd grudging you in grub?\n",
            "(1245116540) n3r0: well, the problem is you need to create a livecd with grub to install windows on ubuntu... what can you do to you?\n",
            "(1245116660) fccf: just install grub on it (installing grub on live cds), install windows from live cd and try to install grub on the live cd\n",
            "(1245116720) n3r0: well... grub live cd has grub grub. and grub grub2.\n",
            "(1245116780) fccf: how many times have you installed grub on a live cd?\n",
            "(1245116780) n3r0: ok well... not that many times, just what i would say... I had a live cd with grub on it but there were only grub install files on the cds... grub live cd is grub2, grub2. grub2. grubhd grubhd live cd grubhdgr\n",
            "(1245116200) n3r0: its ok grub2\n",
            "\n",
            "\n",
            "(1197691420) theadmin: how do i install ubuntu to a pnt with windows?\n",
            "(1197691480) theadmin: how do i install ubuntu to a pnt with windows\n",
            "(1197691540) theadministrator: did you update the repos and do updates?\n",
            "(1197691660) theadmin: it doesn't work\n",
            "(1197691720) theadmin: how do i install ubuntu to a pnt\n",
            "(1197691720) theadministrator: windows\n",
            "(1197691780) theadmin: windows 10\n",
            "(1197691780) theadmin: so\n",
            "(1197691780) theadmin: i would like it to open on my network\n",
            "(1197691780) theadmin: windows is running\n",
            "(1197691780) theadmin: yes\n",
            "(1197691780) theadmin: it opens on my network\n",
            "(1197691780) theadmin: yes\n",
            "(1197691780) theadmin: i want this to work for me\n",
            "(1197691780) theadmin: yes\n",
            "(1197691840) theadmin: yes\n",
            "(1197691840) theadmin: i want it to not work on a new ubuntu that i upgraded to\n",
            "(1197691960) theadmin: i want it to be on my network\n",
            "(1197691960) theadmin: yes\n",
            "(1197692100) theadmin: yes\n",
            "(1197692100) theadmin: i want it to run on my network\n",
            "(1197692100) theadmin: yes\n",
            "(1197692100) theadmin: yes\n",
            "(1197692100) theadministrator: i want to try and add everything\n",
            "(1197692100) theadministrator: and install it on my partition\n",
            "(1197692100) zykotick9: use parted for that\n",
            "\n",
            "\n",
            "(1209936480) rl_: it's in the ubuntu repositories\n",
            "(1209936840) rl_: what packages do you need from this repo?\n",
            "(1209936900) rl_: you can search and install the software with synaptic but it\n",
            "\n",
            "[2000 | 1623.73] loss=1.93 avg=2.34\n",
            "[2001 | 1626.88] loss=2.62 avg=2.34\n",
            "[2002 | 1630.04] loss=2.14 avg=2.34\n",
            "[2003 | 1633.17] loss=2.22 avg=2.34\n",
            "[2004 | 1636.32] loss=2.38 avg=2.34\n",
            "[2005 | 1639.44] loss=2.21 avg=2.34\n",
            "[2006 | 1642.60] loss=1.93 avg=2.33\n",
            "[2007 | 1645.74] loss=2.35 avg=2.33\n",
            "[2008 | 1648.86] loss=2.41 avg=2.33\n",
            "[2009 | 1652.00] loss=2.41 avg=2.34\n",
            "[2010 | 1655.12] loss=2.56 avg=2.34\n",
            "[2011 | 1658.24] loss=2.22 avg=2.34\n",
            "[2012 | 1661.37] loss=2.46 avg=2.34\n",
            "[2013 | 1664.49] loss=2.04 avg=2.33\n",
            "[2014 | 1667.58] loss=2.49 avg=2.34\n",
            "[2015 | 1670.68] loss=2.89 avg=2.34\n",
            "[2016 | 1673.78] loss=2.01 avg=2.34\n",
            "[2017 | 1676.89] loss=2.68 avg=2.34\n",
            "[2018 | 1680.01] loss=2.44 avg=2.34\n",
            "[2019 | 1683.12] loss=2.27 avg=2.34\n",
            "[2020 | 1686.22] loss=2.58 avg=2.34\n",
            "[2021 | 1689.31] loss=2.08 avg=2.34\n",
            "[2022 | 1692.41] loss=2.24 avg=2.34\n",
            "[2023 | 1695.49] loss=2.39 avg=2.34\n",
            "[2024 | 1698.63] loss=2.44 avg=2.34\n",
            "[2025 | 1701.75] loss=2.32 avg=2.34\n",
            "[2026 | 1704.85] loss=2.19 avg=2.34\n",
            "[2027 | 1707.93] loss=2.57 avg=2.34\n",
            "[2028 | 1711.05] loss=2.49 avg=2.34\n",
            "[2029 | 1714.14] loss=2.65 avg=2.35\n",
            "[2030 | 1717.25] loss=2.19 avg=2.35\n",
            "[2031 | 1720.36] loss=2.45 avg=2.35\n",
            "[2032 | 1723.49] loss=2.46 avg=2.35\n",
            "[2033 | 1726.58] loss=2.19 avg=2.35\n",
            "[2034 | 1729.67] loss=2.29 avg=2.35\n",
            "[2035 | 1732.83] loss=2.11 avg=2.34\n",
            "[2036 | 1735.93] loss=2.29 avg=2.34\n",
            "[2037 | 1739.03] loss=2.92 avg=2.35\n",
            "[2038 | 1742.15] loss=2.75 avg=2.35\n",
            "[2039 | 1745.33] loss=2.38 avg=2.35\n",
            "[2040 | 1748.46] loss=2.11 avg=2.35\n",
            "[2041 | 1751.63] loss=2.26 avg=2.35\n",
            "[2042 | 1754.77] loss=1.98 avg=2.35\n",
            "[2043 | 1757.94] loss=2.77 avg=2.35\n",
            "[2044 | 1761.09] loss=2.47 avg=2.35\n",
            "[2045 | 1764.21] loss=2.29 avg=2.35\n",
            "[2046 | 1767.35] loss=2.71 avg=2.35\n",
            "[2047 | 1770.50] loss=2.05 avg=2.35\n",
            "[2048 | 1773.66] loss=1.93 avg=2.35\n",
            "[2049 | 1776.78] loss=2.03 avg=2.34\n",
            "[2050 | 1779.91] loss=2.29 avg=2.34\n",
            "[2051 | 1783.05] loss=2.69 avg=2.35\n",
            "[2052 | 1786.16] loss=2.42 avg=2.35\n",
            "[2053 | 1789.29] loss=2.02 avg=2.34\n",
            "[2054 | 1792.42] loss=2.46 avg=2.35\n",
            "[2055 | 1795.56] loss=2.38 avg=2.35\n",
            "[2056 | 1798.69] loss=2.09 avg=2.34\n",
            "[2057 | 1801.80] loss=2.75 avg=2.35\n",
            "[2058 | 1804.92] loss=2.23 avg=2.35\n",
            "[2059 | 1808.05] loss=2.30 avg=2.35\n",
            "[2060 | 1811.19] loss=2.31 avg=2.35\n",
            "[2061 | 1814.33] loss=2.21 avg=2.34\n",
            "[2062 | 1817.45] loss=3.01 avg=2.35\n",
            "[2063 | 1820.59] loss=2.47 avg=2.35\n",
            "[2064 | 1823.70] loss=1.94 avg=2.35\n",
            "[2065 | 1826.82] loss=1.99 avg=2.34\n",
            "[2066 | 1829.94] loss=2.57 avg=2.35\n",
            "[2067 | 1833.07] loss=2.77 avg=2.35\n",
            "[2068 | 1836.21] loss=2.24 avg=2.35\n",
            "[2069 | 1839.33] loss=2.27 avg=2.35\n",
            "[2070 | 1842.45] loss=2.17 avg=2.35\n",
            "[2071 | 1845.55] loss=2.18 avg=2.35\n",
            "[2072 | 1848.68] loss=2.54 avg=2.35\n",
            "[2073 | 1851.79] loss=2.39 avg=2.35\n",
            "[2074 | 1854.90] loss=2.86 avg=2.35\n",
            "[2075 | 1858.02] loss=2.89 avg=2.36\n",
            "[2076 | 1861.15] loss=1.85 avg=2.35\n",
            "[2077 | 1864.28] loss=2.20 avg=2.35\n",
            "[2078 | 1867.39] loss=2.82 avg=2.36\n",
            "[2079 | 1870.52] loss=2.57 avg=2.36\n",
            "[2080 | 1873.64] loss=2.07 avg=2.36\n",
            "[2081 | 1876.75] loss=2.04 avg=2.35\n",
            "[2082 | 1879.87] loss=2.55 avg=2.35\n",
            "[2083 | 1882.98] loss=2.48 avg=2.36\n",
            "[2084 | 1886.09] loss=2.71 avg=2.36\n",
            "[2085 | 1889.20] loss=2.46 avg=2.36\n",
            "[2086 | 1892.32] loss=2.41 avg=2.36\n",
            "[2087 | 1895.44] loss=2.14 avg=2.36\n",
            "[2088 | 1898.57] loss=2.11 avg=2.36\n",
            "[2089 | 1901.68] loss=2.46 avg=2.36\n",
            "[2090 | 1904.80] loss=2.02 avg=2.35\n",
            "[2091 | 1907.93] loss=2.46 avg=2.35\n",
            "[2092 | 1911.07] loss=2.62 avg=2.36\n",
            "[2093 | 1914.25] loss=2.32 avg=2.36\n",
            "[2094 | 1917.43] loss=2.27 avg=2.36\n",
            "[2095 | 1920.61] loss=2.26 avg=2.36\n",
            "[2096 | 1923.77] loss=2.32 avg=2.35\n",
            "[2097 | 1926.90] loss=2.63 avg=2.36\n",
            "[2098 | 1930.11] loss=2.01 avg=2.35\n",
            "[2099 | 1933.26] loss=2.86 avg=2.36\n",
            "[2100 | 1936.41] loss=2.23 avg=2.36\n",
            "[2101 | 1939.53] loss=2.42 avg=2.36\n",
            "[2102 | 1942.71] loss=2.15 avg=2.36\n",
            "[2103 | 1945.89] loss=2.27 avg=2.36\n",
            "[2104 | 1949.09] loss=2.01 avg=2.35\n",
            "[2105 | 1952.26] loss=2.16 avg=2.35\n",
            "[2106 | 1955.43] loss=2.42 avg=2.35\n",
            "[2107 | 1958.59] loss=2.03 avg=2.35\n",
            "[2108 | 1961.74] loss=2.45 avg=2.35\n",
            "[2109 | 1964.89] loss=2.09 avg=2.35\n",
            "[2110 | 1968.05] loss=2.38 avg=2.35\n",
            "[2111 | 1971.19] loss=2.24 avg=2.35\n",
            "[2112 | 1974.33] loss=2.31 avg=2.35\n",
            "[2113 | 1977.48] loss=2.67 avg=2.35\n",
            "[2114 | 1980.63] loss=2.83 avg=2.35\n",
            "[2115 | 1983.74] loss=2.26 avg=2.35\n",
            "[2116 | 1986.89] loss=2.15 avg=2.35\n",
            "[2117 | 1990.02] loss=2.41 avg=2.35\n",
            "[2118 | 1993.17] loss=2.37 avg=2.35\n",
            "[2119 | 1996.32] loss=2.51 avg=2.35\n",
            "[2120 | 1999.47] loss=2.16 avg=2.35\n",
            "[2121 | 2002.59] loss=2.73 avg=2.35\n",
            "[2122 | 2005.73] loss=2.08 avg=2.35\n",
            "[2123 | 2008.87] loss=2.00 avg=2.35\n",
            "[2124 | 2012.00] loss=2.35 avg=2.35\n",
            "[2125 | 2015.14] loss=2.04 avg=2.35\n",
            "[2126 | 2018.25] loss=2.46 avg=2.35\n",
            "[2127 | 2021.45] loss=2.67 avg=2.35\n",
            "[2128 | 2024.59] loss=2.07 avg=2.35\n",
            "[2129 | 2027.73] loss=2.45 avg=2.35\n",
            "[2130 | 2030.85] loss=2.12 avg=2.35\n",
            "[2131 | 2033.97] loss=2.73 avg=2.35\n",
            "[2132 | 2037.08] loss=2.38 avg=2.35\n",
            "[2133 | 2040.20] loss=2.81 avg=2.35\n",
            "[2134 | 2043.31] loss=2.19 avg=2.35\n",
            "[2135 | 2046.44] loss=2.16 avg=2.35\n",
            "[2136 | 2049.57] loss=2.38 avg=2.35\n",
            "[2137 | 2052.72] loss=2.01 avg=2.35\n",
            "[2138 | 2055.85] loss=1.96 avg=2.34\n",
            "[2139 | 2058.94] loss=2.55 avg=2.35\n",
            "[2140 | 2062.06] loss=2.75 avg=2.35\n",
            "[2141 | 2065.18] loss=2.41 avg=2.35\n",
            "[2142 | 2068.33] loss=2.44 avg=2.35\n",
            "[2143 | 2071.45] loss=2.36 avg=2.35\n",
            "[2144 | 2074.60] loss=2.58 avg=2.35\n",
            "[2145 | 2077.71] loss=1.81 avg=2.35\n",
            "[2146 | 2080.86] loss=2.49 avg=2.35\n",
            "[2147 | 2084.01] loss=2.85 avg=2.35\n",
            "[2148 | 2087.10] loss=2.31 avg=2.35\n",
            "[2149 | 2090.23] loss=2.41 avg=2.35\n",
            "[2150 | 2093.35] loss=2.24 avg=2.35\n",
            "[2151 | 2096.43] loss=2.92 avg=2.36\n",
            "[2152 | 2099.51] loss=2.55 avg=2.36\n",
            "[2153 | 2102.59] loss=2.68 avg=2.36\n",
            "[2154 | 2105.70] loss=2.37 avg=2.36\n",
            "[2155 | 2108.79] loss=2.37 avg=2.36\n",
            "[2156 | 2111.85] loss=2.66 avg=2.37\n",
            "[2157 | 2114.89] loss=2.06 avg=2.36\n",
            "[2158 | 2117.94] loss=1.98 avg=2.36\n",
            "[2159 | 2120.99] loss=2.06 avg=2.36\n",
            "[2160 | 2124.03] loss=2.91 avg=2.36\n",
            "interrupted\n",
            "Saving checkpoint/run1/model-2161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hm1b7ctQnnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clean up old saves\n",
        "files = []\n",
        "iterations = []\n",
        "for p, d, f in os.walk('/content/gpt-2/checkpoint/'):\n",
        "    for file in f:\n",
        "        r = re.findall('model-(\\d+).', file)\n",
        "        if r:\n",
        "            iterations.append(int(r[0]))\n",
        "            files.append(f'{p}/{file}')\n",
        "\n",
        "\n",
        "last_saved = str(max(iterations))\n",
        "for f in files:\n",
        "  if last_saved not in f:\n",
        "    os.remove(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVFKTewkRw8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/345M/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj6i-KQoR2ve",
        "colab_type": "text"
      },
      "source": [
        "# **Unconditioned samples**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tulOrwANR0WU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bfbb85d7-038c-4065-a610-e1cdf0bef347"
      },
      "source": [
        "!python3 src/generate_unconditional_samples.py --top_k 40 --temperature 0.95 --model_name 345M"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From src/generate_unconditional_samples.py:49: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-10-12 19:05:26.807445: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-10-12 19:05:26.830474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:05:26.831307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-12 19:05:26.836292: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-12 19:05:26.846151: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-12 19:05:26.852067: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-12 19:05:26.860353: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-12 19:05:26.875443: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-12 19:05:26.883080: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-12 19:05:26.898178: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-12 19:05:26.898410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:05:26.899273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:05:26.899984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-12 19:05:26.923324: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-10-12 19:05:26.923807: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x188d480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-12 19:05:26.923839: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-10-12 19:05:26.977515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:05:26.978443: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x188d640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-12 19:05:26.978472: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-10-12 19:05:26.978677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:05:26.979360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-12 19:05:26.979449: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-12 19:05:26.979489: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-12 19:05:26.979532: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-12 19:05:26.979577: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-12 19:05:26.979619: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-12 19:05:26.979652: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-12 19:05:26.979687: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-12 19:05:26.979804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:05:26.980560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:05:26.981239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-12 19:05:26.981307: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-12 19:05:26.982848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-10-12 19:05:26.982882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-10-12 19:05:26.982901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-10-12 19:05:26.983098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:05:26.983891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:05:26.984583: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-10-12 19:05:26.984638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From src/generate_unconditional_samples.py:51: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From src/generate_unconditional_samples.py:60: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "2019-10-12 19:05:38.776339: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "======================================== SAMPLE 1 ========================================\n",
            "If you haven't read that article yet, go here: http://www.nvidia.com/support/glxgears/. The idea is to use the .xgl package for GLX.org graphics cards for Linux in order to load the GLX driver into your X server.\n",
            "(1259608980) lmh:  I am in a terminal window\n",
            "(1259608980) lmh: what is the current status of the GLX?  Is there error messages?  Is there something not working?\n",
            "(1259609040) lmh:  what's the current status of the GLX driver?\n",
            "(1259609040) LjL: not sure. Try to do a nvidia-glx-glx.sh /etc/X11/xorg.conf (or something similar) in the sudo shell and let me know the results\n",
            "(1259609100) LjL:  try to nvidia-glx-glx.sh /etc/X11/xorg.conf  I don't know much of the 'dummy' configuration process. (or just use /etc/passwd\n",
            "(1259609160) lmh: why do you want to load the GLX driver into your X server?  The X is loaded at boot time. I assume that you need to add the driver to the list and then run 'sudo dmesg |grep nvidia'\n",
            "(1259609160) LjL:  I think it's a 'feature' of the drivers, a bit like the graphical user interface.  I did find the package glxgears in synaptic, but it doesn't seem to be of much use.\n",
            "(1259609160) LjL:  http://www.nvidia.com/support/glxgears/wiki/index2.php (click on the link to open in a browser)\n",
            "(1259609160) LjL:  you might try /etc/update-alternatives/ [edit] [force]  just change the entry for your driver\n",
            "(1259609220) lmh:  does any of my X drivers work ok?\n",
            "(1259609220) lmh: I've been trying to get a solution to the problem since 6.06.1-1\n",
            "(1259609280) lmh:  I just want to have my X server and the drivers loaded correctly\n",
            "\n",
            "\n",
            "(1149394760) n9k: and can u help me get to that?\n",
            "(1149394760) n9k: i cant have it boot without linux?\n",
            "(1149394760) n9k: it's a usb drive\n",
            "\n",
            "\n",
            "(1212893540) B2_CAD: My sound card (in Ubuntu 9.04) doesn't work using ALSA. I need an external sound card, but that external one doesn't work. Also I'm using Ubuntu 9.10...\n",
            "(1212893600) B2_CAD: The sound card isn't in the mixer, the sound menu doesn't show up on my d-pad\n",
            "(1212893660) B2_CAD: No such thing is in GNOME\n",
            "(1212893660) B2_CAD: Can you give me an example of what happened?\n",
            "(1212893720) B2_CAD: When I boot for ubuntu in ubuntu 9.04, I don't have Sound Preferences installed\n",
            "(1212893720) B2_CAD: I'm trying to open Sound Preferences... but it doesn't appear as a tab on my desktop\n",
            "(1212893720) B2_CAD: I'm in ALSA\n",
            "(1212893720) B2_CAD: Not a sound mixer\n",
            "(1212893720) B2_CAD: I can't use ALSA in ubuntu 9.10\n",
            "(1212893720) B2_CAD: I'm not using ALSA myself\n",
            "\n",
            "\n",
            "(1335158040) yhooof: hi\n",
            "(1335158040) yhooof: how can i change the root password on the ubuntu partition\n",
            "(1335158040) yhooof: i can't make that happen in the system and in the partition is there any tool for it\n",
            "(1335158420) yhooof: can i use the root console for an ipod?\n",
            "(1335158420) yhooof: i can't change it using any console in there\n",
            "(1335158480) yhooof: i've been reading about it i guess\n",
            "\n",
            "\n",
            "(1195845200) Mertz\n",
            "Traceback (most recent call last):\n",
            "  File \"src/generate_unconditional_samples.py\", line 74, in <module>\n",
            "    fire.Fire(sample_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 138, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 471, in _Fire\n",
            "    target=component.__name__)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 675, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"src/generate_unconditional_samples.py\", line 66, in sample_model\n",
            "    out = sess.run(output)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 956, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1180, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n",
            "    target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Rtqtfw9SDLd",
        "colab_type": "text"
      },
      "source": [
        "# **QuestionAnswering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxcclsvISMQN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5df1d982-956e-4a2f-fe05-f2f35a4f362b"
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --model_name='345M'  --nsamples=2 --top_k=100 --temperature=1"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:52: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-10-12 19:07:01.050100: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-10-12 19:07:01.070449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:07:01.071289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-12 19:07:01.071577: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-12 19:07:01.073147: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-12 19:07:01.074634: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-12 19:07:01.075071: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-12 19:07:01.076822: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-12 19:07:01.077983: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-12 19:07:01.081560: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-12 19:07:01.081693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:07:01.082560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:07:01.083277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-12 19:07:01.089090: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-10-12 19:07:01.089403: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2f23640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-12 19:07:01.089440: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-10-12 19:07:01.149480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:07:01.150312: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x891b880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-12 19:07:01.150343: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-10-12 19:07:01.150555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:07:01.151252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-12 19:07:01.151328: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-12 19:07:01.151356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-12 19:07:01.151379: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-12 19:07:01.151402: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-12 19:07:01.151425: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-12 19:07:01.151448: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-12 19:07:01.151472: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-12 19:07:01.151563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:07:01.152332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:07:01.153049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-12 19:07:01.153124: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-12 19:07:01.154626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-10-12 19:07:01.154668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-10-12 19:07:01.154683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-10-12 19:07:01.154831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:07:01.155611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:07:01.156325: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-10-12 19:07:01.156375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:53: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:55: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:63: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Model prompt >>> The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer Olympics, with the theme of “one world, one dream”. Plans for the relay were announced on April 26, 2007, in Beijing, China. The relay, also called by the organizers as the “Journey of Harmony”, lasted 129 days and carried the torch 137,000 km (85,000 mi) – the longest distance of any Olympic torch relay since the tradition was started ahead of the 1936 Summer Olympics. After being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch traveled to the Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. From Beijing, the torch was following a route passing through six continents. The torch has visited cities along the Silk Road, symbolizing ancient links between China and the rest of the world. The relay also included an ascent with the flame to the top of Mount Everest on the border of Nepal and Tibet, China from the Chinese side, which was closed specially for the event. Q: What was the length of the race? A: 137,000 km Q: Was it larger than previous ones? A: No Q: Where did the race begin? A: Olympia, Greece Q: Where did they go after? A: Athens Q: How many days was the race? A: seven Q: Did they visit any notable landmarks? A: Panathinaiko Stadium Q: And did they climb any mountains? A:\n",
            "2019-10-12 19:07:27.965472: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "======================================== SAMPLE 1 ========================================\n",
            " Yes Q: But the relay from Beijing took so long? A: The event took seven days (10 days without a chance) Q: When did it start for them? Wasn't it after the Olympics, what? A: March 26, 2007 Q: And they did the relay on August 8, 2008, so after the Olympics it lasted 11 days after they moved to Beijing\n",
            "(1124983140) cdc: I asked if it's a major travel hub (as in America) I did see a US media story about this story, but I have no direct knowledge about it.\n",
            "(1124831860) cdc: The news stories say that it was taken just before the world had a chance to get used to the Olympics, but since that makes no sense, I'd say the events occurred after the Olympics, which is correct.\n",
            "(1124934200) J-P: you can actually get much far more involved with everything than I can do in terms of reporting\n",
            "(1124934200) J-P: ask a real question\n",
            "(1124934820) cdc: I might not have had the chance to ask another real serious question, after which there are hundreds of people answering, but it would at least give us the name of some people, but whether they were the real answers, or not, it wouldn't help the rest of the world.\n",
            "(1124935400) J-P: also, it says that it couldn't find links\n",
            "(1124935400) J-P: what kind of site? the news page, the news, the news articles? something else is telling you\n",
            "(1124935860) cdc: I think the news page at www.breezynews.com will tell you the whole story, but I don't want to waste your time wondering exactly what you mean.\n",
            "(1124936280) J-P: not in the article, I don't have to\n",
            "(1124934960) J-P: there's no 'we'\n",
            "(1124935020) cdc: so it was only the ones whose information was in the article?\n",
            "(1124935020) J-P: yes, it was just people who were doing the event at that place, but not everyone doing it\n",
            "(1124936260) J-P: you could say the article is saying this for the people who had information in the article right\n",
            "\n",
            "\n",
            "(13507559\n",
            "======================================== SAMPLE 2 ========================================\n",
            " Mount Everest Q: Where was it during the race? A: In Greece near the Panathinaiko Stadium Q: Did they stop by any particular place? A: Olympia Quotation: Run 'Walking the Torch' in Olympic Hall, Athens, Greece. Q: Did they use any special facilities? A: No Q: Did they hold any special ceremonies? A: No Q: Do the runners carry the torch on their back? A: No Q: Did they hold any special celebrations? A: No Q: Did they hold special events for their team? A: No Q: Did they hold any special events for the village/town? A: No Q: Does the relay run past areas/characters people/places? A: Yes, they ran. Q: What were their times? A: 9:00 p.m.\n",
            "(1210776580) ikonia_: and yes they did, but it wasn't in the case that they do so every time, the most important thing was how long the relay had lasted and how many destinations they had traveled after reaching each destination.\n",
            "(1210776580) jrib: you can read 'Greece-2008-2006 Marathon' I can read it when I was at school 1 week\n",
            "(1210776580) jrib: yes but I ran that at a school of 1 week's time\n",
            "(1210776580) jrib: in Greece they might have held special events for their people, at least one of the reasons it was not a run 'Walking the Torch'\n",
            "(1210776640) jrib: even they get a rest once they hit each destination\n",
            "(1210776640) jrib: after reaching the first destination it also includes a ride around the city, or trip to places along the route passing through the various towns/villages that were selected for them, by the way\n",
            "(1210778240) jrib: no, all the runners start running around 9:00 p.m.\n",
            "(1210778240) jrib: no, they start at 9:00 p.m.\n",
            "(1210778240) jrib: 9:00 p.m. is the time you run the time you arrive/ arrive at the destination\n",
            "(1210778300) jrib: ok, so like you said: http://www.yoursite.org.gse/getlog.xhtml \n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 86, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 138, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 471, in _Fire\n",
            "    target=component.__name__)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 675, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 68, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmJ9SaM7SjCu",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_s0WXqRSlQk",
        "colab_type": "text"
      },
      "source": [
        "# **Summarization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tpf8Z-OmSqgr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9016ded6-200f-4e52-9a9e-aecc628b28da"
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --model_name='345M'  --nsamples=2 --top_k=100 --temperature=1"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:52: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-10-12 19:08:32.666490: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-10-12 19:08:32.684602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:08:32.685431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-12 19:08:32.685786: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-12 19:08:32.687320: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-12 19:08:32.688850: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-12 19:08:32.689248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-12 19:08:32.690985: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-12 19:08:32.692297: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-12 19:08:32.696094: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-12 19:08:32.696263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:08:32.697126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:08:32.697959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-12 19:08:32.704754: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-10-12 19:08:32.705070: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2bcd640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-12 19:08:32.705120: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-10-12 19:08:32.752820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:08:32.753711: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x85c5880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-12 19:08:32.753745: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-10-12 19:08:32.753944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:08:32.754643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-12 19:08:32.754720: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-12 19:08:32.754750: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-12 19:08:32.754773: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-12 19:08:32.754797: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-12 19:08:32.754820: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-12 19:08:32.754857: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-12 19:08:32.754896: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-12 19:08:32.754981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:08:32.755777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:08:32.756441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-12 19:08:32.756507: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-12 19:08:32.758070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-10-12 19:08:32.758134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-10-12 19:08:32.758147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-10-12 19:08:32.758332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:08:32.759080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-12 19:08:32.759997: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-10-12 19:08:32.760054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:53: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:55: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:63: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Model prompt >>> Theodore McCarrick is the most senior Catholic figure to be dismissed from the priesthood in modern times. US Church officials said allegations he had sexually assaulted a teenager five decades ago were credible. Mr McCarrick, 88, had previously resigned but said he had \"no recollection\" of the alleged abuse. \"No bishop, no matter how influential, is above the law of the Church,\" Cardinal Daniel DiNardo, president of the United States Conference of Catholic Bishops said in a statement. \"For all those McCarrick abused, I pray this judgment will be one small step, among many, toward healing.\" The alleged abuses may have taken place too long ago for criminal charges to be filed because of the statute of limitations. Mr McCarrick was the archbishop of Washington DC from 2001 to 2006. Since his resignation last year from the College of Cardinals, he has been living in seclusion in a monastery in Kansas. He was the first person to resign as a cardinal since 1927. He is among hundreds of members of the clergy accused of sexually abusing children over several decades and his dismissal comes days before the Vatican hosts a summit on preventing child abuse. The Vatican said Pope Francis had ruled Mr McCarrick's expulsion from the clergy as definitive, and would not allow any further appeals against the decision.  TL;DR: \n",
            "2019-10-12 19:08:59.733264: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "======================================== SAMPLE 1 ========================================\n",
            " I'm not a pedophile, but I'm sure he needs help the way you are!\n",
            "(1255694560) jrib: gmod for my bad (not really making sense)\n",
            "(1255694560) ikonia: gmod is a command line utility in linux\n",
            "(1255694560)   ikonia: sudo apt-get install gmod\n",
            "(1255694620)   ikonia: /gmod ?                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
            "======================================== SAMPLE 2 ========================================\n",
            " I just want to say, because I may have taken part of something that I should not have spoken about. Anyone in doubt, please let me put in this a link to my experience http://www.nakedangerously.com/blog/2007/03/03/09/the-most-senior-bishops-to-be-dismissed-from-the-pastoral-role/\n",
            "I just want to say, because I may have taken part of something that I should not have spoke about. Anyone in doubt, please let me put in this a link to my experience http://www.nakedangerously.com/blog/2007/03/03/09/the-most-senior-bishops-to-be-dismissed-from-the-pastoral-role/\n",
            "(1234233360) Vosse: i did some online research yesterday with google and Google+ so you could get your fix now\n",
            "(1234233420) Vosse: when i go to https://www.google.ca(search:google.ca),\n",
            "(1234233420) Vosse: that's the url i didn't get :p\n",
            "(1234233480) Vosse: google doesn't offer such search but search my google+ page\n",
            "(1234234060) Vosse: http://pastebin.com/m7578b824\n",
            "(1234245140) Vosse: https://help.ubuntu.com/community/WannaGoogle?search=Search&b=google&g=http://searchgoogle.ca&q=&q=%2C&w=IqR4WirzxbvXJQ5&g=http%3A%2F%2F*%2Fhelp.ubuntu.com%3A%2F%2F%2Fsearchresult.php%3F*&q=%3F%3F&w=searchresults.php&q=%3B%3BFqu%3D&q=%3B%3BFn%3DB5\n",
            "(1234245140) Vosse: http://pastebin.com/m1f7c5590\n",
            "(1234454000) Vosse: http://pastebin.com/m84c47270\n",
            "(1234454000) V\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 5480, in get_controller\n",
            "    yield g\n",
            "  File \"src/interactive_conditional_samples.py\", line 68, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 86, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 138, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 471, in _Fire\n",
            "    target=component.__name__)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 675, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 83, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1634, in __exit__\n",
            "    close_thread.join(30.0)\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1060, in join\n",
            "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
            "    elif lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}