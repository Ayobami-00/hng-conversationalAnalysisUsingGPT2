# -*- coding: utf-8 -*-
"""Conversational Analysis using the GPT-2 model on ubuntu corpus data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sAnfGwZD6rRw1wG3RgqjUEHtPORM6Vy_
"""

#Loading up our drive on google colab
from google.colab import drive
drive.mount('/content/drive')

#importing relevant modules
import os
import json
import random
import re
import pandas as pd
from numpy import int64

#Cloning the gpt-2 model
!git clone https://github.com/tenoke/gpt-2

#Changing the root directory
cd gpt-2

#Installing the required files in the requirements.txt file
!pip3 install -r requirements.txt

#Downloading the model
!python download_model.py 345M

#Setting up the kaggle api so that we can access our ubuntu corpus dataset
from google.colab import files
files.upload()

!ls -lha kaggle.json

#Installing the Kaggle API client
!pip install -q kaggle

# The Kaggle API client expects this file to be in ~/.kaggle,
# so move it there.
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

# This permissions change avoids a warning on Kaggle tool startup.
!chmod 600 ~/.kaggle/kaggle.json

#Downloading the dataset from kaggle
!kaggle datasets download -d rtatman/ubuntu-dialogue-corpus

!unzip ubuntu-dialogue-corpus.zip

!mkdir ubuntu-data ubuntu-npz

#Reading the different datasets into pandas
dataset1 = pd.read_csv('Ubuntu-dialogue-corpus/dialogueText.csv', parse_dates=['date'])

dataset1.head()

dataset2 = pd.read_csv('Ubuntu-dialogue-corpus/dialogueText_196.csv', parse_dates=['date'])

dataset2.tail()

dataset3 = pd.read_csv('Ubuntu-dialogue-corpus/dialogueText_301.csv', parse_dates=['date'])

dataset3.tail()

#Merging the two datastes
combined = pd.merge(dataset1,dataset2,how = 'outer')

#Merging it with the third
final_data = pd.merge(combined,dataset3)

#Creating a csv of the final dataset
final_data.to_csv('./final_csv_data')

#Reading the dialogueText_196.csv in chunks
datasets = pd.read_csv('Ubuntu-dialogue-corpus/dialogueText_196.csv', parse_dates=['date'], chunksize=1200000)

i = 1

#Pre-processing our data
for dataset in datasets:
  dataset['date'] = dataset['date'].astype(int64) // 10**9
  
  text_corpus = ''
  current = None
  for msg in dataset.itertuples():
    if msg.dialogueID != current:
      current = msg.dialogueID
      text_corpus += '\n\n'
    try:
      text_corpus += f"({msg.date}) {msg._4}: {msg.text}\n"
    except KeyError:
      pass
  
  with open(f'ubuntu-data/ubuntu-cleaned-{i}.txt', 'w') as f:
    f.write(text_corpus)
  del(text_corpus)
  i +=1
    
del(datasets)
del(dataset)

!du -h ubuntu-data/*

#Encoding our data
!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-1.txt ubuntu-npz/ubuntu-cleaned-1.txt.npz --model_name 345M

!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-2.txt ubuntu-npz/ubuntu-cleaned-2.txt.npz --model_name 345M

!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-3.txt ubuntu-npz/ubuntu-cleaned-3.txt.npz --model_name 345M

!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-4.txt ubuntu-npz/ubuntu-cleaned-4.txt.npz --model_name 345M

!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-5.txt ubuntu-npz/ubuntu-cleaned-5.txt.npz --model_name 345M

!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-6.txt ubuntu-npz/ubuntu-cleaned-6.txt.npz --model_name 345M

!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-7.txt ubuntu-npz/ubuntu-cleaned-7.txt.npz --model_name 345M

!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-8.txt ubuntu-npz/ubuntu-cleaned-8.txt.npz --model_name 345M

!cp -r ubuntu-npz/ /content/drive/My\ Drive/ubuntu-npz/

!du -h ubuntu-npz/*

#TUNING AND TRAINING OUR DATA AND GENERATING UNCONDITIONAL SAMPLES

#With default learnig rate
!PYTHONPATH=src ./train.py --dataset ubuntu-npz/ --sample_every=250 --stop_after=1501 --model_name 345M

#With learning rate of 0.0001
!PYTHONPATH=src ./train.py --dataset ubuntu-npz/ --sample_every=250 --learning_rate 0.0001 --stop_after 3501 --model_name 345M



