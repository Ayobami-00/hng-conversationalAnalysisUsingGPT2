{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wNNx0dLsEtso",
    "outputId": "5f5897c7-4fed-4d87-fa3c-ec2cf53437e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#Loading up our drive on google colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "36UmLVLHS1L8"
   },
   "outputs": [],
   "source": [
    "#importing relevant modules\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "from numpy import int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0Ivs025UF_4Q",
    "outputId": "cc54451e-35ef-4bc1-c21d-805c42272864"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'gpt-2' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "#Cloning the gpt-2 model\n",
    "!git clone https://github.com/tenoke/gpt-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "SnpnW5U2FC75",
    "outputId": "be583d65-178f-4d9a-97a4-2af97e0292e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'gpt-2'\n",
      "/content/gpt-2\n"
     ]
    }
   ],
   "source": [
    "#changing the root directory\n",
    "cd gpt-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "colab_type": "code",
    "id": "eCfeKFy3Gf1u",
    "outputId": "31199f57-afec-4bca-c888-b10a01eea4f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 3.1MB/s \n",
      "\u001b[?25hCollecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
      "\u001b[K     |████████████████████████████████| 604kB 8.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
      "Collecting tqdm==4.31.1 (from -r requirements.txt (line 4))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 20.9MB/s \n",
      "\u001b[?25hCollecting toposort==1.5 (from -r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.9.11)\n",
      "Building wheels for collected packages: fire, regex\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103527 sha256=aa96c119a9b96b911dd30e93abc303a0793928713396332959f323b4160e87d7\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=533181 sha256=53acd071c6364e687abb0c9352ace9428372d85e56e851c20ad5aa507cde6fd3\n",
      "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
      "Successfully built fire regex\n",
      "Installing collected packages: fire, regex, tqdm, toposort\n",
      "  Found existing installation: tqdm 4.28.1\n",
      "    Uninstalling tqdm-4.28.1:\n",
      "      Successfully uninstalled tqdm-4.28.1\n",
      "Successfully installed fire-0.2.1 regex-2017.4.5 toposort-1.5 tqdm-4.31.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "tqdm"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Installing the required files in the requirements.txt file\n",
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "953aqEwAGojm",
    "outputId": "01a341d7-6849-4063-b48b-e7d6a31e77f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Fetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\r",
      "Fetching checkpoint: 1.00kit [00:00, 662kit/s]                                                      \n",
      "\r",
      "Fetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\r",
      "Fetching encoder.json: 1.04Mit [00:00, 36.0Mit/s]                                                   \n",
      "Fetching hparams.json: 1.00kit [00:00, 737kit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:24, 57.5Mit/s]                                 \n",
      "Fetching model.ckpt.index: 11.0kit [00:00, 6.46Mit/s]                                               \n",
      "Fetching model.ckpt.meta: 927kit [00:00, 26.6Mit/s]                                                 \n",
      "Fetching vocab.bpe: 457kit [00:00, 32.0Mit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "#downloading the model\n",
    "!python download_model.py 345M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "OP96ZY3FGzjk",
    "outputId": "a69f6afd-d227-41db-f80b-24f085698b68"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-8fbcbfed-6557-47a1-9f65-440bb95c45a3\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-8fbcbfed-6557-47a1-9f65-440bb95c45a3\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving kaggle.json to kaggle.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kaggle.json': b'{\"username\":\"ayobami00\",\"key\":\"9bb7c11a1acbd33a0040e056d1b9afa5\"}'}"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setting up the kaggle api so that we can access our ubuntu corpus dataset\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8_5M8A35HYVE",
    "outputId": "9396f2bf-feef-4a51-8a1d-e2958c4c1326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 65 Oct 10 23:18 kaggle.json\n"
     ]
    }
   ],
   "source": [
    "!ls -lha kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tKOW61c0HMar"
   },
   "outputs": [],
   "source": [
    "#Installing the Kaggle API client\n",
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "mkZt5vwdHhU7",
    "outputId": "6babebcc-178c-4364-ec40-8bd7f42a11f3"
   },
   "outputs": [
    {
     "ename": "MessageError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-0f4e6e5edc3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# This permissions change avoids a warning on Kaggle tool startup.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chmod 600 ~/.kaggle/kaggle.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0mhide_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_remove_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhide_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMessageError\u001b[0m: CustomError: Timed out waiting for output iframe load."
     ]
    }
   ],
   "source": [
    "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
    "# so move it there.\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "\n",
    "# This permissions change avoids a warning on Kaggle tool startup.\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "7E422w_cHnJP",
    "outputId": "d5d98919-83bb-4bfe-c930-85551a8d3593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ubuntu-dialogue-corpus.zip to /content/gpt-2/gpt-2\n",
      " 99% 794M/799M [00:09<00:00, 134MB/s]\n",
      "100% 799M/799M [00:09<00:00, 88.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "#Downloading the dataset from kaggle\n",
    "!kaggle datasets download -d rtatman/ubuntu-dialogue-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "kfzfQU2aHsem",
    "outputId": "ba3a5f30-56f5-4d05-e4ec-a23c877f63c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ubuntu-dialogue-corpus.zip\n",
      "  inflating: Ubuntu-dialogue-corpus/dialogueText.csv  \n",
      "  inflating: Ubuntu-dialogue-corpus/dialogueText_196.csv  \n",
      "  inflating: Ubuntu-dialogue-corpus/dialogueText_301.csv  \n",
      "  inflating: toc.csv                 \n"
     ]
    }
   ],
   "source": [
    "!unzip ubuntu-dialogue-corpus.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T2SzUkWmH24Z"
   },
   "outputs": [],
   "source": [
    "!mkdir ubuntu-data ubuntu-npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_z8_yWwQIISh"
   },
   "outputs": [],
   "source": [
    "#Reading the different datasets into pandas\n",
    "dataset1 = pd.read_csv('Ubuntu-dialogue-corpus/dialogueText.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "mzST2hgqI_5O",
    "outputId": "c00ddce2-c030-46bd-dd16-6a8b655fed0a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>dialogueID</th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>126125.tsv</td>\n",
       "      <td>2008-04-23 14:55:00+00:00</td>\n",
       "      <td>bad_image</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hello folks, please help me a bit with the fol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>126125.tsv</td>\n",
       "      <td>2008-04-23 14:56:00+00:00</td>\n",
       "      <td>bad_image</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Did I choose a bad channel? I ask because you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>126125.tsv</td>\n",
       "      <td>2008-04-23 14:57:00+00:00</td>\n",
       "      <td>lordleemo</td>\n",
       "      <td>bad_image</td>\n",
       "      <td>the second sentence is better english   and we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>64545.tsv</td>\n",
       "      <td>2009-08-01 06:22:00+00:00</td>\n",
       "      <td>mechtech</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sock Puppe?t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>64545.tsv</td>\n",
       "      <td>2009-08-01 06:22:00+00:00</td>\n",
       "      <td>mechtech</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WTF?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   folder  ...                                               text\n",
       "0       3  ...  Hello folks, please help me a bit with the fol...\n",
       "1       3  ...  Did I choose a bad channel? I ask because you ...\n",
       "2       3  ...  the second sentence is better english   and we...\n",
       "3       3  ...                                       Sock Puppe?t\n",
       "4       3  ...                                               WTF?\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MIcJa-ckJJEJ"
   },
   "outputs": [],
   "source": [
    "dataset2 = pd.read_csv('Ubuntu-dialogue-corpus/dialogueText_196.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "vVZJQxMtJp3e",
    "outputId": "66ec91f3-afda-47b9-8fc3-ba2381e67aa8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>dialogueID</th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9212872</th>\n",
       "      <td>13</td>\n",
       "      <td>3676.tsv</td>\n",
       "      <td>2012-07-07 20:17:00+00:00</td>\n",
       "      <td>MonkeyDust</td>\n",
       "      <td>legolas</td>\n",
       "      <td>= arian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9212873</th>\n",
       "      <td>13</td>\n",
       "      <td>3676.tsv</td>\n",
       "      <td>2012-07-07 20:18:00+00:00</td>\n",
       "      <td>MonkeyDust</td>\n",
       "      <td>legolas</td>\n",
       "      <td>observation and deduction, dear watson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9212874</th>\n",
       "      <td>13</td>\n",
       "      <td>16586.tsv</td>\n",
       "      <td>2008-07-25 01:53:00+00:00</td>\n",
       "      <td>linuxfce</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i am trying to install nvidia drivers from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9212875</th>\n",
       "      <td>13</td>\n",
       "      <td>16586.tsv</td>\n",
       "      <td>2008-07-25 01:53:00+00:00</td>\n",
       "      <td>linuxfce</td>\n",
       "      <td>NaN</td>\n",
       "      <td>how do i enter runlevel 3? when i try telinit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9212876</th>\n",
       "      <td>13</td>\n",
       "      <td>16586.tsv</td>\n",
       "      <td>2008-07-25 01:54:00+00:00</td>\n",
       "      <td>linuxfce</td>\n",
       "      <td>NaN</td>\n",
       "      <td>anyone know how to enter runlevel 3 in ubuntu?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         folder  ...                                               text\n",
       "9212872      13  ...                                            = arian\n",
       "9212873      13  ...             observation and deduction, dear watson\n",
       "9212874      13  ...  i am trying to install nvidia drivers from the...\n",
       "9212875      13  ...  how do i enter runlevel 3? when i try telinit ...\n",
       "9212876      13  ...     anyone know how to enter runlevel 3 in ubuntu?\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A8ouKhCGJyLd"
   },
   "outputs": [],
   "source": [
    "dataset3 = pd.read_csv('Ubuntu-dialogue-corpus/dialogueText_301.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "Yg7oGpjIK_AS",
    "outputId": "f4b807f2-bbb3-43e7-ae64-a59ed6127ea1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>dialogueID</th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16587825</th>\n",
       "      <td>32</td>\n",
       "      <td>1783.tsv</td>\n",
       "      <td>2007-11-15 03:38:00+00:00</td>\n",
       "      <td>koyo001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16587826</th>\n",
       "      <td>32</td>\n",
       "      <td>1783.tsv</td>\n",
       "      <td>2007-11-15 03:39:00+00:00</td>\n",
       "      <td>koyo001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>does anyone know something</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16587827</th>\n",
       "      <td>32</td>\n",
       "      <td>1783.tsv</td>\n",
       "      <td>2007-11-15 03:39:00+00:00</td>\n",
       "      <td>neverblue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no, no one knows everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16587828</th>\n",
       "      <td>32</td>\n",
       "      <td>1783.tsv</td>\n",
       "      <td>2007-11-15 03:40:00+00:00</td>\n",
       "      <td>koyo001</td>\n",
       "      <td>ikonia</td>\n",
       "      <td>the camera doesnt work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16587829</th>\n",
       "      <td>32</td>\n",
       "      <td>1783.tsv</td>\n",
       "      <td>2007-11-15 03:40:00+00:00</td>\n",
       "      <td>neverblue</td>\n",
       "      <td>koyo001</td>\n",
       "      <td>I believe you missed a post or two while you w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          folder  ...                                               text\n",
       "16587825      32  ...                                             thanks\n",
       "16587826      32  ...                         does anyone know something\n",
       "16587827      32  ...                        no, no one knows everything\n",
       "16587828      32  ...                             the camera doesnt work\n",
       "16587829      32  ...  I believe you missed a post or two while you w...\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q0I2X-l4MQcL"
   },
   "outputs": [],
   "source": [
    "#Merging the two datastes\n",
    "combined = pd.merge(dataset1,dataset2,how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ljsoEZeIP969"
   },
   "outputs": [],
   "source": [
    "#Merging it with the third\n",
    "final_data = pd.merge(combined,dataset3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q4Yae524Q2zw"
   },
   "outputs": [],
   "source": [
    "final_data.to_csv('./final_csv_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dwPxj9PJRcKD"
   },
   "outputs": [],
   "source": [
    "datasets = pd.read_csv('Ubuntu-dialogue-corpus/dialogueText_196.csv', parse_dates=['date'], chunksize=1200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CZ17wc04Rd1Y"
   },
   "outputs": [],
   "source": [
    "#Pre-processing our data\n",
    "i = 1\n",
    "\n",
    "for dataset in datasets:\n",
    "  dataset['date'] = dataset['date'].astype(int64) // 10**9\n",
    "  \n",
    "  text_corpus = ''\n",
    "  current = None\n",
    "  for msg in dataset.itertuples():\n",
    "    if msg.dialogueID != current:\n",
    "      current = msg.dialogueID\n",
    "      text_corpus += '\\n\\n'\n",
    "    try:\n",
    "      text_corpus += f\"({msg.date}) {msg._4}: {msg.text}\\n\"\n",
    "    except KeyError:\n",
    "      pass\n",
    "  \n",
    "  with open(f'ubuntu-data/ubuntu-cleaned-{i}.txt', 'w') as f:\n",
    "    f.write(text_corpus)\n",
    "  del(text_corpus)\n",
    "  i +=1\n",
    "    \n",
    "del(datasets)\n",
    "del(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "Q4TmFtx2SqbI",
    "outputId": "0e240fd0-2288-46c8-b58e-18ec77b17550"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91M\tubuntu-data/ubuntu-cleaned-1.txt\n",
      "92M\tubuntu-data/ubuntu-cleaned-2.txt\n",
      "90M\tubuntu-data/ubuntu-cleaned-3.txt\n",
      "89M\tubuntu-data/ubuntu-cleaned-4.txt\n",
      "89M\tubuntu-data/ubuntu-cleaned-5.txt\n",
      "86M\tubuntu-data/ubuntu-cleaned-6.txt\n",
      "85M\tubuntu-data/ubuntu-cleaned-7.txt\n",
      "60M\tubuntu-data/ubuntu-cleaned-8.txt\n"
     ]
    }
   ],
   "source": [
    "!du -h ubuntu-data/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "E0ZPdGgFVo_b",
    "outputId": "53d8df36-0a2b-42e1-a66e-1370e71aaea1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      "100% 1/1 [03:36<00:00, 216.66s/it]\n",
      "Writing ubuntu-npz/ubuntu-cleaned-1.txt.npz\n"
     ]
    }
   ],
   "source": [
    "#Encoding our data\n",
    "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-1.txt ubuntu-npz/ubuntu-cleaned-1.txt.npz --model_name 345M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "KmNhnB9ZVr5I",
    "outputId": "e3dae071-3f95-4593-8c6d-814ab28c244a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      "100% 1/1 [03:32<00:00, 212.22s/it]\n",
      "Writing ubuntu-npz/ubuntu-cleaned-2.txt.npz\n"
     ]
    }
   ],
   "source": [
    "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-2.txt ubuntu-npz/ubuntu-cleaned-2.txt.npz --model_name 345M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "MvWAusLfVwx7",
    "outputId": "f01d8486-64dc-4c3a-bca7-2649bb31e37d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      "100% 1/1 [03:18<00:00, 198.31s/it]\n",
      "Writing ubuntu-npz/ubuntu-cleaned-3.txt.npz\n"
     ]
    }
   ],
   "source": [
    "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-3.txt ubuntu-npz/ubuntu-cleaned-3.txt.npz --model_name 345M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "-Ixg24HlVvAu",
    "outputId": "78af017e-be25-4262-bfc1-a64266c27403"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      "100% 1/1 [03:27<00:00, 207.94s/it]\n",
      "Writing ubuntu-npz/ubuntu-cleaned-4.txt.npz\n"
     ]
    }
   ],
   "source": [
    "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-4.txt ubuntu-npz/ubuntu-cleaned-4.txt.npz --model_name 345M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Qxu2e5n1V0UX",
    "outputId": "6f40edde-c3d7-46d0-f777-1e146b477657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      "100% 1/1 [03:14<00:00, 194.02s/it]\n",
      "Writing ubuntu-npz/ubuntu-cleaned-5.txt.npz\n"
     ]
    }
   ],
   "source": [
    "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-5.txt ubuntu-npz/ubuntu-cleaned-5.txt.npz --model_name 345M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "6xhC5yr7V2rG",
    "outputId": "4ea22287-852d-4b31-db0e-e03211532cf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      "100% 1/1 [03:15<00:00, 195.86s/it]\n",
      "Writing ubuntu-npz/ubuntu-cleaned-6.txt.npz\n"
     ]
    }
   ],
   "source": [
    "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-6.txt ubuntu-npz/ubuntu-cleaned-6.txt.npz --model_name 345M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "N19RnlVhV5Qv",
    "outputId": "6934e9ef-5af3-402d-888e-3cdef5e5f4e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      "100% 1/1 [03:03<00:00, 183.55s/it]\n",
      "Writing ubuntu-npz/ubuntu-cleaned-7.txt.npz\n"
     ]
    }
   ],
   "source": [
    "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-7.txt ubuntu-npz/ubuntu-cleaned-7.txt.npz --model_name 345M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "nQYfWuAQdimK",
    "outputId": "ef8b602c-e8c0-410e-bfcd-0e3b9eb68401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      "100% 1/1 [02:13<00:00, 133.68s/it]\n",
      "Writing ubuntu-npz/ubuntu-cleaned-8.txt.npz\n"
     ]
    }
   ],
   "source": [
    "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-8.txt ubuntu-npz/ubuntu-cleaned-8.txt.npz --model_name 345M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqL9YMhPdlUu"
   },
   "outputs": [],
   "source": [
    "!cp -r ubuntu-npz/ /content/drive/My\\ Drive/ubuntu-npz/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "WRs1eR_sdreR",
    "outputId": "7768dec5-313a-463a-cf83-87f8b785152a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41M\tubuntu-npz/ubuntu-cleaned-1.txt.npz\n",
      "46M\tubuntu-npz/ubuntu-cleaned-2.txt.npz\n",
      "41M\tubuntu-npz/ubuntu-cleaned-3.txt.npz\n",
      "41M\tubuntu-npz/ubuntu-cleaned-4.txt.npz\n",
      "39M\tubuntu-npz/ubuntu-cleaned-5.txt.npz\n",
      "39M\tubuntu-npz/ubuntu-cleaned-6.txt.npz\n",
      "37M\tubuntu-npz/ubuntu-cleaned-7.txt.npz\n",
      "27M\tubuntu-npz/ubuntu-cleaned-8.txt.npz\n"
     ]
    }
   ],
   "source": [
    "!du -h ubuntu-npz/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LGSMXTIVds8D",
    "outputId": "1393ad65-ad22-4c3f-e8a8-6d4e3d62a597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From ./train.py:73: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From ./train.py:76: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2019-10-11 00:20:19.260601: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "2019-10-11 00:20:19.261123: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1e13480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2019-10-11 00:20:19.261174: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2019-10-11 00:20:19.293222: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2019-10-11 00:20:19.424125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-10-11 00:20:19.424997: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x75c3180 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2019-10-11 00:20:19.425028: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2019-10-11 00:20:19.425509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-10-11 00:20:19.426190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "2019-10-11 00:20:19.434250: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-10-11 00:20:19.616582: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-10-11 00:20:19.714232: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2019-10-11 00:20:19.746417: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2019-10-11 00:20:19.975593: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2019-10-11 00:20:20.116477: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2019-10-11 00:20:20.585256: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-10-11 00:20:20.585572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-10-11 00:20:20.586435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-10-11 00:20:20.587139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2019-10-11 00:20:20.591409: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-10-11 00:20:20.593177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-10-11 00:20:20.593218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2019-10-11 00:20:20.593234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2019-10-11 00:20:20.596093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-10-11 00:20:20.596896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-10-11 00:20:20.597640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "WARNING:tensorflow:From ./train.py:77: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "WARNING:tensorflow:From ./train.py:100: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From ./train.py:113: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
      "Instructions for updating:\n",
      "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/memory_saving_gradients.py:89: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From ./train.py:120: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From ./train.py:122: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From ./train.py:125: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From ./train.py:129: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "Loading checkpoint models/345M/model.ckpt\n",
      "Loading dataset...\n",
      "100% 8/8 [00:08<00:00,  1.09s/it]\n",
      "dataset has 233929304 tokens\n",
      "Training...\n",
      "2019-10-11 00:21:39.859671: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "[1 | 17.26] loss=2.62 avg=2.62\n",
      "[2 | 20.23] loss=3.02 avg=2.82\n",
      "[3 | 23.21] loss=2.74 avg=2.79\n",
      "[4 | 26.19] loss=2.50 avg=2.72\n",
      "[5 | 29.18] loss=2.62 avg=2.70\n",
      "[6 | 32.16] loss=2.61 avg=2.68\n",
      "[7 | 35.15] loss=2.61 avg=2.67\n",
      "[8 | 38.15] loss=3.05 avg=2.72\n",
      "[9 | 41.15] loss=2.70 avg=2.72\n",
      "[10 | 44.16] loss=2.60 avg=2.71\n",
      "[11 | 47.16] loss=3.08 avg=2.74\n",
      "[12 | 50.18] loss=2.42 avg=2.71\n",
      "[13 | 53.19] loss=2.71 avg=2.71\n",
      "[14 | 56.21] loss=2.54 avg=2.70\n",
      "[15 | 59.22] loss=3.31 avg=2.74\n",
      "[16 | 62.25] loss=2.48 avg=2.73\n",
      "[17 | 65.28] loss=2.54 avg=2.71\n",
      "[18 | 68.38] loss=2.98 avg=2.73\n",
      "[19 | 71.47] loss=2.83 avg=2.74\n",
      "[20 | 74.57] loss=2.42 avg=2.72\n",
      "[21 | 77.67] loss=2.68 avg=2.72\n",
      "[22 | 80.77] loss=2.58 avg=2.71\n",
      "[23 | 83.84] loss=2.82 avg=2.71\n",
      "[24 | 86.92] loss=2.72 avg=2.71\n",
      "[25 | 90.00] loss=2.26 avg=2.69\n",
      "[26 | 93.09] loss=2.65 avg=2.69\n",
      "[27 | 96.20] loss=2.41 avg=2.68\n",
      "[28 | 99.28] loss=2.16 avg=2.66\n",
      "[29 | 102.35] loss=2.28 avg=2.64\n",
      "[30 | 105.41] loss=2.74 avg=2.65\n",
      "[31 | 108.46] loss=2.84 avg=2.65\n",
      "[32 | 111.52] loss=2.39 avg=2.65\n",
      "[33 | 114.59] loss=2.86 avg=2.65\n",
      "[34 | 117.66] loss=2.23 avg=2.64\n",
      "[35 | 120.77] loss=2.82 avg=2.64\n",
      "[36 | 123.86] loss=2.69 avg=2.65\n",
      "[37 | 126.97] loss=2.50 avg=2.64\n",
      "[38 | 130.07] loss=2.57 avg=2.64\n",
      "[39 | 133.16] loss=2.37 avg=2.63\n",
      "[40 | 136.25] loss=2.27 avg=2.62\n",
      "[41 | 139.35] loss=2.75 avg=2.62\n",
      "[42 | 142.42] loss=2.50 avg=2.62\n",
      "[43 | 145.53] loss=2.50 avg=2.62\n",
      "[44 | 148.61] loss=2.52 avg=2.61\n",
      "[45 | 151.70] loss=2.22 avg=2.60\n",
      "[46 | 154.73] loss=2.81 avg=2.61\n",
      "[47 | 157.75] loss=2.45 avg=2.60\n",
      "[48 | 160.79] loss=2.45 avg=2.60\n",
      "[49 | 163.82] loss=2.45 avg=2.60\n",
      "[50 | 166.84] loss=2.37 avg=2.59\n",
      "[51 | 169.88] loss=2.48 avg=2.59\n",
      "[52 | 172.90] loss=1.99 avg=2.57\n",
      "[53 | 175.93] loss=2.51 avg=2.57\n",
      "[54 | 178.97] loss=1.91 avg=2.56\n",
      "[55 | 182.00] loss=3.14 avg=2.57\n",
      "[56 | 185.03] loss=2.49 avg=2.57\n",
      "[57 | 188.05] loss=2.29 avg=2.56\n",
      "[58 | 191.09] loss=2.21 avg=2.55\n",
      "[59 | 194.13] loss=2.60 avg=2.55\n",
      "[60 | 197.16] loss=2.81 avg=2.56\n",
      "[61 | 200.19] loss=2.40 avg=2.56\n",
      "[62 | 203.22] loss=2.17 avg=2.55\n",
      "[63 | 206.26] loss=2.04 avg=2.54\n",
      "[64 | 209.29] loss=2.94 avg=2.55\n",
      "[65 | 212.31] loss=2.45 avg=2.54\n",
      "[66 | 215.34] loss=2.53 avg=2.54\n",
      "[67 | 218.37] loss=2.06 avg=2.53\n",
      "[68 | 221.40] loss=2.40 avg=2.53\n",
      "[69 | 224.43] loss=2.59 avg=2.53\n",
      "[70 | 227.47] loss=2.48 avg=2.53\n",
      "[71 | 230.50] loss=2.42 avg=2.53\n",
      "[72 | 233.54] loss=2.30 avg=2.53\n",
      "[73 | 236.57] loss=2.38 avg=2.52\n",
      "[74 | 239.60] loss=2.29 avg=2.52\n",
      "[75 | 242.63] loss=2.90 avg=2.53\n",
      "[76 | 245.66] loss=2.48 avg=2.52\n",
      "[77 | 248.70] loss=2.89 avg=2.53\n",
      "[78 | 251.73] loss=3.07 avg=2.54\n",
      "[79 | 254.75] loss=3.11 avg=2.55\n",
      "[80 | 257.77] loss=2.50 avg=2.55\n",
      "[81 | 260.81] loss=2.60 avg=2.55\n",
      "[82 | 263.85] loss=3.17 avg=2.56\n",
      "[83 | 266.90] loss=2.40 avg=2.56\n",
      "[84 | 269.94] loss=2.56 avg=2.56\n",
      "[85 | 272.98] loss=2.21 avg=2.55\n",
      "[86 | 276.01] loss=2.43 avg=2.55\n",
      "[87 | 279.04] loss=2.72 avg=2.55\n",
      "[88 | 282.08] loss=3.02 avg=2.56\n",
      "[89 | 285.10] loss=2.46 avg=2.56\n",
      "[90 | 288.12] loss=2.67 avg=2.56\n",
      "[91 | 291.16] loss=2.15 avg=2.56\n",
      "[92 | 294.21] loss=2.92 avg=2.56\n",
      "[93 | 297.27] loss=2.16 avg=2.55\n",
      "[94 | 300.35] loss=2.55 avg=2.55\n",
      "[95 | 303.43] loss=2.62 avg=2.56\n",
      "[96 | 306.51] loss=2.78 avg=2.56\n",
      "[97 | 309.59] loss=2.78 avg=2.56\n",
      "[98 | 312.67] loss=3.09 avg=2.57\n",
      "[99 | 315.75] loss=2.15 avg=2.56\n",
      "[100 | 318.85] loss=2.44 avg=2.56\n",
      "[101 | 321.94] loss=3.15 avg=2.57\n",
      "[102 | 325.03] loss=2.27 avg=2.57\n",
      "[103 | 328.12] loss=2.41 avg=2.56\n",
      "[104 | 331.19] loss=2.66 avg=2.57\n",
      "[105 | 334.27] loss=2.73 avg=2.57\n",
      "[106 | 337.37] loss=2.16 avg=2.56\n",
      "[107 | 340.43] loss=3.10 avg=2.57\n",
      "[108 | 343.53] loss=2.50 avg=2.57\n",
      "[109 | 346.63] loss=2.87 avg=2.57\n",
      "[110 | 349.72] loss=2.26 avg=2.57\n",
      "[111 | 352.81] loss=2.38 avg=2.57\n",
      "[112 | 355.90] loss=3.23 avg=2.58\n",
      "[113 | 359.02] loss=1.89 avg=2.57\n",
      "[114 | 362.10] loss=2.59 avg=2.57\n",
      "[115 | 365.19] loss=3.08 avg=2.57\n",
      "[116 | 368.27] loss=2.40 avg=2.57\n",
      "[117 | 371.37] loss=2.23 avg=2.57\n",
      "[118 | 374.45] loss=2.39 avg=2.56\n",
      "[119 | 377.49] loss=2.17 avg=2.56\n",
      "[120 | 380.55] loss=2.63 avg=2.56\n",
      "[121 | 383.59] loss=2.20 avg=2.55\n",
      "[122 | 386.66] loss=2.37 avg=2.55\n",
      "[123 | 389.71] loss=2.23 avg=2.55\n",
      "[124 | 392.76] loss=2.46 avg=2.55\n",
      "[125 | 395.82] loss=2.35 avg=2.54\n",
      "[126 | 398.87] loss=2.52 avg=2.54\n",
      "[127 | 401.93] loss=3.07 avg=2.55\n",
      "[128 | 404.98] loss=2.17 avg=2.55\n",
      "[129 | 408.07] loss=2.34 avg=2.54\n",
      "[130 | 411.18] loss=2.56 avg=2.54\n",
      "[131 | 414.28] loss=2.79 avg=2.55\n",
      "[132 | 417.36] loss=2.57 avg=2.55\n",
      "[133 | 420.43] loss=2.46 avg=2.55\n",
      "[134 | 423.51] loss=2.77 avg=2.55\n",
      "[135 | 426.59] loss=2.29 avg=2.54\n",
      "[136 | 429.68] loss=2.73 avg=2.55\n",
      "[137 | 432.78] loss=2.62 avg=2.55\n",
      "[138 | 435.87] loss=2.12 avg=2.54\n",
      "[139 | 438.96] loss=2.30 avg=2.54\n",
      "[140 | 442.03] loss=2.79 avg=2.54\n",
      "[141 | 445.14] loss=2.28 avg=2.54\n",
      "[142 | 448.25] loss=2.37 avg=2.54\n",
      "[143 | 451.32] loss=2.61 avg=2.54\n",
      "[144 | 454.41] loss=2.26 avg=2.53\n",
      "[145 | 457.50] loss=2.63 avg=2.54\n",
      "[146 | 460.58] loss=2.76 avg=2.54\n",
      "[147 | 463.67] loss=3.01 avg=2.54\n",
      "[148 | 466.74] loss=2.97 avg=2.55\n",
      "[149 | 469.82] loss=2.00 avg=2.54\n",
      "[150 | 472.90] loss=2.76 avg=2.55\n",
      "[151 | 475.99] loss=2.60 avg=2.55\n",
      "[152 | 479.05] loss=2.60 avg=2.55\n",
      "[153 | 482.14] loss=2.07 avg=2.54\n",
      "[154 | 485.22] loss=2.22 avg=2.54\n",
      "[155 | 488.30] loss=2.43 avg=2.54\n",
      "[156 | 491.38] loss=1.94 avg=2.53\n",
      "[157 | 494.43] loss=2.19 avg=2.52\n",
      "[158 | 497.47] loss=2.49 avg=2.52\n",
      "[159 | 500.50] loss=2.49 avg=2.52\n",
      "[160 | 503.52] loss=2.81 avg=2.53\n",
      "[161 | 506.55] loss=3.01 avg=2.53\n",
      "[162 | 509.58] loss=2.93 avg=2.54\n",
      "[163 | 512.61] loss=2.33 avg=2.53\n",
      "[164 | 515.65] loss=2.54 avg=2.53\n",
      "[165 | 518.68] loss=2.53 avg=2.53\n",
      "[166 | 521.70] loss=2.40 avg=2.53\n",
      "[167 | 524.72] loss=2.27 avg=2.53\n",
      "[168 | 527.75] loss=2.56 avg=2.53\n",
      "[169 | 530.77] loss=2.16 avg=2.53\n",
      "[170 | 533.81] loss=2.31 avg=2.52\n",
      "[171 | 536.84] loss=2.57 avg=2.52\n",
      "[172 | 539.87] loss=2.52 avg=2.52\n",
      "[173 | 542.90] loss=2.58 avg=2.52\n",
      "[174 | 545.94] loss=3.06 avg=2.53\n",
      "[175 | 548.97] loss=1.72 avg=2.52\n",
      "[176 | 552.00] loss=2.38 avg=2.52\n",
      "[177 | 555.03] loss=2.14 avg=2.51\n",
      "[178 | 558.07] loss=2.98 avg=2.52\n",
      "[179 | 561.10] loss=2.68 avg=2.52\n",
      "[180 | 564.13] loss=2.15 avg=2.52\n",
      "[181 | 567.16] loss=2.74 avg=2.52\n",
      "[182 | 570.19] loss=2.13 avg=2.52\n",
      "[183 | 573.23] loss=3.02 avg=2.52\n",
      "[184 | 576.27] loss=2.53 avg=2.52\n",
      "[185 | 579.30] loss=2.81 avg=2.53\n",
      "[186 | 582.32] loss=2.34 avg=2.52\n",
      "[187 | 585.36] loss=2.61 avg=2.52\n",
      "[188 | 588.39] loss=2.86 avg=2.53\n",
      "[189 | 591.43] loss=2.54 avg=2.53\n",
      "[190 | 594.47] loss=2.55 avg=2.53\n",
      "[191 | 597.50] loss=2.30 avg=2.53\n",
      "[192 | 600.55] loss=2.33 avg=2.52\n",
      "[193 | 603.58] loss=2.15 avg=2.52\n",
      "[194 | 606.60] loss=2.28 avg=2.52\n",
      "[195 | 609.64] loss=1.94 avg=2.51\n",
      "[196 | 612.67] loss=2.60 avg=2.51\n",
      "[197 | 615.70] loss=2.50 avg=2.51\n",
      "[198 | 618.74] loss=2.17 avg=2.51\n",
      "[199 | 621.77] loss=2.44 avg=2.51\n",
      "[200 | 624.80] loss=2.42 avg=2.50\n",
      "[201 | 627.82] loss=2.35 avg=2.50\n",
      "[202 | 630.85] loss=2.88 avg=2.51\n",
      "[203 | 633.88] loss=2.26 avg=2.50\n",
      "[204 | 636.91] loss=2.63 avg=2.51\n",
      "[205 | 639.97] loss=2.98 avg=2.51\n",
      "[206 | 643.06] loss=2.14 avg=2.51\n",
      "[207 | 646.16] loss=2.31 avg=2.50\n",
      "[208 | 649.26] loss=2.57 avg=2.51\n",
      "[209 | 652.35] loss=2.73 avg=2.51\n",
      "[210 | 655.42] loss=2.08 avg=2.50\n",
      "[211 | 658.53] loss=2.90 avg=2.51\n",
      "[212 | 661.62] loss=2.81 avg=2.51\n",
      "[213 | 664.71] loss=3.29 avg=2.52\n",
      "[214 | 667.81] loss=2.72 avg=2.52\n",
      "[215 | 670.88] loss=2.11 avg=2.52\n",
      "[216 | 674.00] loss=2.61 avg=2.52\n",
      "[217 | 677.06] loss=2.53 avg=2.52\n",
      "[218 | 680.15] loss=2.12 avg=2.51\n",
      "[219 | 683.22] loss=2.45 avg=2.51\n",
      "[220 | 686.29] loss=2.61 avg=2.51\n",
      "[221 | 689.34] loss=2.06 avg=2.51\n",
      "[222 | 692.40] loss=2.29 avg=2.51\n",
      "[223 | 695.47] loss=2.83 avg=2.51\n",
      "[224 | 698.54] loss=2.50 avg=2.51\n",
      "[225 | 701.63] loss=2.42 avg=2.51\n",
      "[226 | 704.72] loss=2.34 avg=2.51\n",
      "[227 | 707.82] loss=2.98 avg=2.51\n",
      "[228 | 710.90] loss=2.19 avg=2.51\n",
      "[229 | 713.96] loss=2.13 avg=2.51\n",
      "[230 | 717.04] loss=1.85 avg=2.50\n",
      "[231 | 720.13] loss=2.55 avg=2.50\n",
      "[232 | 723.22] loss=1.95 avg=2.49\n",
      "[233 | 726.31] loss=2.49 avg=2.49\n",
      "[234 | 729.41] loss=2.17 avg=2.49\n",
      "[235 | 732.50] loss=2.04 avg=2.48\n",
      "[236 | 735.60] loss=2.51 avg=2.48\n",
      "[237 | 738.68] loss=2.62 avg=2.49\n",
      "[238 | 741.71] loss=2.36 avg=2.48\n",
      "[239 | 744.74] loss=2.12 avg=2.48\n",
      "[240 | 747.77] loss=2.25 avg=2.48\n",
      "[241 | 750.80] loss=2.48 avg=2.48\n",
      "[242 | 753.83] loss=2.14 avg=2.47\n",
      "[243 | 756.86] loss=2.11 avg=2.47\n",
      "[244 | 759.89] loss=2.19 avg=2.47\n",
      "[245 | 762.92] loss=2.45 avg=2.47\n",
      "[246 | 765.95] loss=2.30 avg=2.46\n",
      "[247 | 768.97] loss=2.14 avg=2.46\n",
      "[248 | 772.00] loss=2.20 avg=2.46\n",
      "[249 | 775.03] loss=2.59 avg=2.46\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "\n",
      "I guess my problem isn't that I couldnt get that to work, but that the gui doesn't work for some reason. I don't know why but I just can't get my gnuplus gui to run.  I have tried all the different programs I can think of or at least tried the ones that offer a nice gui, so far this is the only one that will allow it to be run.\n",
      "(1271823000) Jeeves: does anyone at http://debian.org/ have the answer?\n",
      "(1271823040) Jeeves: if you can find a forum that has a good set of commands...\n",
      "(1271823040) Jeeves: I'm on Ubuntu 11.04, and I'm using gnome-terminal.\n",
      "(1271823000) Jeeves: no clue\n",
      "(1271825020) Jeeves: I'd prefer the KDE thing but I see no reason not to use it\n",
      "(1271825120) Jeeves: oh, sorry I forgot the name of the forum. I'll look.\n",
      "\n",
      "\n",
      "(1226271620) mwe: i'm a little confused\n",
      "(1226271620) mwe: i need a friend to tell me for the first time how to fix all my problems in ubuntu\n",
      "(1226271540) mwe: what did i just download?\n",
      "(1226271540) mwe: i need a friend to tell me for the first time how to fix all my problems on ubuntu\n",
      "(1226271540) mwe: what did i download?\n",
      "(1226271540) mwe: i need a friend to tell me for the first time how to fix all my problems in ubuntu\n",
      "(1226271540) mwe: how do i fix all my problems in ubuntu?\n",
      "(1226271540) mwe: i need a friend to tell me for the first time how to fix all my problems on ubuntu\n",
      "(1226271800) mwe: i cant even see the screens.\n",
      "(1225977560) mwe: i'm new to ubuntu\n",
      "(1225977560) mwe: i am new to ubuntu\n",
      "(1225977560) baktus: I don't know much about ubuntu but I know that you may have some ideas.\n",
      "\n",
      "\n",
      "(1182568250) oskam: What's it all mean\n",
      "(1182568250) oskam: ? i can't understand what that meant\n",
      "(1182568320) oskam: or any\n",
      "(1182568480) oskam: I'm in need some help.. i have trouble with installing and removing from an optical disc .. i got a floppy, i'm trying to connect it to the hard drive... i can't\n",
      "(1182568480) oskam: I see, but i know what disk to get and i want to use it..\n",
      "(1182568480) iRanger: yes . a floppy. and then the software...\n",
      "(1182568500) iRanger: then I can write to that floppy\n",
      "(1182568600) oskam: thanks.. I have to use something to make the whole install process so much faster. i have a floppy and i know i can go through that process with no problems\n",
      "(1182568600) Oskam: oh, that was so frustrating.. but I need to install some programs so the rest of my work is as simple as possible\n",
      "(1182568600) IRob:  if you're going to run a DOS installer, you probably haven't heard about an optical disc system then\n",
      "(1182568620) IRob:  you can't buy them from your pc, they do it for $200 or so\n",
      "(1182568620) iRob:  if you are running Windows, chances are that you have a disk and can write to it\n",
      "(1182568620) IRob:  what are the linux tools for using floppy disks?\n",
      "(1182568620) iRob:  read on and search the internet\n",
      "(1182568620) IRob: or at least check your windows system\n",
      "(1182568620) IRob:  if you use Ubuntu, there are plenty of things available for DOS users\n",
      "(1182568620) Oskam: yes\n",
      "(1182568620) IRob:  then you can try the floppy driver\n",
      "(1182568620) IRob:  you could maybe use another linux tool like vdw-installer or like iRanger and install a iso file using that\n",
      "(1182568680) Oskam: i think I may want to just go back to OS/2\n",
      "\n",
      "\n",
      "(1235473060) h2u\n",
      "\n",
      "[250 | 818.01] loss=2.32 avg=2.46\n",
      "[251 | 821.05] loss=2.45 avg=2.46\n",
      "[252 | 824.09] loss=2.30 avg=2.46\n",
      "[253 | 827.14] loss=2.64 avg=2.46\n",
      "[254 | 830.18] loss=3.07 avg=2.47\n",
      "[255 | 833.23] loss=2.47 avg=2.47\n",
      "[256 | 836.27] loss=2.05 avg=2.46\n",
      "[257 | 839.31] loss=2.40 avg=2.46\n",
      "[258 | 842.34] loss=2.56 avg=2.46\n",
      "[259 | 845.37] loss=2.12 avg=2.46\n",
      "[260 | 848.42] loss=2.76 avg=2.46\n",
      "[261 | 851.44] loss=2.85 avg=2.47\n",
      "[262 | 854.47] loss=2.96 avg=2.47\n",
      "[263 | 857.50] loss=2.15 avg=2.47\n",
      "[264 | 860.53] loss=2.06 avg=2.46\n",
      "[265 | 863.57] loss=2.32 avg=2.46\n",
      "[266 | 866.61] loss=2.25 avg=2.46\n",
      "[267 | 869.65] loss=2.65 avg=2.46\n",
      "[268 | 872.68] loss=2.35 avg=2.46\n",
      "[269 | 875.71] loss=2.63 avg=2.46\n",
      "[270 | 878.74] loss=2.15 avg=2.46\n",
      "[271 | 881.79] loss=2.55 avg=2.46\n",
      "[272 | 884.87] loss=2.21 avg=2.46\n",
      "[273 | 887.96] loss=2.69 avg=2.46\n",
      "[274 | 891.06] loss=2.46 avg=2.46\n",
      "[275 | 894.13] loss=2.68 avg=2.46\n",
      "[276 | 897.31] loss=2.10 avg=2.46\n",
      "[277 | 900.38] loss=2.43 avg=2.46\n",
      "[278 | 903.49] loss=2.45 avg=2.46\n",
      "[279 | 906.58] loss=2.20 avg=2.45\n",
      "[280 | 909.66] loss=2.56 avg=2.46\n",
      "[281 | 912.75] loss=2.34 avg=2.45\n",
      "[282 | 915.82] loss=2.98 avg=2.46\n",
      "[283 | 918.88] loss=2.52 avg=2.46\n",
      "[284 | 921.94] loss=2.87 avg=2.46\n",
      "[285 | 925.00] loss=2.45 avg=2.46\n",
      "[286 | 928.06] loss=2.73 avg=2.47\n",
      "[287 | 931.12] loss=2.13 avg=2.46\n",
      "[288 | 934.19] loss=2.84 avg=2.47\n",
      "[289 | 937.29] loss=2.40 avg=2.47\n",
      "[290 | 940.40] loss=2.05 avg=2.46\n",
      "[291 | 943.49] loss=2.96 avg=2.47\n",
      "[292 | 946.59] loss=2.12 avg=2.46\n",
      "[293 | 949.68] loss=2.45 avg=2.46\n",
      "[294 | 952.78] loss=2.42 avg=2.46\n",
      "[295 | 955.87] loss=2.07 avg=2.46\n",
      "[296 | 958.96] loss=2.57 avg=2.46\n",
      "[297 | 962.04] loss=2.25 avg=2.46\n",
      "[298 | 965.11] loss=2.34 avg=2.46\n",
      "[299 | 968.15] loss=2.75 avg=2.46\n",
      "[300 | 971.19] loss=2.06 avg=2.46\n",
      "[301 | 974.22] loss=2.00 avg=2.45\n",
      "[302 | 977.24] loss=2.25 avg=2.45\n",
      "[303 | 980.26] loss=2.26 avg=2.45\n",
      "[304 | 983.30] loss=1.75 avg=2.44\n",
      "[305 | 986.32] loss=2.28 avg=2.44\n",
      "[306 | 989.34] loss=2.27 avg=2.44\n",
      "[307 | 992.37] loss=2.56 avg=2.44\n",
      "[308 | 995.40] loss=2.55 avg=2.44\n",
      "[309 | 998.42] loss=2.12 avg=2.44\n",
      "[310 | 1001.44] loss=2.47 avg=2.44\n",
      "[311 | 1004.47] loss=2.49 avg=2.44\n",
      "[312 | 1007.50] loss=2.97 avg=2.44\n",
      "[313 | 1010.54] loss=2.66 avg=2.44\n",
      "[314 | 1013.55] loss=2.90 avg=2.45\n",
      "[315 | 1016.58] loss=2.69 avg=2.45\n",
      "[316 | 1019.62] loss=2.37 avg=2.45\n",
      "[317 | 1022.64] loss=2.70 avg=2.45\n",
      "[318 | 1025.66] loss=2.54 avg=2.45\n",
      "[319 | 1028.69] loss=2.53 avg=2.46\n",
      "[320 | 1031.72] loss=2.56 avg=2.46\n",
      "[321 | 1034.75] loss=2.16 avg=2.45\n",
      "[322 | 1037.78] loss=2.17 avg=2.45\n",
      "[323 | 1040.82] loss=2.09 avg=2.45\n",
      "[324 | 1043.85] loss=2.25 avg=2.44\n",
      "[325 | 1046.88] loss=2.10 avg=2.44\n",
      "[326 | 1049.91] loss=3.07 avg=2.45\n",
      "[327 | 1052.93] loss=2.32 avg=2.45\n",
      "[328 | 1055.96] loss=2.47 avg=2.45\n",
      "[329 | 1059.00] loss=2.50 avg=2.45\n",
      "[330 | 1062.04] loss=2.03 avg=2.44\n",
      "[331 | 1065.07] loss=2.37 avg=2.44\n",
      "[332 | 1068.11] loss=2.45 avg=2.44\n",
      "[333 | 1071.13] loss=2.38 avg=2.44\n",
      "[334 | 1074.17] loss=2.42 avg=2.44\n",
      "[335 | 1077.20] loss=3.18 avg=2.45\n",
      "[336 | 1080.22] loss=2.39 avg=2.45\n",
      "[337 | 1083.26] loss=2.18 avg=2.45\n",
      "[338 | 1086.30] loss=2.09 avg=2.44\n",
      "[339 | 1089.32] loss=2.35 avg=2.44\n",
      "[340 | 1092.35] loss=2.39 avg=2.44\n",
      "[341 | 1095.39] loss=2.99 avg=2.45\n",
      "[342 | 1098.41] loss=2.64 avg=2.45\n",
      "[343 | 1101.44] loss=2.44 avg=2.45\n",
      "[344 | 1104.47] loss=2.37 avg=2.45\n",
      "[345 | 1107.49] loss=2.07 avg=2.44\n",
      "[346 | 1110.56] loss=2.31 avg=2.44\n",
      "[347 | 1113.64] loss=2.39 avg=2.44\n",
      "[348 | 1116.72] loss=2.69 avg=2.44\n",
      "[349 | 1119.81] loss=2.23 avg=2.44\n",
      "[350 | 1122.88] loss=2.22 avg=2.44\n",
      "[351 | 1125.95] loss=2.49 avg=2.44\n",
      "[352 | 1129.04] loss=1.84 avg=2.43\n",
      "[353 | 1132.11] loss=3.04 avg=2.44\n",
      "[354 | 1135.18] loss=2.15 avg=2.44\n",
      "[355 | 1138.28] loss=2.62 avg=2.44\n",
      "[356 | 1141.36] loss=2.14 avg=2.44\n",
      "[357 | 1144.44] loss=2.27 avg=2.43\n",
      "[358 | 1147.52] loss=2.20 avg=2.43\n",
      "[359 | 1150.59] loss=2.35 avg=2.43\n",
      "[360 | 1153.66] loss=2.92 avg=2.44\n",
      "[361 | 1156.74] loss=2.85 avg=2.44\n",
      "[362 | 1159.81] loss=2.98 avg=2.45\n",
      "[363 | 1162.90] loss=2.60 avg=2.45\n",
      "[364 | 1165.97] loss=3.07 avg=2.45\n",
      "[365 | 1169.07] loss=2.66 avg=2.46\n",
      "[366 | 1172.14] loss=2.23 avg=2.45\n",
      "[367 | 1175.22] loss=3.07 avg=2.46\n",
      "[368 | 1178.32] loss=2.72 avg=2.46\n",
      "[369 | 1181.37] loss=2.17 avg=2.46\n",
      "[370 | 1184.43] loss=2.40 avg=2.46\n",
      "[371 | 1187.49] loss=2.35 avg=2.46\n",
      "[372 | 1190.54] loss=2.49 avg=2.46\n",
      "[373 | 1193.60] loss=2.43 avg=2.46\n",
      "[374 | 1196.65] loss=2.38 avg=2.46\n",
      "[375 | 1199.71] loss=2.51 avg=2.46\n",
      "[376 | 1202.78] loss=2.57 avg=2.46\n",
      "[377 | 1205.83] loss=2.74 avg=2.46\n",
      "[378 | 1208.90] loss=2.76 avg=2.46\n",
      "[379 | 1212.00] loss=2.32 avg=2.46\n",
      "[380 | 1215.08] loss=2.45 avg=2.46\n",
      "[381 | 1218.15] loss=2.54 avg=2.46\n",
      "[382 | 1221.23] loss=2.38 avg=2.46\n",
      "[383 | 1224.30] loss=2.27 avg=2.46\n",
      "[384 | 1227.40] loss=2.45 avg=2.46\n",
      "[385 | 1230.49] loss=2.51 avg=2.46\n",
      "[386 | 1233.58] loss=2.40 avg=2.46\n",
      "[387 | 1236.69] loss=2.74 avg=2.46\n",
      "[388 | 1239.78] loss=2.40 avg=2.46\n",
      "[389 | 1242.88] loss=2.27 avg=2.46\n",
      "[390 | 1245.97] loss=3.12 avg=2.47\n",
      "[391 | 1249.04] loss=2.53 avg=2.47\n",
      "[392 | 1252.14] loss=2.25 avg=2.47\n",
      "[393 | 1255.24] loss=2.29 avg=2.46\n",
      "[394 | 1258.33] loss=2.17 avg=2.46\n",
      "[395 | 1261.41] loss=2.79 avg=2.46\n",
      "[396 | 1264.49] loss=2.20 avg=2.46\n",
      "[397 | 1267.58] loss=2.36 avg=2.46\n",
      "[398 | 1270.68] loss=2.45 avg=2.46\n",
      "[399 | 1273.75] loss=2.59 avg=2.46\n",
      "[400 | 1276.85] loss=2.19 avg=2.46\n",
      "[401 | 1279.92] loss=2.31 avg=2.46\n",
      "[402 | 1282.98] loss=2.89 avg=2.46\n",
      "[403 | 1286.02] loss=2.52 avg=2.46\n",
      "[404 | 1289.04] loss=2.56 avg=2.46\n",
      "[405 | 1292.07] loss=2.49 avg=2.46\n",
      "[406 | 1295.09] loss=2.36 avg=2.46\n",
      "[407 | 1298.12] loss=2.08 avg=2.46\n",
      "[408 | 1301.15] loss=2.81 avg=2.46\n",
      "[409 | 1304.18] loss=2.60 avg=2.46\n",
      "[410 | 1307.20] loss=2.37 avg=2.46\n",
      "[411 | 1310.23] loss=2.51 avg=2.46\n",
      "[412 | 1313.26] loss=2.43 avg=2.46\n",
      "[413 | 1316.29] loss=2.22 avg=2.46\n",
      "[414 | 1319.33] loss=2.22 avg=2.46\n",
      "[415 | 1322.36] loss=2.07 avg=2.45\n",
      "[416 | 1325.38] loss=2.66 avg=2.46\n",
      "[417 | 1328.41] loss=2.41 avg=2.46\n",
      "[418 | 1331.44] loss=2.32 avg=2.45\n",
      "[419 | 1334.46] loss=3.07 avg=2.46\n",
      "[420 | 1337.49] loss=2.91 avg=2.47\n",
      "[421 | 1340.52] loss=2.44 avg=2.46\n",
      "[422 | 1343.55] loss=2.29 avg=2.46\n",
      "[423 | 1346.57] loss=2.26 avg=2.46\n",
      "[424 | 1349.60] loss=1.92 avg=2.46\n",
      "[425 | 1352.64] loss=2.13 avg=2.45\n",
      "[426 | 1355.68] loss=2.28 avg=2.45\n",
      "[427 | 1358.72] loss=2.31 avg=2.45\n",
      "[428 | 1361.74] loss=2.19 avg=2.45\n",
      "[429 | 1364.76] loss=2.35 avg=2.45\n",
      "[430 | 1367.80] loss=2.60 avg=2.45\n",
      "[431 | 1370.83] loss=2.00 avg=2.44\n",
      "[432 | 1373.86] loss=2.34 avg=2.44\n",
      "[433 | 1376.90] loss=2.08 avg=2.44\n",
      "[434 | 1379.92] loss=2.20 avg=2.44\n",
      "[435 | 1382.95] loss=2.06 avg=2.43\n",
      "[436 | 1385.97] loss=2.42 avg=2.43\n",
      "[437 | 1389.01] loss=2.35 avg=2.43\n",
      "[438 | 1392.04] loss=2.08 avg=2.43\n",
      "[439 | 1395.07] loss=2.67 avg=2.43\n",
      "[440 | 1398.10] loss=2.70 avg=2.43\n",
      "[441 | 1401.14] loss=2.34 avg=2.43\n",
      "[442 | 1404.18] loss=2.50 avg=2.43\n",
      "[443 | 1407.21] loss=2.35 avg=2.43\n",
      "[444 | 1410.25] loss=2.12 avg=2.43\n",
      "[445 | 1413.29] loss=2.98 avg=2.43\n",
      "[446 | 1416.31] loss=2.12 avg=2.43\n",
      "[447 | 1419.33] loss=2.93 avg=2.44\n",
      "[448 | 1422.38] loss=2.49 avg=2.44\n",
      "[449 | 1425.46] loss=3.12 avg=2.44\n",
      "[450 | 1428.54] loss=2.19 avg=2.44\n",
      "[451 | 1431.64] loss=2.30 avg=2.44\n",
      "[452 | 1434.73] loss=2.28 avg=2.44\n",
      "[453 | 1437.80] loss=2.42 avg=2.44\n",
      "[454 | 1440.88] loss=1.98 avg=2.43\n",
      "[455 | 1443.96] loss=2.55 avg=2.43\n",
      "[456 | 1447.05] loss=2.90 avg=2.44\n",
      "[457 | 1450.14] loss=2.60 avg=2.44\n",
      "[458 | 1453.23] loss=2.14 avg=2.44\n",
      "[459 | 1456.34] loss=2.11 avg=2.43\n",
      "[460 | 1459.40] loss=2.05 avg=2.43\n",
      "[461 | 1462.52] loss=2.12 avg=2.43\n",
      "[462 | 1465.60] loss=2.67 avg=2.43\n",
      "[463 | 1468.67] loss=2.25 avg=2.43\n",
      "[464 | 1471.74] loss=2.19 avg=2.43\n",
      "[465 | 1474.79] loss=2.81 avg=2.43\n",
      "[466 | 1477.85] loss=2.54 avg=2.43\n",
      "[467 | 1480.91] loss=3.02 avg=2.44\n",
      "[468 | 1483.96] loss=2.44 avg=2.44\n",
      "[469 | 1487.02] loss=2.40 avg=2.44\n",
      "[470 | 1490.13] loss=2.14 avg=2.43\n",
      "[471 | 1493.21] loss=2.46 avg=2.43\n",
      "[472 | 1496.32] loss=2.32 avg=2.43\n",
      "[473 | 1499.42] loss=2.80 avg=2.44\n",
      "[474 | 1502.50] loss=2.63 avg=2.44\n",
      "[475 | 1505.60] loss=2.30 avg=2.44\n",
      "[476 | 1508.69] loss=2.45 avg=2.44\n",
      "[477 | 1511.79] loss=2.49 avg=2.44\n",
      "[478 | 1514.87] loss=2.01 avg=2.43\n",
      "[479 | 1517.96] loss=2.42 avg=2.43\n",
      "[480 | 1521.05] loss=2.21 avg=2.43\n",
      "[481 | 1524.14] loss=2.63 avg=2.43\n",
      "[482 | 1527.23] loss=2.23 avg=2.43\n",
      "[483 | 1530.31] loss=2.33 avg=2.43\n",
      "[484 | 1533.38] loss=2.89 avg=2.43\n",
      "[485 | 1536.41] loss=2.56 avg=2.44\n",
      "[486 | 1539.43] loss=2.49 avg=2.44\n",
      "[487 | 1542.47] loss=2.39 avg=2.44\n",
      "[488 | 1545.51] loss=2.31 avg=2.43\n",
      "[489 | 1548.54] loss=2.33 avg=2.43\n",
      "[490 | 1551.57] loss=2.36 avg=2.43\n",
      "[491 | 1554.60] loss=2.05 avg=2.43\n",
      "[492 | 1557.61] loss=2.51 avg=2.43\n",
      "[493 | 1560.64] loss=2.10 avg=2.43\n",
      "[494 | 1563.68] loss=2.75 avg=2.43\n",
      "[495 | 1566.71] loss=2.09 avg=2.43\n",
      "[496 | 1569.73] loss=2.46 avg=2.43\n",
      "[497 | 1572.75] loss=2.18 avg=2.42\n",
      "[498 | 1575.77] loss=2.33 avg=2.42\n",
      "[499 | 1578.80] loss=2.43 avg=2.42\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "7440) shane: is that an automatic install for rt310k?\n",
      "(1183768560) jk_: yes, that's what you see from the console\n",
      "(1183768160) jk_: what about you?\n",
      "(1183768160) shane: it doesn't give me the option to reboot, you have to do something\n",
      "(1183768260) jk_: does the ubuntu installer have this command\n",
      "(1183768320) shane: it just tells me 'Installation of update failed and you will not be able to install a normal update?' and I have to choose reboot for it to work\n",
      "(1183768280) jk_: if you're using a dual-boot, maybe it will say so on bootup\n",
      "(1183768300) jk_: if that's the case, maybe you just need to get the new installer and reboot first\n",
      "(1183768480) jk_: I did, it takes me a bit\n",
      "(1183768480) jk_: it takes quite a while, and it does a lot of work in the process\n",
      "(1183768540) jk_: don't reboot you can do that, right?\n",
      "(1183768660) shane: if not, then I can't think of anything else that has the name i need\n",
      "(1183768680) jk_: no, you cant install the same install on a different machine and find out what it is\n",
      "(1183768680) jk_: it will be different only with the partition it's on\n",
      "(1183768680) jk_: you see ? a few others do it but none that will give me the same problems on different versions\n",
      "(1183768680) jk_: try the same install with different files\n",
      "(1190418220) jk_: if only the install with the same data is working it does not mean the installer works\n",
      "(1190418220) jk_: if you do install the same install twice with different files it will work, but it wont work since it is a backup program\n",
      "(1190418320) shane: ok, let me copy the .iso from my laptop into my laptop\n",
      "(1190418640) jk_: does that work, I don't get the error\n",
      "(1190418640) shane: yes it does\n",
      "(1190418560) jk_: you get the same error the next time you try to do the install on that machine\n",
      "(1190418620) jk_: you need to check on whether the system is using the same installation or different\n",
      "(1190418620) jk_: yes\n",
      "(1190418680) jk_: in the console type 'install' and it'll do the install from start with 'install error'\n",
      "(1190426380) jk_: do you need to do a boot-up?'\n",
      "(1190426380) shane: do you actually have another hard drive with a CD?\n",
      "(1190501100) jk_: yes\n",
      "(1196075500) jk_: what's it supposed to do for you when you need to fix something or make something better?\n",
      "(1196046560) jk_: what's it supposed to do now\n",
      "(1196145540) jk_: if you can't type sudo, you can use sudo instead, to run the command, which is apt-get install upgrade\n",
      "(1196145660) jk_: and you can then type it off the console, and type apt-get install upgrade\n",
      "(1196145720) jk_: yes? if you can't type sudo do you know what command you need to use to do anything with root, and what's that command you got in your terminal\n",
      "(1196145720) shane: you need to type it off the console\n",
      "(1196145780) jk_: it's apt-get install update\n",
      "(1196145960) jk_: its apt-get install update, there will be a 'sudo' to fix up, which is apt-get update\n",
      "(1196160620) jk_: I did what?\n",
      "(1196174700) shane: what is it that says it doesn't accept my command?\n",
      "(1196174560) jk_: no, not that I know of\n",
      "(1196174680) jk_: apt-get is like aptitude, you don't use apt-get\n",
      "(1196174780) jk_: apt-get is like aptitude, you don't use apt-get, i got used to it, there are many solutions\n",
      "(1196174860) jk_: it helps you do something else\n",
      "\n",
      "[500 | 1618.23] loss=2.37 avg=2.42\n",
      "[501 | 1621.28] loss=2.22 avg=2.42\n",
      "[502 | 1624.32] loss=2.39 avg=2.42\n",
      "[503 | 1627.36] loss=2.12 avg=2.42\n",
      "[504 | 1630.40] loss=2.45 avg=2.42\n",
      "[505 | 1633.44] loss=2.60 avg=2.42\n",
      "[506 | 1636.48] loss=2.36 avg=2.42\n",
      "[507 | 1639.51] loss=2.54 avg=2.42\n",
      "[508 | 1642.55] loss=2.06 avg=2.42\n",
      "[509 | 1645.57] loss=2.20 avg=2.41\n",
      "[510 | 1648.60] loss=2.30 avg=2.41\n",
      "[511 | 1651.64] loss=2.45 avg=2.41\n",
      "[512 | 1654.67] loss=2.10 avg=2.41\n",
      "[513 | 1657.72] loss=2.65 avg=2.41\n",
      "[514 | 1660.74] loss=2.05 avg=2.41\n",
      "[515 | 1663.77] loss=2.20 avg=2.41\n",
      "[516 | 1666.82] loss=2.29 avg=2.41\n",
      "[517 | 1669.85] loss=2.51 avg=2.41\n",
      "[518 | 1672.89] loss=2.61 avg=2.41\n",
      "[519 | 1675.91] loss=2.12 avg=2.41\n",
      "[520 | 1679.01] loss=2.54 avg=2.41\n",
      "[521 | 1682.08] loss=2.78 avg=2.41\n",
      "[522 | 1685.17] loss=2.44 avg=2.41\n",
      "[523 | 1688.26] loss=2.42 avg=2.41\n",
      "[524 | 1691.34] loss=2.29 avg=2.41\n",
      "[525 | 1694.42] loss=2.28 avg=2.41\n",
      "[526 | 1697.52] loss=2.72 avg=2.41\n",
      "[527 | 1700.59] loss=2.58 avg=2.41\n",
      "[528 | 1703.70] loss=1.94 avg=2.41\n",
      "[529 | 1706.80] loss=2.31 avg=2.41\n",
      "[530 | 1709.88] loss=2.26 avg=2.41\n",
      "[531 | 1712.98] loss=2.89 avg=2.41\n",
      "[532 | 1716.07] loss=2.40 avg=2.41\n",
      "[533 | 1719.16] loss=2.67 avg=2.41\n",
      "[534 | 1722.24] loss=2.30 avg=2.41\n",
      "[535 | 1725.35] loss=2.22 avg=2.41\n",
      "[536 | 1728.40] loss=2.33 avg=2.41\n",
      "[537 | 1731.46] loss=2.63 avg=2.41\n",
      "[538 | 1734.53] loss=3.06 avg=2.42\n",
      "[539 | 1737.58] loss=3.01 avg=2.42\n",
      "[540 | 1740.64] loss=2.19 avg=2.42\n",
      "[541 | 1743.69] loss=2.34 avg=2.42\n",
      "[542 | 1746.75] loss=2.93 avg=2.43\n",
      "[543 | 1749.83] loss=2.19 avg=2.42\n",
      "[544 | 1752.92] loss=2.51 avg=2.42\n",
      "[545 | 1756.03] loss=2.29 avg=2.42\n",
      "[546 | 1759.13] loss=2.20 avg=2.42\n",
      "[547 | 1762.23] loss=2.38 avg=2.42\n",
      "[548 | 1765.32] loss=2.45 avg=2.42\n",
      "[549 | 1768.42] loss=2.34 avg=2.42\n",
      "[550 | 1771.53] loss=2.28 avg=2.42\n",
      "[551 | 1774.61] loss=2.78 avg=2.42\n",
      "[552 | 1777.70] loss=2.55 avg=2.42\n",
      "[553 | 1780.76] loss=2.43 avg=2.42\n",
      "[554 | 1783.86] loss=3.05 avg=2.43\n",
      "[555 | 1786.94] loss=2.18 avg=2.43\n",
      "[556 | 1790.04] loss=2.22 avg=2.43\n",
      "[557 | 1793.12] loss=2.45 avg=2.43\n",
      "[558 | 1796.22] loss=2.82 avg=2.43\n",
      "[559 | 1799.30] loss=2.90 avg=2.43\n",
      "[560 | 1802.32] loss=2.45 avg=2.43\n",
      "[561 | 1805.35] loss=2.22 avg=2.43\n",
      "[562 | 1808.37] loss=2.45 avg=2.43\n",
      "[563 | 1811.40] loss=2.30 avg=2.43\n",
      "[564 | 1814.43] loss=2.25 avg=2.43\n",
      "[565 | 1817.46] loss=2.20 avg=2.43\n",
      "[566 | 1820.49] loss=2.87 avg=2.43\n",
      "[567 | 1823.52] loss=2.20 avg=2.43\n",
      "[568 | 1826.55] loss=2.46 avg=2.43\n",
      "[569 | 1829.57] loss=2.77 avg=2.43\n",
      "[570 | 1832.61] loss=2.20 avg=2.43\n",
      "[571 | 1835.64] loss=2.15 avg=2.43\n",
      "[572 | 1838.67] loss=2.63 avg=2.43\n",
      "[573 | 1841.70] loss=2.32 avg=2.43\n",
      "[574 | 1844.74] loss=2.17 avg=2.43\n",
      "[575 | 1847.77] loss=2.99 avg=2.43\n",
      "[576 | 1850.80] loss=2.55 avg=2.43\n",
      "[577 | 1853.83] loss=2.30 avg=2.43\n",
      "[578 | 1856.85] loss=2.34 avg=2.43\n",
      "[579 | 1859.88] loss=2.22 avg=2.43\n",
      "[580 | 1862.92] loss=2.56 avg=2.43\n",
      "[581 | 1865.95] loss=2.11 avg=2.43\n",
      "[582 | 1868.98] loss=2.28 avg=2.43\n",
      "[583 | 1872.02] loss=2.47 avg=2.43\n",
      "[584 | 1875.04] loss=2.12 avg=2.42\n",
      "[585 | 1878.06] loss=2.20 avg=2.42\n",
      "[586 | 1881.09] loss=2.56 avg=2.42\n",
      "[587 | 1884.13] loss=2.54 avg=2.42\n",
      "[588 | 1887.16] loss=2.61 avg=2.42\n",
      "[589 | 1890.20] loss=2.33 avg=2.42\n",
      "[590 | 1893.23] loss=2.60 avg=2.43\n",
      "[591 | 1896.25] loss=2.00 avg=2.42\n",
      "[592 | 1899.28] loss=2.79 avg=2.42\n",
      "[593 | 1902.31] loss=2.59 avg=2.43\n",
      "[594 | 1905.34] loss=2.73 avg=2.43\n",
      "[595 | 1908.38] loss=2.26 avg=2.43\n",
      "[596 | 1911.41] loss=2.12 avg=2.42\n",
      "[597 | 1914.43] loss=2.69 avg=2.43\n",
      "[598 | 1917.46] loss=2.52 avg=2.43\n",
      "[599 | 1920.48] loss=1.92 avg=2.42\n",
      "[600 | 1923.52] loss=2.18 avg=2.42\n",
      "[601 | 1926.54] loss=2.27 avg=2.42\n",
      "[602 | 1929.58] loss=2.17 avg=2.42\n",
      "[603 | 1932.65] loss=2.35 avg=2.42\n",
      "[604 | 1935.73] loss=2.34 avg=2.42\n",
      "[605 | 1938.82] loss=2.13 avg=2.41\n",
      "[606 | 1941.93] loss=2.28 avg=2.41\n",
      "[607 | 1945.02] loss=2.35 avg=2.41\n",
      "[608 | 1948.09] loss=2.18 avg=2.41\n",
      "[609 | 1951.20] loss=2.64 avg=2.41\n",
      "[610 | 1954.28] loss=2.40 avg=2.41\n",
      "[611 | 1957.36] loss=2.66 avg=2.41\n",
      "[612 | 1960.44] loss=2.15 avg=2.41\n",
      "[613 | 1963.49] loss=2.08 avg=2.41\n",
      "[614 | 1966.56] loss=2.62 avg=2.41\n",
      "[615 | 1969.62] loss=2.19 avg=2.41\n",
      "[616 | 1972.71] loss=2.72 avg=2.41\n",
      "[617 | 1975.80] loss=2.97 avg=2.42\n",
      "[618 | 1978.90] loss=2.54 avg=2.42\n",
      "[619 | 1981.99] loss=2.50 avg=2.42\n",
      "[620 | 1985.08] loss=2.92 avg=2.42\n",
      "[621 | 1988.17] loss=2.44 avg=2.42\n",
      "[622 | 1991.25] loss=3.01 avg=2.43\n",
      "[623 | 1994.33] loss=2.33 avg=2.43\n",
      "[624 | 1997.42] loss=2.78 avg=2.43\n",
      "[625 | 2000.50] loss=2.80 avg=2.44\n",
      "[626 | 2003.55] loss=2.36 avg=2.43\n",
      "[627 | 2006.57] loss=2.95 avg=2.44\n",
      "[628 | 2009.59] loss=2.35 avg=2.44\n",
      "[629 | 2012.62] loss=2.14 avg=2.44\n",
      "[630 | 2015.65] loss=2.63 avg=2.44\n",
      "[631 | 2018.67] loss=2.26 avg=2.44\n",
      "[632 | 2021.70] loss=2.32 avg=2.43\n",
      "[633 | 2024.73] loss=2.24 avg=2.43\n",
      "[634 | 2027.77] loss=2.77 avg=2.44\n",
      "[635 | 2030.80] loss=2.22 avg=2.43\n",
      "[636 | 2033.83] loss=2.52 avg=2.44\n",
      "[637 | 2036.86] loss=2.19 avg=2.43\n",
      "[638 | 2039.88] loss=2.34 avg=2.43\n",
      "[639 | 2042.90] loss=2.25 avg=2.43\n",
      "[640 | 2045.95] loss=2.47 avg=2.43\n",
      "[641 | 2048.99] loss=2.48 avg=2.43\n",
      "[642 | 2052.02] loss=2.66 avg=2.43\n",
      "[643 | 2055.06] loss=3.07 avg=2.44\n",
      "[644 | 2058.09] loss=2.44 avg=2.44\n",
      "[645 | 2061.12] loss=2.51 avg=2.44\n",
      "[646 | 2064.14] loss=2.99 avg=2.45\n",
      "[647 | 2067.18] loss=2.34 avg=2.44\n",
      "[648 | 2070.22] loss=2.26 avg=2.44\n",
      "[649 | 2073.25] loss=2.40 avg=2.44\n",
      "[650 | 2076.29] loss=2.39 avg=2.44\n",
      "[651 | 2079.32] loss=2.42 avg=2.44\n",
      "[652 | 2082.35] loss=2.24 avg=2.44\n",
      "[653 | 2085.38] loss=2.37 avg=2.44\n",
      "[654 | 2088.42] loss=2.43 avg=2.44\n",
      "[655 | 2091.46] loss=2.61 avg=2.44\n",
      "[656 | 2094.49] loss=2.18 avg=2.44\n",
      "[657 | 2097.52] loss=2.52 avg=2.44\n",
      "[658 | 2100.56] loss=2.07 avg=2.44\n",
      "[659 | 2103.58] loss=2.29 avg=2.43\n",
      "[660 | 2106.61] loss=2.58 avg=2.44\n",
      "[661 | 2109.65] loss=2.35 avg=2.43\n",
      "[662 | 2112.67] loss=2.10 avg=2.43\n",
      "[663 | 2115.71] loss=2.52 avg=2.43\n",
      "[664 | 2118.75] loss=2.88 avg=2.44\n",
      "[665 | 2121.79] loss=2.35 avg=2.44\n",
      "[666 | 2124.83] loss=2.28 avg=2.43\n",
      "[667 | 2127.84] loss=2.68 avg=2.44\n",
      "[668 | 2130.87] loss=2.36 avg=2.44\n",
      "[669 | 2133.90] loss=1.92 avg=2.43\n",
      "[670 | 2136.95] loss=2.42 avg=2.43\n",
      "[671 | 2140.03] loss=2.37 avg=2.43\n",
      "[672 | 2143.12] loss=2.30 avg=2.43\n",
      "[673 | 2146.19] loss=2.33 avg=2.43\n",
      "[674 | 2149.28] loss=2.72 avg=2.43\n",
      "[675 | 2152.35] loss=2.31 avg=2.43\n",
      "[676 | 2155.43] loss=2.39 avg=2.43\n",
      "[677 | 2158.52] loss=2.45 avg=2.43\n",
      "[678 | 2161.60] loss=2.35 avg=2.43\n",
      "[679 | 2164.70] loss=1.83 avg=2.42\n",
      "[680 | 2167.79] loss=2.14 avg=2.42\n",
      "[681 | 2170.88] loss=2.09 avg=2.42\n",
      "[682 | 2173.92] loss=3.12 avg=2.42\n",
      "[683 | 2176.98] loss=2.39 avg=2.42\n",
      "[684 | 2180.04] loss=1.96 avg=2.42\n",
      "[685 | 2183.11] loss=2.84 avg=2.42\n",
      "[686 | 2186.17] loss=2.54 avg=2.42\n",
      "[687 | 2189.27] loss=2.79 avg=2.43\n",
      "[688 | 2192.35] loss=1.98 avg=2.42\n",
      "[689 | 2195.45] loss=2.35 avg=2.42\n",
      "[690 | 2198.54] loss=1.86 avg=2.42\n",
      "[691 | 2201.65] loss=2.36 avg=2.42\n",
      "[692 | 2204.74] loss=2.27 avg=2.41\n",
      "[693 | 2207.81] loss=2.53 avg=2.42\n",
      "[694 | 2210.89] loss=2.13 avg=2.41\n",
      "[695 | 2213.99] loss=3.19 avg=2.42\n",
      "[696 | 2217.08] loss=2.94 avg=2.43\n",
      "[697 | 2220.17] loss=2.09 avg=2.42\n",
      "[698 | 2223.26] loss=2.24 avg=2.42\n",
      "[699 | 2226.31] loss=2.22 avg=2.42\n",
      "[700 | 2229.34] loss=2.03 avg=2.41\n",
      "[701 | 2232.37] loss=2.30 avg=2.41\n",
      "[702 | 2235.40] loss=2.70 avg=2.42\n",
      "[703 | 2238.43] loss=2.40 avg=2.42\n",
      "[704 | 2241.46] loss=1.93 avg=2.41\n",
      "[705 | 2244.49] loss=2.98 avg=2.42\n",
      "[706 | 2247.51] loss=2.47 avg=2.42\n",
      "[707 | 2250.54] loss=3.08 avg=2.42\n",
      "[708 | 2253.57] loss=2.16 avg=2.42\n",
      "[709 | 2256.60] loss=1.89 avg=2.42\n",
      "[710 | 2259.62] loss=2.48 avg=2.42\n",
      "[711 | 2262.64] loss=2.62 avg=2.42\n",
      "[712 | 2265.67] loss=3.06 avg=2.42\n",
      "[713 | 2268.70] loss=2.58 avg=2.43\n",
      "[714 | 2271.72] loss=2.06 avg=2.42\n",
      "[715 | 2274.74] loss=2.25 avg=2.42\n",
      "[716 | 2277.77] loss=2.22 avg=2.42\n",
      "[717 | 2280.81] loss=2.20 avg=2.42\n",
      "[718 | 2283.84] loss=2.98 avg=2.42\n",
      "[719 | 2286.87] loss=2.48 avg=2.42\n",
      "[720 | 2289.91] loss=2.19 avg=2.42\n",
      "[721 | 2292.93] loss=2.67 avg=2.42\n",
      "[722 | 2295.95] loss=2.59 avg=2.42\n",
      "[723 | 2298.98] loss=2.47 avg=2.43\n",
      "[724 | 2302.01] loss=2.36 avg=2.42\n",
      "[725 | 2305.03] loss=3.07 avg=2.43\n",
      "[726 | 2308.07] loss=2.97 avg=2.44\n",
      "[727 | 2311.10] loss=2.17 avg=2.43\n",
      "[728 | 2314.13] loss=2.28 avg=2.43\n",
      "[729 | 2317.15] loss=2.17 avg=2.43\n",
      "[730 | 2320.18] loss=2.43 avg=2.43\n",
      "[731 | 2323.21] loss=2.25 avg=2.43\n",
      "[732 | 2326.24] loss=2.57 avg=2.43\n",
      "[733 | 2329.27] loss=2.18 avg=2.43\n",
      "[734 | 2332.31] loss=1.98 avg=2.42\n",
      "[735 | 2335.33] loss=2.33 avg=2.42\n",
      "[736 | 2338.36] loss=2.27 avg=2.42\n",
      "[737 | 2341.39] loss=2.16 avg=2.42\n",
      "[738 | 2344.42] loss=2.22 avg=2.42\n",
      "[739 | 2347.46] loss=2.07 avg=2.41\n",
      "[740 | 2350.49] loss=2.32 avg=2.41\n",
      "[741 | 2353.52] loss=2.16 avg=2.41\n",
      "[742 | 2356.54] loss=3.01 avg=2.41\n",
      "[743 | 2359.56] loss=2.22 avg=2.41\n",
      "[744 | 2362.59] loss=2.00 avg=2.41\n",
      "[745 | 2365.63] loss=2.17 avg=2.41\n",
      "[746 | 2368.66] loss=2.54 avg=2.41\n",
      "[747 | 2371.70] loss=1.82 avg=2.40\n",
      "[748 | 2374.73] loss=2.28 avg=2.40\n",
      "[749 | 2377.76] loss=2.12 avg=2.40\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      " Still, I can understand that you can make a folder for the /home partition, /share, and then run mkdir -p /home/, if those directories are already existing, or if you're not sure which ones to create first.\n",
      "(1206374400) kvk: that doesnt allow you to create that folder if I assume you're already there, would you?\n",
      "(1206374460) kvk: I know that's a bit of a shortcut.\n",
      "(1206374520) kvk: I had the same issue. but then I made a folder just for this.\n",
      "(1206374580) kvk: did you try making your own /home folder ?\n",
      "(1206374640) kvk: well, it's a common problem that you run into too.\n",
      "(1206374640) kvk: yeah, I suppose you could, that would be cool (I think it would be even more cool if you could use the commands on the desktop to navigate back to the user panel, or if you could use the keyboard to navigate to the other partitions).\n",
      "(1206374800) kvk: well, that way you can create a new /home partition if your computers are already there, and the current ones won't be needed, if you can create the home partition for the correct directory\n",
      "\n",
      "\n",
      "(1179162740) _gasp: no, this isn't a good time, so let's do this in another window then use sudo:\n",
      "(1179162740) _gasp: I see - sudo, but it's a way to change permissions of file permissions.  If you don't use sudo you can't change the file permissions\n",
      "(1179162800) ikonia: i try that and it doesn't work, maybe that's why the sudo will be the right way\n",
      "(117916300) ikonia: and sudo?\n",
      "(11791660) ikonia: it doesn't say sudo is using sudo, only a 'sudo -s'\n",
      "(1179166220) ikonia: sudo -S allows you to create symbolic links, so i don't see how the sudo is in use to add the sudo\n",
      "(1179167320) ikonia: just ask the question, you don't need sudo in a non-english language, the sudo command can be added as the last command\n",
      "(1179167320) ikonia: you don't need sudo in a non-english language; the sudo command can be added as the last command\n",
      "(1179167380) ikonia: sudo -X doesn't allow you to use sudo from a non-english language; just ask the question\n",
      "\n",
      "\n",
      "(1327488880) shubatat: I have an idea where I need to be in terms of setting up the wireless network. This is a wireless network, not a wired network, and we've had a few ideas about how to set that up (I could even have a laptop with the wireless on the same wireless and some desktops on the same wireless), only that my idea was to set up wireless to not pick up WiFi, and maybe to try to find WiFi hotspots that are offline but could easily be found on the network.\n",
      "(1327492000) shubatat: Is that the plan?\n",
      "(1327492200) shubatat: Is that the plan of the wireless router?\n",
      "(1327492260) shubatat: how do I find out if a router is already on, and to do that I have to set an environment variable?\n",
      "(1327492380) yoyr: use netstat\n",
      "(1327492440) yoyr: you can also try to grep the /etc/network/interfaces and see if that helps\n",
      "(1327492440) yoyr: you may have to search your /etc/network/interfaces file\n",
      "(1327487000) yoyr: look in the file for your name\n",
      "\n",
      "\n",
      "(1186789160) dr_dude: aha... now I see why this guy was on a bad track? The manpage says his wireless card is not supported on Ubuntu\n",
      "\n",
      "\n",
      "(1197949160) yaz_: how do i download or unpack it ?\n",
      "(1197949220) yaz_: do any of you have access to a firewall on the router ?\n",
      "(1197949220) dr_dude: I have, I do not.\n",
      "(1197949280) yaz_: i dont.\n",
      "(1197949280) dr_dude: it's not there.\n",
      "(1197949280) yaz_: does it show on a web browser?\n",
      "(1197949280\n",
      "\n",
      "[750 | 2417.84] loss=2.75 avg=2.40\n",
      "[751 | 2420.92] loss=2.27 avg=2.40\n",
      "[752 | 2424.00] loss=2.17 avg=2.40\n",
      "[753 | 2427.08] loss=2.13 avg=2.39\n",
      "[754 | 2430.14] loss=2.35 avg=2.39\n",
      "[755 | 2433.22] loss=2.36 avg=2.39\n",
      "[756 | 2436.33] loss=2.51 avg=2.40\n",
      "[757 | 2439.42] loss=2.78 avg=2.40\n",
      "[758 | 2442.52] loss=2.06 avg=2.40\n",
      "[759 | 2445.62] loss=2.60 avg=2.40\n",
      "[760 | 2448.71] loss=1.80 avg=2.39\n",
      "[761 | 2451.80] loss=2.54 avg=2.39\n",
      "[762 | 2454.88] loss=2.47 avg=2.39\n",
      "[763 | 2457.96] loss=2.63 avg=2.40\n",
      "[764 | 2461.04] loss=2.44 avg=2.40\n",
      "[765 | 2464.07] loss=3.05 avg=2.40\n",
      "[766 | 2467.11] loss=1.93 avg=2.40\n",
      "[767 | 2470.15] loss=2.50 avg=2.40\n",
      "[768 | 2473.17] loss=2.12 avg=2.40\n",
      "[769 | 2476.20] loss=2.85 avg=2.40\n",
      "[770 | 2479.23] loss=1.72 avg=2.39\n",
      "[771 | 2482.25] loss=2.47 avg=2.40\n",
      "[772 | 2485.28] loss=2.51 avg=2.40\n",
      "[773 | 2488.31] loss=2.48 avg=2.40\n",
      "[774 | 2491.34] loss=2.58 avg=2.40\n",
      "[775 | 2494.36] loss=2.09 avg=2.40\n",
      "[776 | 2497.39] loss=2.14 avg=2.39\n",
      "[777 | 2500.42] loss=2.30 avg=2.39\n",
      "[778 | 2503.45] loss=2.18 avg=2.39\n",
      "[779 | 2506.47] loss=2.40 avg=2.39\n",
      "[780 | 2509.50] loss=2.32 avg=2.39\n",
      "[781 | 2512.54] loss=2.47 avg=2.39\n",
      "[782 | 2515.56] loss=2.61 avg=2.39\n",
      "[783 | 2518.59] loss=2.75 avg=2.40\n",
      "[784 | 2521.62] loss=2.26 avg=2.39\n",
      "[785 | 2524.65] loss=2.34 avg=2.39\n",
      "[786 | 2527.68] loss=2.41 avg=2.39\n",
      "[787 | 2530.71] loss=2.90 avg=2.40\n",
      "[788 | 2533.74] loss=3.00 avg=2.41\n",
      "[789 | 2536.77] loss=2.27 avg=2.40\n",
      "[790 | 2539.80] loss=2.18 avg=2.40\n",
      "[791 | 2542.82] loss=2.44 avg=2.40\n",
      "[792 | 2545.85] loss=2.22 avg=2.40\n",
      "[793 | 2548.88] loss=3.41 avg=2.41\n",
      "[794 | 2551.91] loss=2.28 avg=2.41\n",
      "[795 | 2554.94] loss=2.00 avg=2.41\n",
      "[796 | 2557.96] loss=2.20 avg=2.40\n",
      "[797 | 2560.98] loss=2.36 avg=2.40\n",
      "[798 | 2564.01] loss=1.86 avg=2.40\n",
      "[799 | 2567.04] loss=2.13 avg=2.39\n",
      "[800 | 2570.07] loss=2.63 avg=2.40\n",
      "[801 | 2573.10] loss=3.00 avg=2.40\n",
      "[802 | 2576.14] loss=2.55 avg=2.40\n",
      "[803 | 2579.16] loss=2.57 avg=2.41\n",
      "[804 | 2582.20] loss=2.34 avg=2.41\n",
      "[805 | 2585.22] loss=2.01 avg=2.40\n",
      "[806 | 2588.25] loss=2.48 avg=2.40\n",
      "[807 | 2591.27] loss=2.23 avg=2.40\n",
      "[808 | 2594.31] loss=2.50 avg=2.40\n",
      "[809 | 2597.39] loss=2.64 avg=2.40\n",
      "[810 | 2600.49] loss=2.29 avg=2.40\n",
      "[811 | 2603.58] loss=2.17 avg=2.40\n",
      "[812 | 2606.66] loss=3.11 avg=2.41\n",
      "[813 | 2609.75] loss=2.56 avg=2.41\n",
      "[814 | 2612.81] loss=2.27 avg=2.41\n",
      "[815 | 2615.93] loss=2.38 avg=2.41\n",
      "[816 | 2618.99] loss=2.12 avg=2.40\n",
      "[817 | 2622.08] loss=2.15 avg=2.40\n",
      "[818 | 2625.19] loss=2.57 avg=2.40\n",
      "[819 | 2628.30] loss=2.52 avg=2.40\n",
      "[820 | 2631.41] loss=2.80 avg=2.41\n",
      "[821 | 2634.51] loss=1.82 avg=2.40\n",
      "[822 | 2637.60] loss=2.20 avg=2.40\n",
      "[823 | 2640.70] loss=2.53 avg=2.40\n",
      "[824 | 2643.77] loss=2.65 avg=2.40\n",
      "[825 | 2646.82] loss=2.92 avg=2.41\n",
      "[826 | 2649.88] loss=2.20 avg=2.41\n",
      "[827 | 2652.94] loss=2.90 avg=2.41\n",
      "[828 | 2655.99] loss=2.33 avg=2.41\n",
      "[829 | 2659.05] loss=2.81 avg=2.42\n",
      "[830 | 2662.10] loss=2.64 avg=2.42\n",
      "[831 | 2665.19] loss=2.45 avg=2.42\n",
      "[832 | 2668.29] loss=2.20 avg=2.42\n",
      "[833 | 2671.37] loss=2.21 avg=2.41\n",
      "[834 | 2674.46] loss=2.79 avg=2.42\n",
      "[835 | 2677.55] loss=2.98 avg=2.42\n",
      "[836 | 2680.64] loss=2.68 avg=2.43\n",
      "[837 | 2683.75] loss=2.19 avg=2.42\n",
      "[838 | 2686.84] loss=2.09 avg=2.42\n",
      "[839 | 2689.95] loss=2.14 avg=2.42\n",
      "[840 | 2693.04] loss=2.48 avg=2.42\n",
      "[841 | 2696.15] loss=2.46 avg=2.42\n",
      "[842 | 2699.24] loss=1.80 avg=2.41\n",
      "[843 | 2702.34] loss=2.61 avg=2.41\n",
      "[844 | 2705.41] loss=2.90 avg=2.42\n",
      "[845 | 2708.50] loss=2.44 avg=2.42\n",
      "[846 | 2711.53] loss=2.95 avg=2.42\n",
      "[847 | 2714.56] loss=2.57 avg=2.43\n",
      "[848 | 2717.59] loss=2.16 avg=2.42\n",
      "[849 | 2720.62] loss=2.28 avg=2.42\n",
      "[850 | 2723.65] loss=2.60 avg=2.42\n",
      "[851 | 2726.68] loss=2.73 avg=2.43\n",
      "[852 | 2729.70] loss=2.38 avg=2.43\n",
      "[853 | 2732.72] loss=2.31 avg=2.43\n",
      "[854 | 2735.75] loss=2.33 avg=2.42\n",
      "[855 | 2738.77] loss=2.15 avg=2.42\n",
      "[856 | 2741.81] loss=2.94 avg=2.43\n",
      "[857 | 2744.84] loss=2.91 avg=2.43\n",
      "[858 | 2747.86] loss=2.38 avg=2.43\n",
      "[859 | 2750.89] loss=2.38 avg=2.43\n",
      "[860 | 2753.93] loss=2.55 avg=2.43\n",
      "[861 | 2756.96] loss=3.05 avg=2.44\n",
      "[862 | 2759.99] loss=2.44 avg=2.44\n",
      "[863 | 2763.02] loss=2.18 avg=2.44\n",
      "[864 | 2766.04] loss=2.54 avg=2.44\n",
      "[865 | 2769.07] loss=2.22 avg=2.43\n",
      "[866 | 2772.10] loss=2.80 avg=2.44\n",
      "[867 | 2775.12] loss=2.23 avg=2.44\n",
      "[868 | 2778.15] loss=2.54 avg=2.44\n",
      "[869 | 2781.17] loss=2.48 avg=2.44\n",
      "[870 | 2784.20] loss=2.84 avg=2.44\n",
      "[871 | 2787.22] loss=2.23 avg=2.44\n",
      "[872 | 2790.26] loss=2.04 avg=2.44\n",
      "[873 | 2793.30] loss=2.78 avg=2.44\n",
      "[874 | 2796.32] loss=2.20 avg=2.44\n",
      "[875 | 2799.36] loss=2.35 avg=2.44\n",
      "[876 | 2802.41] loss=2.05 avg=2.43\n",
      "[877 | 2805.44] loss=2.89 avg=2.44\n",
      "[878 | 2808.47] loss=2.43 avg=2.44\n",
      "[879 | 2811.50] loss=2.32 avg=2.44\n",
      "[880 | 2814.53] loss=2.71 avg=2.44\n",
      "[881 | 2817.58] loss=2.90 avg=2.44\n",
      "[882 | 2820.61] loss=2.32 avg=2.44\n",
      "[883 | 2823.64] loss=2.21 avg=2.44\n",
      "[884 | 2826.68] loss=2.69 avg=2.44\n",
      "[885 | 2829.71] loss=2.79 avg=2.44\n",
      "[886 | 2832.75] loss=2.21 avg=2.44\n",
      "[887 | 2835.78] loss=2.23 avg=2.44\n",
      "[888 | 2838.82] loss=2.47 avg=2.44\n",
      "[889 | 2841.85] loss=2.53 avg=2.44\n",
      "[890 | 2844.88] loss=1.80 avg=2.44\n",
      "[891 | 2847.91] loss=2.93 avg=2.44\n",
      "[892 | 2850.95] loss=2.08 avg=2.44\n",
      "[893 | 2854.02] loss=2.94 avg=2.44\n",
      "[894 | 2857.11] loss=2.37 avg=2.44\n",
      "[895 | 2860.20] loss=2.11 avg=2.44\n",
      "[896 | 2863.30] loss=2.93 avg=2.44\n",
      "[897 | 2866.40] loss=2.48 avg=2.44\n",
      "[898 | 2869.46] loss=2.50 avg=2.44\n",
      "[899 | 2872.55] loss=2.68 avg=2.45\n",
      "[900 | 2875.62] loss=2.30 avg=2.44\n",
      "[901 | 2878.72] loss=2.46 avg=2.44\n",
      "[902 | 2881.78] loss=2.69 avg=2.45\n",
      "[903 | 2884.85] loss=2.02 avg=2.44\n",
      "[904 | 2887.92] loss=2.10 avg=2.44\n",
      "[905 | 2891.00] loss=2.37 avg=2.44\n",
      "[906 | 2894.11] loss=2.13 avg=2.44\n",
      "[907 | 2897.21] loss=2.37 avg=2.43\n",
      "[908 | 2900.29] loss=2.61 avg=2.44\n",
      "[909 | 2903.38] loss=2.52 avg=2.44\n",
      "[910 | 2906.47] loss=2.28 avg=2.44\n",
      "[911 | 2909.57] loss=2.53 avg=2.44\n",
      "[912 | 2912.70] loss=2.28 avg=2.44\n",
      "[913 | 2915.79] loss=2.00 avg=2.43\n",
      "[914 | 2918.87] loss=2.81 avg=2.43\n",
      "[915 | 2921.90] loss=2.16 avg=2.43\n",
      "[916 | 2924.94] loss=2.37 avg=2.43\n",
      "[917 | 2927.98] loss=2.38 avg=2.43\n",
      "[918 | 2931.02] loss=2.41 avg=2.43\n",
      "[919 | 2934.05] loss=2.52 avg=2.43\n",
      "[920 | 2937.08] loss=2.42 avg=2.43\n",
      "[921 | 2940.11] loss=2.42 avg=2.43\n",
      "[922 | 2943.15] loss=2.48 avg=2.43\n",
      "[923 | 2946.19] loss=2.56 avg=2.43\n",
      "[924 | 2949.22] loss=2.31 avg=2.43\n",
      "[925 | 2952.25] loss=2.22 avg=2.43\n",
      "[926 | 2955.28] loss=2.64 avg=2.43\n",
      "[927 | 2958.32] loss=2.71 avg=2.43\n",
      "[928 | 2961.36] loss=2.14 avg=2.43\n",
      "[929 | 2964.39] loss=2.75 avg=2.43\n",
      "[930 | 2967.42] loss=2.45 avg=2.43\n",
      "[931 | 2970.46] loss=2.36 avg=2.43\n",
      "[932 | 2973.50] loss=2.31 avg=2.43\n",
      "[933 | 2976.54] loss=2.54 avg=2.43\n",
      "[934 | 2979.57] loss=2.35 avg=2.43\n",
      "[935 | 2982.60] loss=2.52 avg=2.43\n",
      "[936 | 2985.64] loss=2.28 avg=2.43\n",
      "[937 | 2988.70] loss=2.61 avg=2.43\n",
      "[938 | 2991.73] loss=1.53 avg=2.43\n",
      "[939 | 2994.78] loss=2.96 avg=2.43\n",
      "[940 | 2997.81] loss=2.31 avg=2.43\n",
      "[941 | 3000.86] loss=2.39 avg=2.43\n",
      "[942 | 3003.89] loss=2.10 avg=2.43\n",
      "[943 | 3006.93] loss=2.63 avg=2.43\n",
      "[944 | 3009.97] loss=2.16 avg=2.42\n",
      "[945 | 3013.01] loss=2.46 avg=2.43\n",
      "[946 | 3016.05] loss=2.74 avg=2.43\n",
      "[947 | 3019.09] loss=2.50 avg=2.43\n",
      "[948 | 3022.13] loss=2.46 avg=2.43\n",
      "[949 | 3025.17] loss=2.93 avg=2.43\n",
      "[950 | 3028.22] loss=2.31 avg=2.43\n",
      "[951 | 3031.25] loss=2.07 avg=2.43\n",
      "[952 | 3034.28] loss=2.39 avg=2.43\n",
      "[953 | 3037.32] loss=2.23 avg=2.43\n",
      "[954 | 3040.35] loss=2.29 avg=2.43\n",
      "[955 | 3043.39] loss=2.20 avg=2.42\n",
      "[956 | 3046.42] loss=2.10 avg=2.42\n",
      "[957 | 3049.46] loss=2.29 avg=2.42\n",
      "[958 | 3052.49] loss=2.18 avg=2.42\n",
      "[959 | 3055.53] loss=2.06 avg=2.41\n",
      "[960 | 3058.58] loss=2.98 avg=2.42\n",
      "[961 | 3061.68] loss=2.20 avg=2.42\n",
      "[962 | 3064.79] loss=3.08 avg=2.42\n",
      "[963 | 3067.89] loss=2.50 avg=2.42\n",
      "[964 | 3070.99] loss=2.32 avg=2.42\n",
      "[965 | 3074.08] loss=2.14 avg=2.42\n",
      "[966 | 3077.14] loss=2.15 avg=2.42\n",
      "[967 | 3080.24] loss=2.63 avg=2.42\n",
      "[968 | 3083.32] loss=2.82 avg=2.42\n",
      "[969 | 3086.39] loss=2.30 avg=2.42\n",
      "[970 | 3089.45] loss=2.22 avg=2.42\n",
      "[971 | 3092.51] loss=2.24 avg=2.42\n",
      "[972 | 3095.58] loss=2.39 avg=2.42\n",
      "[973 | 3098.67] loss=2.44 avg=2.42\n",
      "[974 | 3101.77] loss=2.02 avg=2.41\n",
      "[975 | 3104.87] loss=2.14 avg=2.41\n",
      "[976 | 3107.96] loss=2.57 avg=2.41\n",
      "[977 | 3111.06] loss=2.41 avg=2.41\n",
      "[978 | 3114.16] loss=2.21 avg=2.41\n",
      "[979 | 3117.25] loss=2.72 avg=2.41\n",
      "[980 | 3120.34] loss=2.35 avg=2.41\n",
      "[981 | 3123.42] loss=2.71 avg=2.42\n",
      "[982 | 3126.45] loss=2.35 avg=2.42\n",
      "[983 | 3129.47] loss=1.68 avg=2.41\n",
      "[984 | 3132.50] loss=2.08 avg=2.41\n",
      "[985 | 3135.53] loss=2.45 avg=2.41\n",
      "[986 | 3138.57] loss=2.37 avg=2.41\n",
      "[987 | 3141.61] loss=2.54 avg=2.41\n",
      "[988 | 3144.65] loss=2.36 avg=2.41\n",
      "[989 | 3147.67] loss=2.40 avg=2.41\n",
      "[990 | 3150.71] loss=2.30 avg=2.40\n",
      "[991 | 3153.74] loss=2.04 avg=2.40\n",
      "[992 | 3156.76] loss=2.40 avg=2.40\n",
      "[993 | 3159.80] loss=2.29 avg=2.40\n",
      "[994 | 3162.82] loss=1.80 avg=2.39\n",
      "[995 | 3165.86] loss=2.32 avg=2.39\n",
      "[996 | 3168.89] loss=2.52 avg=2.39\n",
      "[997 | 3171.92] loss=2.47 avg=2.40\n",
      "[998 | 3174.95] loss=2.47 avg=2.40\n",
      "[999 | 3177.97] loss=2.57 avg=2.40\n",
      "Saving checkpoint/run1/model-1000\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      ":\n",
      "(1277527300) kane66: no prob, i've just done: sudo apt-get install bower\n",
      "(1277543400) dg: why is the bower? bower is a library for .deb installer\n",
      "(1277464340) kane66: i never used it... i used it for everything in the install directory, only one directory though.\n",
      "(1277464340) kane66: bower is designed with a .deb for installing\n",
      "(1277464340) dg: okay, good then, just open the installer ( cd in your folder) and try\n",
      "(1277464340) kane66: yeah i just installed 'deb' bower but it just doesn't work\n",
      "(1277464400) dg: I don't know for sure what might be wrong. You have to be very smart with what you do though. There has to be something going wrong.  I think the problem might be that you just install the .deb from bower. If it works, then it is working.\n",
      "(1277564400) kane66: i know, i will try that as well.\n",
      "(1277564460) kane66: if i uninstall, then bower does not exist as .deb\n",
      "(1277564520) dg: you are working on how to change the bower so then bower.deb works?\n",
      "(1277564520) kane66: i tried that now. i'm running something else now.\n",
      "(1277564420) kane66: it is just .deb installer but I never tried it for all files in the install folder.\n",
      "(1277564420) dg: you are using dpkg, which might try?\n",
      "(1277564420) kane66: I ran dpkg -r bower.deb from a terminal when i had to update it, and it took exactly one minute and half\n",
      "(1277564420) kane66: It is the easiest way to know.\n",
      "(1277564420) kane66: I don't find the 'd' stuff and it worked fine.\n",
      "(1277564420) kane66: it had nothing to do with my installation. I am sure it would be easier if the bower.deb were just something to install and bower.deb just a directory in the install folder. But it's not, the same exact files just never change from the install directory.\n",
      "(1277564420) kane66: I found this:  http://pastebin.com/bdd7f577a , not sure how to change it. But it works: http://pastebin.com/bdd7f5779\n",
      "(1277524540) kane66: I'm looking for a better way to go about it. I think the best option would be to just not bother with bower. It's a lot of work, and it is not easy. I think it would be easier if it was only one file, or a certain file in the directory. That way, you could just update bower. Now that I need some file-specific options for some file-thing, and i guess i'll try all other options.\n",
      "(1277524540) kane66: dum, my next thing to try is to try the latest version of unity-terminator. Unfortunately, the current one doesn't have a .deb installer, and so i'm stuck with just one file\n",
      "(1277564660) kane66: i hope bower works out. i'm glad i'm able to help...\n",
      "(1277564660) kane66: maybe unity is easier, though I'm pretty new with both compiz and xcrunner.\n",
      "(1277564660) kane66: it's easy\n",
      "(1277564660) kane66: bower doesn't help too often, and maybe unity is easier as its a less complex package\n",
      "(1277564660) kane66: or maybe unity is easy, though i'm pretty new with compiz and xcrunner. I find unity a much easier way to add stuff without needing complicated packages.\n",
      "(1277564660) kane66: maybe unity is easier, though I'm pretty new with both compiz and xcrunner. I find unity a much easier way to add stuff without needing complicated packages.\n",
      "(1277564660) dg: bower does work. but Unity won't.  So, it's much easier to use unity.\n",
      "(1277564660) _jason: you're a novice?  Or do you even know how to use the commandline\n",
      "(1277564660) _jason: if that's your problem. then ubuntus command line\n",
      "(1277564420) _jason: I found some\n",
      "\n",
      "[1000 | 3226.17] loss=2.33 avg=2.40\n",
      "[1001 | 3229.22] loss=2.26 avg=2.40\n",
      "[1002 | 3232.25] loss=2.72 avg=2.40\n",
      "[1003 | 3235.30] loss=2.87 avg=2.40\n",
      "[1004 | 3238.35] loss=2.50 avg=2.40\n",
      "[1005 | 3241.39] loss=2.69 avg=2.41\n",
      "[1006 | 3244.44] loss=2.43 avg=2.41\n",
      "[1007 | 3247.50] loss=2.15 avg=2.41\n",
      "[1008 | 3250.53] loss=2.54 avg=2.41\n",
      "[1009 | 3253.57] loss=2.12 avg=2.40\n",
      "[1010 | 3256.61] loss=1.97 avg=2.40\n",
      "[1011 | 3259.65] loss=2.45 avg=2.40\n",
      "[1012 | 3262.68] loss=2.43 avg=2.40\n",
      "[1013 | 3265.71] loss=2.56 avg=2.40\n",
      "[1014 | 3268.74] loss=2.65 avg=2.40\n",
      "[1015 | 3271.78] loss=2.83 avg=2.41\n",
      "[1016 | 3274.84] loss=2.71 avg=2.41\n",
      "[1017 | 3277.93] loss=2.30 avg=2.41\n",
      "[1018 | 3281.03] loss=2.86 avg=2.42\n",
      "[1019 | 3284.12] loss=2.40 avg=2.41\n",
      "[1020 | 3287.21] loss=2.50 avg=2.42\n",
      "[1021 | 3290.29] loss=2.25 avg=2.41\n",
      "[1022 | 3293.36] loss=2.47 avg=2.41\n",
      "[1023 | 3296.45] loss=2.82 avg=2.42\n",
      "[1024 | 3299.52] loss=2.32 avg=2.42\n",
      "[1025 | 3302.61] loss=2.11 avg=2.41\n",
      "[1026 | 3305.70] loss=2.39 avg=2.41\n",
      "[1027 | 3308.80] loss=2.26 avg=2.41\n",
      "[1028 | 3311.89] loss=2.38 avg=2.41\n",
      "[1029 | 3314.99] loss=2.14 avg=2.41\n",
      "[1030 | 3318.10] loss=2.36 avg=2.41\n",
      "[1031 | 3321.20] loss=2.34 avg=2.41\n",
      "[1032 | 3324.32] loss=2.41 avg=2.41\n",
      "[1033 | 3327.42] loss=2.42 avg=2.41\n",
      "[1034 | 3330.50] loss=2.57 avg=2.41\n",
      "[1035 | 3333.55] loss=1.93 avg=2.41\n",
      "[1036 | 3336.61] loss=2.76 avg=2.41\n",
      "[1037 | 3339.66] loss=2.81 avg=2.41\n",
      "[1038 | 3342.73] loss=2.54 avg=2.41\n",
      "[1039 | 3345.80] loss=3.05 avg=2.42\n",
      "[1040 | 3348.84] loss=2.14 avg=2.42\n",
      "[1041 | 3351.91] loss=2.25 avg=2.42\n",
      "[1042 | 3354.97] loss=2.32 avg=2.42\n",
      "[1043 | 3358.07] loss=2.52 avg=2.42\n",
      "[1044 | 3361.17] loss=2.18 avg=2.41\n",
      "[1045 | 3364.26] loss=2.22 avg=2.41\n",
      "[1046 | 3367.35] loss=2.21 avg=2.41\n",
      "[1047 | 3370.45] loss=2.25 avg=2.41\n",
      "[1048 | 3373.54] loss=2.55 avg=2.41\n",
      "[1049 | 3376.63] loss=2.27 avg=2.41\n",
      "[1050 | 3379.71] loss=2.26 avg=2.41\n",
      "[1051 | 3382.78] loss=2.28 avg=2.41\n",
      "[1052 | 3385.86] loss=1.93 avg=2.40\n",
      "[1053 | 3388.94] loss=2.51 avg=2.40\n",
      "[1054 | 3392.04] loss=2.29 avg=2.40\n",
      "[1055 | 3395.14] loss=2.72 avg=2.40\n",
      "[1056 | 3398.23] loss=2.70 avg=2.41\n",
      "[1057 | 3401.32] loss=2.41 avg=2.41\n",
      "[1058 | 3404.41] loss=2.64 avg=2.41\n",
      "[1059 | 3407.49] loss=2.29 avg=2.41\n",
      "[1060 | 3410.59] loss=2.13 avg=2.41\n",
      "[1061 | 3413.63] loss=2.30 avg=2.40\n",
      "[1062 | 3416.66] loss=2.33 avg=2.40\n",
      "[1063 | 3419.69] loss=1.26 avg=2.39\n",
      "[1064 | 3422.72] loss=2.27 avg=2.39\n",
      "[1065 | 3425.75] loss=2.54 avg=2.39\n",
      "[1066 | 3428.77] loss=2.60 avg=2.39\n",
      "[1067 | 3431.80] loss=2.02 avg=2.39\n",
      "[1068 | 3434.84] loss=2.30 avg=2.39\n",
      "[1069 | 3437.87] loss=2.72 avg=2.39\n",
      "[1070 | 3440.90] loss=1.97 avg=2.39\n",
      "[1071 | 3443.93] loss=2.64 avg=2.39\n",
      "[1072 | 3446.95] loss=2.23 avg=2.39\n",
      "[1073 | 3449.97] loss=2.85 avg=2.39\n",
      "[1074 | 3452.98] loss=2.12 avg=2.39\n",
      "[1075 | 3455.99] loss=2.49 avg=2.39\n",
      "[1076 | 3459.02] loss=2.27 avg=2.39\n",
      "[1077 | 3462.04] loss=2.65 avg=2.39\n",
      "[1078 | 3465.07] loss=2.06 avg=2.39\n",
      "[1079 | 3468.10] loss=2.32 avg=2.39\n",
      "[1080 | 3471.13] loss=2.01 avg=2.39\n",
      "[1081 | 3474.17] loss=2.17 avg=2.38\n",
      "[1082 | 3477.20] loss=1.96 avg=2.38\n",
      "[1083 | 3480.23] loss=2.51 avg=2.38\n",
      "[1084 | 3483.25] loss=2.28 avg=2.38\n",
      "[1085 | 3486.28] loss=2.26 avg=2.38\n",
      "[1086 | 3489.32] loss=2.13 avg=2.38\n",
      "[1087 | 3492.36] loss=2.16 avg=2.37\n",
      "[1088 | 3495.39] loss=2.16 avg=2.37\n",
      "[1089 | 3498.41] loss=2.25 avg=2.37\n",
      "[1090 | 3501.44] loss=2.45 avg=2.37\n",
      "[1091 | 3504.48] loss=2.87 avg=2.38\n",
      "[1092 | 3507.52] loss=2.52 avg=2.38\n",
      "[1093 | 3510.55] loss=2.55 avg=2.38\n",
      "[1094 | 3513.57] loss=2.38 avg=2.38\n",
      "[1095 | 3516.60] loss=2.42 avg=2.38\n",
      "[1096 | 3519.63] loss=2.80 avg=2.38\n",
      "[1097 | 3522.66] loss=2.47 avg=2.39\n",
      "[1098 | 3525.71] loss=2.25 avg=2.38\n",
      "[1099 | 3528.74] loss=2.30 avg=2.38\n",
      "[1100 | 3531.77] loss=2.76 avg=2.39\n",
      "[1101 | 3534.78] loss=2.50 avg=2.39\n",
      "[1102 | 3537.81] loss=2.36 avg=2.39\n",
      "[1103 | 3540.85] loss=2.41 avg=2.39\n",
      "[1104 | 3543.89] loss=2.33 avg=2.39\n",
      "[1105 | 3546.97] loss=2.06 avg=2.38\n",
      "[1106 | 3550.06] loss=3.05 avg=2.39\n",
      "[1107 | 3553.15] loss=2.31 avg=2.39\n",
      "[1108 | 3556.23] loss=2.67 avg=2.39\n",
      "[1109 | 3559.32] loss=2.58 avg=2.39\n",
      "[1110 | 3562.39] loss=2.24 avg=2.39\n",
      "[1111 | 3565.46] loss=2.36 avg=2.39\n",
      "[1112 | 3568.54] loss=2.17 avg=2.39\n",
      "[1113 | 3571.60] loss=2.16 avg=2.39\n",
      "[1114 | 3574.67] loss=1.71 avg=2.38\n",
      "[1115 | 3577.74] loss=2.43 avg=2.38\n",
      "[1116 | 3580.81] loss=2.19 avg=2.38\n",
      "[1117 | 3583.90] loss=2.08 avg=2.38\n",
      "[1118 | 3587.00] loss=2.79 avg=2.38\n",
      "[1119 | 3590.10] loss=2.27 avg=2.38\n",
      "[1120 | 3593.19] loss=2.67 avg=2.38\n",
      "[1121 | 3596.29] loss=2.19 avg=2.38\n",
      "[1122 | 3599.39] loss=2.26 avg=2.38\n",
      "[1123 | 3602.46] loss=2.22 avg=2.38\n",
      "[1124 | 3605.55] loss=2.26 avg=2.38\n",
      "[1125 | 3608.60] loss=2.46 avg=2.38\n",
      "[1126 | 3611.63] loss=2.12 avg=2.38\n",
      "[1127 | 3614.64] loss=2.72 avg=2.38\n",
      "[1128 | 3617.66] loss=2.43 avg=2.38\n",
      "[1129 | 3620.69] loss=2.40 avg=2.38\n",
      "[1130 | 3623.73] loss=2.51 avg=2.38\n",
      "[1131 | 3626.75] loss=2.77 avg=2.38\n",
      "[1132 | 3629.79] loss=2.18 avg=2.38\n",
      "[1133 | 3632.82] loss=2.19 avg=2.38\n",
      "[1134 | 3635.86] loss=2.54 avg=2.38\n",
      "[1135 | 3638.88] loss=2.56 avg=2.38\n",
      "[1136 | 3641.92] loss=2.26 avg=2.38\n",
      "[1137 | 3644.94] loss=2.86 avg=2.39\n",
      "[1138 | 3647.97] loss=2.25 avg=2.39\n",
      "[1139 | 3651.00] loss=2.15 avg=2.38\n",
      "[1140 | 3654.03] loss=2.62 avg=2.39\n",
      "[1141 | 3657.05] loss=2.26 avg=2.38\n",
      "[1142 | 3660.07] loss=2.60 avg=2.39\n",
      "[1143 | 3663.09] loss=2.25 avg=2.39\n",
      "[1144 | 3666.12] loss=2.43 avg=2.39\n",
      "[1145 | 3669.15] loss=2.37 avg=2.39\n",
      "[1146 | 3672.17] loss=2.92 avg=2.39\n",
      "[1147 | 3675.19] loss=2.19 avg=2.39\n",
      "[1148 | 3678.21] loss=2.38 avg=2.39\n",
      "[1149 | 3681.25] loss=3.05 avg=2.40\n",
      "[1150 | 3684.27] loss=2.30 avg=2.39\n",
      "[1151 | 3687.30] loss=2.28 avg=2.39\n",
      "[1152 | 3690.32] loss=2.53 avg=2.40\n",
      "[1153 | 3693.35] loss=2.10 avg=2.39\n",
      "[1154 | 3696.38] loss=2.26 avg=2.39\n",
      "[1155 | 3699.42] loss=1.69 avg=2.38\n",
      "[1156 | 3702.45] loss=2.80 avg=2.39\n",
      "[1157 | 3705.47] loss=2.35 avg=2.39\n",
      "[1158 | 3708.49] loss=2.24 avg=2.39\n",
      "[1159 | 3711.52] loss=2.22 avg=2.38\n",
      "[1160 | 3714.55] loss=2.67 avg=2.39\n",
      "[1161 | 3717.57] loss=2.05 avg=2.38\n",
      "[1162 | 3720.60] loss=1.81 avg=2.38\n",
      "[1163 | 3723.63] loss=2.49 avg=2.38\n",
      "[1164 | 3726.66] loss=2.28 avg=2.38\n",
      "[1165 | 3729.69] loss=2.61 avg=2.38\n",
      "[1166 | 3732.71] loss=2.85 avg=2.39\n",
      "[1167 | 3735.75] loss=2.16 avg=2.38\n",
      "[1168 | 3738.77] loss=2.40 avg=2.38\n",
      "[1169 | 3741.80] loss=2.01 avg=2.38\n",
      "[1170 | 3744.82] loss=2.73 avg=2.38\n",
      "[1171 | 3747.86] loss=2.55 avg=2.38\n",
      "[1172 | 3750.93] loss=2.13 avg=2.38\n",
      "[1173 | 3754.01] loss=3.05 avg=2.39\n",
      "[1174 | 3757.10] loss=2.20 avg=2.39\n",
      "[1175 | 3760.19] loss=2.50 avg=2.39\n",
      "[1176 | 3763.30] loss=2.08 avg=2.39\n",
      "[1177 | 3766.40] loss=2.30 avg=2.38\n",
      "[1178 | 3769.47] loss=2.09 avg=2.38\n",
      "[1179 | 3772.57] loss=2.49 avg=2.38\n",
      "[1180 | 3775.65] loss=2.57 avg=2.38\n",
      "[1181 | 3778.76] loss=2.19 avg=2.38\n",
      "[1182 | 3781.84] loss=2.31 avg=2.38\n",
      "[1183 | 3784.93] loss=2.83 avg=2.39\n",
      "[1184 | 3788.05] loss=2.45 avg=2.39\n",
      "[1185 | 3791.10] loss=2.39 avg=2.39\n",
      "[1186 | 3794.16] loss=2.24 avg=2.39\n",
      "[1187 | 3797.21] loss=2.26 avg=2.38\n",
      "[1188 | 3800.27] loss=2.62 avg=2.39\n",
      "[1189 | 3803.33] loss=2.53 avg=2.39\n",
      "[1190 | 3806.39] loss=2.78 avg=2.39\n",
      "[1191 | 3809.47] loss=2.49 avg=2.39\n",
      "[1192 | 3812.57] loss=2.55 avg=2.39\n",
      "[1193 | 3815.66] loss=2.35 avg=2.39\n",
      "[1194 | 3818.75] loss=2.56 avg=2.40\n",
      "[1195 | 3821.85] loss=2.39 avg=2.40\n",
      "[1196 | 3824.95] loss=2.45 avg=2.40\n",
      "[1197 | 3828.02] loss=2.67 avg=2.40\n",
      "[1198 | 3831.11] loss=2.40 avg=2.40\n",
      "[1199 | 3834.21] loss=2.34 avg=2.40\n",
      "[1200 | 3837.30] loss=2.27 avg=2.40\n",
      "[1201 | 3840.38] loss=2.69 avg=2.40\n",
      "[1202 | 3843.47] loss=1.96 avg=2.40\n",
      "[1203 | 3846.54] loss=2.20 avg=2.39\n",
      "[1204 | 3849.57] loss=2.19 avg=2.39\n",
      "[1205 | 3852.59] loss=2.50 avg=2.39\n",
      "[1206 | 3855.63] loss=2.48 avg=2.39\n",
      "[1207 | 3858.66] loss=2.23 avg=2.39\n",
      "[1208 | 3861.69] loss=2.51 avg=2.39\n",
      "[1209 | 3864.72] loss=2.09 avg=2.39\n",
      "[1210 | 3867.74] loss=2.95 avg=2.40\n",
      "[1211 | 3870.76] loss=0.89 avg=2.38\n",
      "[1212 | 3873.79] loss=2.07 avg=2.38\n",
      "[1213 | 3876.83] loss=2.78 avg=2.38\n",
      "[1214 | 3879.84] loss=2.28 avg=2.38\n",
      "[1215 | 3882.88] loss=2.49 avg=2.38\n",
      "[1216 | 3885.91] loss=2.40 avg=2.38\n",
      "[1217 | 3888.93] loss=2.27 avg=2.38\n",
      "[1218 | 3891.95] loss=2.13 avg=2.38\n",
      "[1219 | 3894.97] loss=2.41 avg=2.38\n",
      "[1220 | 3897.99] loss=2.25 avg=2.38\n",
      "[1221 | 3901.02] loss=2.74 avg=2.38\n",
      "[1222 | 3904.05] loss=2.78 avg=2.38\n",
      "[1223 | 3907.08] loss=2.41 avg=2.38\n",
      "[1224 | 3910.12] loss=2.42 avg=2.39\n",
      "[1225 | 3913.15] loss=2.17 avg=2.38\n",
      "[1226 | 3916.17] loss=2.61 avg=2.39\n",
      "[1227 | 3919.20] loss=2.32 avg=2.38\n",
      "[1228 | 3922.24] loss=2.14 avg=2.38\n",
      "[1229 | 3925.27] loss=2.57 avg=2.38\n",
      "[1230 | 3928.30] loss=2.65 avg=2.39\n",
      "[1231 | 3931.33] loss=2.35 avg=2.39\n",
      "[1232 | 3934.35] loss=2.15 avg=2.38\n",
      "[1233 | 3937.37] loss=2.24 avg=2.38\n",
      "[1234 | 3940.40] loss=2.51 avg=2.38\n",
      "[1235 | 3943.43] loss=2.02 avg=2.38\n",
      "[1236 | 3946.46] loss=2.48 avg=2.38\n",
      "[1237 | 3949.50] loss=2.34 avg=2.38\n",
      "[1238 | 3952.53] loss=2.79 avg=2.39\n",
      "[1239 | 3955.56] loss=2.24 avg=2.38\n",
      "[1240 | 3958.59] loss=2.26 avg=2.38\n",
      "[1241 | 3961.62] loss=2.39 avg=2.38\n",
      "[1242 | 3964.65] loss=2.35 avg=2.38\n",
      "[1243 | 3967.67] loss=2.33 avg=2.38\n",
      "[1244 | 3970.70] loss=1.90 avg=2.38\n",
      "[1245 | 3973.74] loss=2.45 avg=2.38\n",
      "[1246 | 3976.76] loss=2.41 avg=2.38\n",
      "[1247 | 3979.79] loss=2.13 avg=2.38\n",
      "[1248 | 3982.84] loss=1.98 avg=2.37\n",
      "[1249 | 3985.92] loss=2.45 avg=2.37\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "fec_: you have to have installed the live CD too\n",
      "(1272613800) han_: no prob...\n",
      "(1272613800) fccf:   no, but yes.  you can get the full install, then try the live CD and then install from there\n",
      "(1272613800) fec_: i know i did...\n",
      "(1272613800) fccf: what if you just try the live CD with no settings?\n",
      "(1272613860) fec_:  is linux ready?\n",
      "(1272613860) fccf:  no, but I was talking about apt-get to install the packages first.  apt-get might as well have downloaded them if you already have them, or aptitude might have added them in.\n",
      "(1272613920) fccf:  do you have apt-get?\n",
      "(1272613980) han_: i have\n",
      "(1272613980) fccf:  and where are you on apt-get?\n",
      "(1272613980) han_: nah...\n",
      "(1272613980) fccf:  you need only to run apt-get to add the packages for you to use.  you need to have the cd with the Ubuntu CD.\n",
      "(1272613980) han_: i have and i don't want to install ubuntu again :S  :(\n",
      "(1272613980) fccf:  it may be easier to just add the packages in the CD, then it will work\n",
      "(1272613980) fccf:  just run: sudo apt-get add-apt-repository\n",
      "(1272613980) han_: i am not even in ubuntu :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :( :) :P\n",
      "(1272613980) fccf:  I was referring to apt-get.  you don't need to run apt-get first.\n",
      "(1272613980) fccf:  sudo apt-get install ubuntu-desktop\n",
      "(1272613980) han_: thanks\n",
      "\n",
      "\n",
      "(12120140) m3w_: how do i install the required programs ? (ubuntu, mibbit, ubuntu)\n",
      "(1212010140) pahadonx: what are you looking for?\n",
      "(1212010140) pahadonx: then type that in your terminal, and you should be able to install ubuntu\n",
      "(1212010140) pahadonx: and it's that easy?\n",
      "(1212010200) pahadonx: so you're trying to find some application.\n",
      "(1212010200) tao_: aeek, and apt-get is just apt-get\n",
      "(1212010400) tao_: apt-get is just in a ftp, which can't use ftp at all ;)\n",
      "(1212010400) tao_: apt-get is an apt-get clone\n",
      "\n",
      "\n",
      "(1218194080) ikonia: hey, if you are really stuck on gnome 2:X, and you don't know how to get back to ubuntu, how can you reinstall gnome 2.6 for windows ?\n",
      "(1218194100) ikonia: can I get back to Ubuntu in gnome 2:X with Win XP with no problems?\n",
      "(1218194100) m0resto: gnome-look-2.6-alternative-installer doesn't work\n",
      "(1218194220) m0resto: just uncheck the Ubuntu version if you have Windows installed\n",
      "(1218194280) ikonia: is that an option from System?\n",
      "(1218194280) m0resto: don't know about reinstall, but you should try it first\n",
      "(1218194280) m0resto: there is no support for reinstall on windows\n",
      "(1218194280) m0resto: so you should reinstall with the windows version\n",
      "(1218194340) m0resto: so you should reinstall gnome-look-linux\n",
      "(1218194400) ikonia: okay and in that case?\n",
      "(1218194400) ikonia: I'll try it, maybe I'll get to the gnome 2:X thing...I guess I can keep doing what I want.\n",
      "(1218194400) m0resto\n",
      "\n",
      "[1250 | 4026.23] loss=2.21 avg=2.37\n",
      "[1251 | 4029.34] loss=2.30 avg=2.37\n",
      "[1252 | 4032.43] loss=2.29 avg=2.37\n",
      "[1253 | 4035.55] loss=2.48 avg=2.37\n",
      "[1254 | 4038.64] loss=2.60 avg=2.37\n",
      "[1255 | 4041.72] loss=2.49 avg=2.37\n",
      "[1256 | 4044.75] loss=2.28 avg=2.37\n",
      "[1257 | 4047.78] loss=2.58 avg=2.37\n",
      "[1258 | 4050.81] loss=2.38 avg=2.37\n",
      "[1259 | 4053.83] loss=2.37 avg=2.37\n",
      "[1260 | 4056.86] loss=2.37 avg=2.37\n",
      "[1261 | 4059.90] loss=3.07 avg=2.38\n",
      "[1262 | 4062.95] loss=2.25 avg=2.38\n",
      "[1263 | 4065.97] loss=2.77 avg=2.38\n",
      "[1264 | 4068.98] loss=1.94 avg=2.38\n",
      "[1265 | 4072.02] loss=3.01 avg=2.39\n",
      "[1266 | 4075.06] loss=2.77 avg=2.39\n",
      "[1267 | 4078.08] loss=2.41 avg=2.39\n",
      "[1268 | 4081.12] loss=2.40 avg=2.39\n",
      "[1269 | 4084.15] loss=2.09 avg=2.39\n",
      "[1270 | 4087.18] loss=2.33 avg=2.39\n",
      "[1271 | 4090.21] loss=2.36 avg=2.39\n",
      "[1272 | 4093.25] loss=2.55 avg=2.39\n",
      "[1273 | 4096.28] loss=2.15 avg=2.39\n",
      "[1274 | 4099.32] loss=2.12 avg=2.38\n",
      "[1275 | 4102.33] loss=2.83 avg=2.39\n",
      "[1276 | 4105.36] loss=2.53 avg=2.39\n",
      "[1277 | 4108.40] loss=2.40 avg=2.39\n",
      "[1278 | 4111.43] loss=3.05 avg=2.40\n",
      "[1279 | 4114.48] loss=2.91 avg=2.40\n",
      "[1280 | 4117.51] loss=2.34 avg=2.40\n",
      "[1281 | 4120.54] loss=1.77 avg=2.39\n",
      "[1282 | 4123.58] loss=2.76 avg=2.40\n",
      "[1283 | 4126.61] loss=2.46 avg=2.40\n",
      "[1284 | 4129.65] loss=2.56 avg=2.40\n",
      "[1285 | 4132.68] loss=2.43 avg=2.40\n",
      "[1286 | 4135.71] loss=2.30 avg=2.40\n",
      "[1287 | 4138.75] loss=2.42 avg=2.40\n",
      "[1288 | 4141.78] loss=2.09 avg=2.40\n",
      "[1289 | 4144.82] loss=2.03 avg=2.39\n",
      "[1290 | 4147.85] loss=2.23 avg=2.39\n",
      "[1291 | 4150.90] loss=2.38 avg=2.39\n",
      "[1292 | 4153.93] loss=2.26 avg=2.39\n",
      "[1293 | 4156.97] loss=1.78 avg=2.38\n",
      "[1294 | 4160.01] loss=2.25 avg=2.38\n",
      "[1295 | 4163.04] loss=2.20 avg=2.38\n",
      "[1296 | 4166.07] loss=2.50 avg=2.38\n",
      "[1297 | 4169.10] loss=2.33 avg=2.38\n",
      "[1298 | 4172.13] loss=1.69 avg=2.37\n",
      "[1299 | 4175.16] loss=2.58 avg=2.38\n",
      "[1300 | 4178.22] loss=2.21 avg=2.37\n",
      "[1301 | 4181.31] loss=2.02 avg=2.37\n",
      "[1302 | 4184.40] loss=2.26 avg=2.37\n",
      "[1303 | 4187.48] loss=2.83 avg=2.37\n",
      "[1304 | 4190.56] loss=2.09 avg=2.37\n",
      "[1305 | 4193.62] loss=2.56 avg=2.37\n",
      "[1306 | 4196.72] loss=1.59 avg=2.37\n",
      "[1307 | 4199.80] loss=2.41 avg=2.37\n",
      "[1308 | 4202.88] loss=3.05 avg=2.37\n",
      "[1309 | 4205.94] loss=1.80 avg=2.37\n",
      "[1310 | 4209.00] loss=2.13 avg=2.36\n",
      "[1311 | 4212.07] loss=2.93 avg=2.37\n",
      "[1312 | 4215.13] loss=2.66 avg=2.37\n",
      "[1313 | 4218.22] loss=3.14 avg=2.38\n",
      "[1314 | 4221.32] loss=1.67 avg=2.37\n",
      "[1315 | 4224.40] loss=2.70 avg=2.38\n",
      "[1316 | 4227.50] loss=2.36 avg=2.38\n",
      "[1317 | 4230.59] loss=2.09 avg=2.37\n",
      "[1318 | 4233.69] loss=1.94 avg=2.37\n",
      "[1319 | 4236.80] loss=2.57 avg=2.37\n",
      "[1320 | 4239.88] loss=2.56 avg=2.37\n",
      "[1321 | 4242.96] loss=2.25 avg=2.37\n",
      "[1322 | 4246.01] loss=3.22 avg=2.38\n",
      "[1323 | 4249.03] loss=1.95 avg=2.38\n",
      "[1324 | 4252.05] loss=2.19 avg=2.37\n",
      "[1325 | 4255.08] loss=2.10 avg=2.37\n",
      "[1326 | 4258.11] loss=1.72 avg=2.37\n",
      "[1327 | 4261.14] loss=2.28 avg=2.36\n",
      "[1328 | 4264.16] loss=2.46 avg=2.37\n",
      "[1329 | 4267.20] loss=2.47 avg=2.37\n",
      "[1330 | 4270.23] loss=2.49 avg=2.37\n",
      "[1331 | 4273.25] loss=2.28 avg=2.37\n",
      "[1332 | 4276.28] loss=2.10 avg=2.36\n",
      "[1333 | 4279.31] loss=2.47 avg=2.37\n",
      "[1334 | 4282.34] loss=2.87 avg=2.37\n",
      "[1335 | 4285.37] loss=2.35 avg=2.37\n",
      "[1336 | 4288.40] loss=2.27 avg=2.37\n",
      "[1337 | 4291.44] loss=2.40 avg=2.37\n",
      "[1338 | 4294.47] loss=2.10 avg=2.37\n",
      "[1339 | 4297.50] loss=2.52 avg=2.37\n",
      "[1340 | 4300.53] loss=2.51 avg=2.37\n",
      "[1341 | 4303.58] loss=1.85 avg=2.36\n",
      "[1342 | 4306.60] loss=2.90 avg=2.37\n",
      "[1343 | 4309.63] loss=2.22 avg=2.37\n",
      "[1344 | 4312.66] loss=2.17 avg=2.37\n",
      "[1345 | 4315.68] loss=2.00 avg=2.36\n",
      "[1346 | 4318.72] loss=2.28 avg=2.36\n",
      "[1347 | 4321.77] loss=2.50 avg=2.36\n",
      "[1348 | 4324.80] loss=3.07 avg=2.37\n",
      "[1349 | 4327.83] loss=2.10 avg=2.37\n",
      "[1350 | 4330.86] loss=2.63 avg=2.37\n",
      "[1351 | 4333.89] loss=2.21 avg=2.37\n",
      "[1352 | 4336.91] loss=2.37 avg=2.37\n",
      "[1353 | 4339.95] loss=2.73 avg=2.37\n",
      "[1354 | 4342.98] loss=2.46 avg=2.37\n",
      "[1355 | 4346.02] loss=2.21 avg=2.37\n",
      "[1356 | 4349.05] loss=2.69 avg=2.37\n",
      "[1357 | 4352.08] loss=2.35 avg=2.37\n",
      "[1358 | 4355.13] loss=2.55 avg=2.38\n",
      "[1359 | 4358.16] loss=2.08 avg=2.37\n",
      "[1360 | 4361.19] loss=2.43 avg=2.37\n",
      "[1361 | 4364.23] loss=1.96 avg=2.37\n",
      "[1362 | 4367.26] loss=1.93 avg=2.37\n",
      "[1363 | 4370.29] loss=1.91 avg=2.36\n",
      "[1364 | 4373.33] loss=2.32 avg=2.36\n",
      "[1365 | 4376.35] loss=2.17 avg=2.36\n",
      "[1366 | 4379.38] loss=2.33 avg=2.36\n",
      "[1367 | 4382.41] loss=2.15 avg=2.36\n",
      "[1368 | 4385.45] loss=2.43 avg=2.36\n",
      "[1369 | 4388.50] loss=2.45 avg=2.36\n",
      "[1370 | 4391.58] loss=2.41 avg=2.36\n",
      "[1371 | 4394.65] loss=2.60 avg=2.36\n",
      "[1372 | 4397.74] loss=2.44 avg=2.36\n",
      "[1373 | 4400.84] loss=2.45 avg=2.36\n",
      "[1374 | 4403.92] loss=2.78 avg=2.37\n",
      "[1375 | 4406.99] loss=1.44 avg=2.36\n",
      "[1376 | 4410.10] loss=2.56 avg=2.36\n",
      "[1377 | 4413.19] loss=2.31 avg=2.36\n",
      "[1378 | 4416.28] loss=2.11 avg=2.36\n",
      "[1379 | 4419.37] loss=1.97 avg=2.35\n",
      "[1380 | 4422.46] loss=1.98 avg=2.35\n",
      "[1381 | 4425.55] loss=2.07 avg=2.35\n",
      "[1382 | 4428.63] loss=2.77 avg=2.35\n",
      "[1383 | 4431.74] loss=1.91 avg=2.35\n",
      "[1384 | 4434.84] loss=2.83 avg=2.35\n",
      "[1385 | 4437.94] loss=2.57 avg=2.35\n",
      "[1386 | 4440.99] loss=2.12 avg=2.35\n",
      "[1387 | 4444.06] loss=2.54 avg=2.35\n",
      "[1388 | 4447.12] loss=2.68 avg=2.36\n",
      "[1389 | 4450.18] loss=2.33 avg=2.35\n",
      "[1390 | 4453.24] loss=2.24 avg=2.35\n",
      "[1391 | 4456.30] loss=2.28 avg=2.35\n",
      "[1392 | 4459.38] loss=2.32 avg=2.35\n",
      "[1393 | 4462.47] loss=2.27 avg=2.35\n",
      "[1394 | 4465.57] loss=2.50 avg=2.35\n",
      "[1395 | 4468.66] loss=2.30 avg=2.35\n",
      "[1396 | 4471.74] loss=2.22 avg=2.35\n",
      "[1397 | 4474.84] loss=2.14 avg=2.35\n",
      "[1398 | 4477.95] loss=2.81 avg=2.35\n",
      "[1399 | 4481.06] loss=2.01 avg=2.35\n",
      "[1400 | 4484.14] loss=1.94 avg=2.35\n",
      "[1401 | 4487.21] loss=2.32 avg=2.35\n",
      "[1402 | 4490.32] loss=2.37 avg=2.35\n",
      "[1403 | 4493.43] loss=2.00 avg=2.34\n",
      "[1404 | 4496.59] loss=2.45 avg=2.34\n",
      "[1405 | 4499.68] loss=2.06 avg=2.34\n",
      "[1406 | 4502.76] loss=2.76 avg=2.35\n",
      "[1407 | 4505.86] loss=2.21 avg=2.34\n",
      "[1408 | 4508.94] loss=2.04 avg=2.34\n",
      "[1409 | 4512.03] loss=2.31 avg=2.34\n",
      "[1410 | 4515.07] loss=2.32 avg=2.34\n",
      "[1411 | 4518.10] loss=2.45 avg=2.34\n",
      "[1412 | 4521.12] loss=2.52 avg=2.34\n",
      "[1413 | 4524.17] loss=2.59 avg=2.35\n",
      "[1414 | 4527.19] loss=2.50 avg=2.35\n",
      "[1415 | 4530.24] loss=2.45 avg=2.35\n",
      "[1416 | 4533.26] loss=1.89 avg=2.34\n",
      "[1417 | 4536.27] loss=2.49 avg=2.35\n",
      "[1418 | 4539.30] loss=2.10 avg=2.34\n",
      "[1419 | 4542.32] loss=2.10 avg=2.34\n",
      "[1420 | 4545.36] loss=2.10 avg=2.34\n",
      "[1421 | 4548.40] loss=2.29 avg=2.34\n",
      "[1422 | 4551.44] loss=2.28 avg=2.34\n",
      "[1423 | 4554.47] loss=2.23 avg=2.34\n",
      "[1424 | 4557.49] loss=2.75 avg=2.34\n",
      "[1425 | 4560.53] loss=2.57 avg=2.34\n",
      "[1426 | 4563.57] loss=2.03 avg=2.34\n",
      "[1427 | 4566.61] loss=2.94 avg=2.35\n",
      "[1428 | 4569.65] loss=2.58 avg=2.35\n",
      "[1429 | 4572.69] loss=2.33 avg=2.35\n",
      "[1430 | 4575.73] loss=2.81 avg=2.35\n",
      "[1431 | 4578.77] loss=2.80 avg=2.36\n",
      "[1432 | 4581.80] loss=2.47 avg=2.36\n",
      "[1433 | 4584.83] loss=2.19 avg=2.36\n",
      "[1434 | 4587.88] loss=2.96 avg=2.36\n",
      "[1435 | 4590.92] loss=2.53 avg=2.36\n",
      "[1436 | 4593.95] loss=2.56 avg=2.37\n",
      "[1437 | 4597.01] loss=2.69 avg=2.37\n",
      "[1438 | 4600.05] loss=2.31 avg=2.37\n",
      "[1439 | 4603.10] loss=2.39 avg=2.37\n",
      "[1440 | 4606.14] loss=2.00 avg=2.36\n",
      "[1441 | 4609.17] loss=2.47 avg=2.37\n",
      "[1442 | 4612.21] loss=2.18 avg=2.36\n",
      "[1443 | 4615.27] loss=2.29 avg=2.36\n",
      "[1444 | 4618.32] loss=2.36 avg=2.36\n",
      "[1445 | 4621.35] loss=2.59 avg=2.37\n",
      "[1446 | 4624.41] loss=2.77 avg=2.37\n",
      "[1447 | 4627.45] loss=2.28 avg=2.37\n",
      "[1448 | 4630.50] loss=3.01 avg=2.37\n",
      "[1449 | 4633.54] loss=1.90 avg=2.37\n",
      "[1450 | 4636.58] loss=2.45 avg=2.37\n",
      "[1451 | 4639.63] loss=2.35 avg=2.37\n",
      "[1452 | 4642.68] loss=2.35 avg=2.37\n",
      "[1453 | 4645.72] loss=2.07 avg=2.37\n",
      "[1454 | 4648.76] loss=2.21 avg=2.37\n",
      "[1455 | 4651.80] loss=2.13 avg=2.36\n",
      "[1456 | 4654.83] loss=2.27 avg=2.36\n",
      "[1457 | 4657.87] loss=2.60 avg=2.36\n",
      "[1458 | 4660.92] loss=2.73 avg=2.37\n",
      "[1459 | 4663.96] loss=2.39 avg=2.37\n",
      "[1460 | 4667.01] loss=2.71 avg=2.37\n",
      "[1461 | 4670.07] loss=2.47 avg=2.37\n",
      "[1462 | 4673.17] loss=2.43 avg=2.37\n",
      "[1463 | 4676.28] loss=2.41 avg=2.37\n",
      "[1464 | 4679.39] loss=2.23 avg=2.37\n",
      "[1465 | 4682.48] loss=2.09 avg=2.37\n",
      "[1466 | 4685.59] loss=2.16 avg=2.37\n",
      "[1467 | 4688.65] loss=1.94 avg=2.36\n",
      "[1468 | 4691.74] loss=2.28 avg=2.36\n",
      "[1469 | 4694.83] loss=2.47 avg=2.36\n",
      "[1470 | 4697.92] loss=1.93 avg=2.36\n",
      "[1471 | 4701.03] loss=2.66 avg=2.36\n",
      "[1472 | 4704.10] loss=2.46 avg=2.36\n",
      "[1473 | 4707.19] loss=2.19 avg=2.36\n",
      "[1474 | 4710.27] loss=2.51 avg=2.36\n",
      "[1475 | 4713.38] loss=2.13 avg=2.36\n",
      "[1476 | 4716.49] loss=2.18 avg=2.36\n",
      "[1477 | 4719.58] loss=2.03 avg=2.36\n",
      "[1478 | 4722.70] loss=2.39 avg=2.36\n",
      "[1479 | 4725.78] loss=2.79 avg=2.36\n",
      "[1480 | 4728.84] loss=2.04 avg=2.36\n",
      "[1481 | 4731.91] loss=2.46 avg=2.36\n",
      "[1482 | 4734.96] loss=2.05 avg=2.36\n",
      "[1483 | 4738.03] loss=2.34 avg=2.35\n",
      "[1484 | 4741.08] loss=2.15 avg=2.35\n",
      "[1485 | 4744.14] loss=2.18 avg=2.35\n",
      "[1486 | 4747.21] loss=2.30 avg=2.35\n",
      "[1487 | 4750.28] loss=2.56 avg=2.35\n",
      "[1488 | 4753.37] loss=1.83 avg=2.35\n",
      "[1489 | 4756.46] loss=1.70 avg=2.34\n",
      "[1490 | 4759.57] loss=2.43 avg=2.34\n",
      "[1491 | 4762.66] loss=2.06 avg=2.34\n",
      "[1492 | 4765.76] loss=2.15 avg=2.34\n",
      "[1493 | 4768.87] loss=2.05 avg=2.33\n",
      "[1494 | 4771.96] loss=2.35 avg=2.33\n",
      "[1495 | 4775.06] loss=2.39 avg=2.33\n",
      "[1496 | 4778.16] loss=2.85 avg=2.34\n",
      "[1497 | 4781.24] loss=2.20 avg=2.34\n",
      "[1498 | 4784.33] loss=1.98 avg=2.34\n",
      "[1499 | 4787.44] loss=2.48 avg=2.34\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      " or bad.\n",
      "(1229072480) cbab: sorry, what was your problem?\n",
      "(1229072480) cbab: are you still here ??\n",
      "(1229072540) cbab: I'm not sure how all of this happened. I know that when I was at work one more time my USB adapter died and i had to restart and run xubuntu through the whole machine.\n",
      "(1229072540) cbab: and then ubuntu stopped it dead because it froze.\n",
      "(1229072540) cbab: i'm just thinking if you can even get it to auto up\n",
      "(1229072540) cbab: or if you can't do it at all\n",
      "(1229072540) cbab: is it possible to restore the drive?\n",
      "(1229072540) cbab: is it possible to recover what it did on\n",
      "(1229072540) cbab: how much does your drive have?\n",
      "(1229072540) cbab: try to find this guide at wubi\n",
      "(1229072540) cbab: is your fstab okay?\n",
      "(1229072600) cbab: look at the 'chroot' option\n",
      "(1229072600) cbab: that should give you the list of the directories\n",
      "(1229072720) cbab: there is a guide on that page\n",
      "(1229072720) cbab: you have your usb adapter\n",
      "(1229072720) cbab: what does ctrl f find?\n",
      "(1229072720) cbab: i think that's a hardware error\n",
      "(1229072780) cbab: does a chroot look like a normal mount?\n",
      "(1229072780) cbab: ok if your usb adapter has been using the same ntfs devices that are now in /etc/fstab should have been 'mounted'\n",
      "(1229072780) gsog: if you have the right /var/lib/fstab/options set there, and you're on /dev/sda1 /dev/sda1 should be 'sda1'\n",
      "(1229072780) gsog: it's normal to see that this option can be toggled off in mount.conf, see if you can see that 'mount -o loop /dev/sda1 /media/sda1'\n",
      "(1229072780) cbab: I'm in the middle of fixing this and it looks like it's not the mount option, because the ntfs directory is still pointing to /dev/sda1 instead of /media/sda1\n",
      "(1229072780) cbab: you'll have to look at ntfs.conf and see if the path was set correctly in any of the fstab options and see if it's ok\n",
      "(1230752800) cbab: your machine may be having a different ntfs setup, and you might need to adjust your fstab, try something like: ntfs.conf -nntfs -s /media/sda1             sda1                                   fstab                                           \n",
      "(1230662800) cbab: it isnt possible to set ntfs.conf without ntfsconf-help and ntfsconf-progutils for doing it\n",
      "(1230662800) cbab: try /path/to/ntfs.conf               ntfs.conf -s ntfs.conf\n",
      "(1230663820) cbab: then /path/of/ntfs.conf                                                                                                                          \n",
      "\n",
      "[1500 | 4827.68] loss=2.09 avg=2.33\n",
      "Saving checkpoint/run1/model-1501\n"
     ]
    }
   ],
   "source": [
    "#TRAINING OUR DATA AND GENERATING UNCONDITIONAL SAMPLES\n",
    "\n",
    "#With default learnig rate\n",
    "!PYTHONPATH=src ./train.py --dataset ubuntu-npz/ --sample_every=250 --stop_after=1501 --model_name 345M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "WCNHGNQFfcD8",
    "outputId": "8e1c1637-504b-40ae-bf2b-274e23a76647"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From ./train.py:73: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From ./train.py:76: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2019-10-11 01:54:06.138686: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "2019-10-11 01:54:06.139650: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x134b480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2019-10-11 01:54:06.139760: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2019-10-11 01:54:06.144903: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2019-10-11 01:54:06.231669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-10-11 01:54:06.232508: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6afb180 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2019-10-11 01:54:06.232554: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2019-10-11 01:54:06.232797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-10-11 01:54:06.233438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "2019-10-11 01:54:06.233877: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-10-11 01:54:06.235369: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-10-11 01:54:06.237985: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2019-10-11 01:54:06.238792: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2019-10-11 01:54:06.241274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2019-10-11 01:54:06.243578: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2019-10-11 01:54:06.251535: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-10-11 01:54:06.251712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-10-11 01:54:06.252556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-10-11 01:54:06.253263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2019-10-11 01:54:06.253387: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-10-11 01:54:06.254950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-10-11 01:54:06.254991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2019-10-11 01:54:06.255011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2019-10-11 01:54:06.255279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-10-11 01:54:06.256078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-10-11 01:54:06.256828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "WARNING:tensorflow:From ./train.py:77: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "WARNING:tensorflow:From ./train.py:100: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From ./train.py:113: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
      "Instructions for updating:\n",
      "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
      "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/memory_saving_gradients.py:89: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From ./train.py:120: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From ./train.py:122: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From ./train.py:125: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From ./train.py:129: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "Loading checkpoint checkpoint/run1/model-1501\n",
      "Loading dataset...\n",
      "100% 8/8 [00:08<00:00,  1.08s/it]\n",
      "dataset has 233929304 tokens\n",
      "Training...\n",
      "2019-10-11 01:55:19.668223: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "[1502 | 17.83] loss=2.45 avg=2.45\n",
      "[1503 | 20.89] loss=2.42 avg=2.43\n",
      "[1504 | 23.93] loss=2.49 avg=2.45\n",
      "[1505 | 26.99] loss=2.56 avg=2.48\n",
      "[1506 | 30.03] loss=2.31 avg=2.44\n",
      "[1507 | 33.07] loss=2.46 avg=2.45\n",
      "[1508 | 36.14] loss=2.17 avg=2.41\n",
      "[1509 | 39.20] loss=2.94 avg=2.48\n",
      "[1510 | 42.30] loss=2.10 avg=2.43\n",
      "[1511 | 45.43] loss=2.44 avg=2.43\n",
      "[1512 | 48.58] loss=2.68 avg=2.46\n",
      "[1513 | 51.74] loss=2.04 avg=2.42\n",
      "[1514 | 54.86] loss=2.22 avg=2.40\n",
      "[1515 | 58.03] loss=3.09 avg=2.46\n",
      "[1516 | 61.19] loss=2.50 avg=2.46\n",
      "[1517 | 64.36] loss=2.28 avg=2.45\n",
      "[1518 | 67.52] loss=2.18 avg=2.43\n",
      "[1519 | 70.68] loss=2.13 avg=2.41\n",
      "[1520 | 73.86] loss=2.84 avg=2.44\n",
      "[1521 | 77.02] loss=2.95 avg=2.47\n",
      "[1522 | 80.20] loss=2.20 avg=2.45\n",
      "[1523 | 83.31] loss=2.64 avg=2.46\n",
      "[1524 | 86.44] loss=2.65 avg=2.47\n",
      "[1525 | 89.57] loss=2.34 avg=2.46\n",
      "[1526 | 92.69] loss=2.52 avg=2.47\n",
      "[1527 | 95.81] loss=2.67 avg=2.47\n",
      "[1528 | 98.92] loss=2.29 avg=2.47\n",
      "[1529 | 102.04] loss=2.18 avg=2.46\n",
      "[1530 | 105.23] loss=2.32 avg=2.45\n",
      "[1531 | 108.38] loss=2.10 avg=2.44\n",
      "[1532 | 111.55] loss=2.48 avg=2.44\n",
      "[1533 | 114.75] loss=1.85 avg=2.42\n",
      "[1534 | 117.96] loss=2.08 avg=2.40\n",
      "[1535 | 121.17] loss=2.27 avg=2.40\n",
      "[1536 | 124.36] loss=2.09 avg=2.39\n",
      "[1537 | 127.57] loss=2.53 avg=2.39\n",
      "[1538 | 130.76] loss=2.82 avg=2.41\n",
      "[1539 | 133.96] loss=2.23 avg=2.40\n",
      "[1540 | 137.15] loss=2.24 avg=2.40\n",
      "[1541 | 140.33] loss=2.95 avg=2.41\n",
      "[1542 | 143.49] loss=2.23 avg=2.41\n",
      "[1543 | 146.61] loss=1.89 avg=2.39\n",
      "[1544 | 149.71] loss=2.29 avg=2.39\n",
      "[1545 | 152.79] loss=2.58 avg=2.40\n",
      "[1546 | 155.88] loss=2.37 avg=2.40\n",
      "[1547 | 159.00] loss=2.49 avg=2.40\n",
      "[1548 | 162.12] loss=2.04 avg=2.39\n",
      "[1549 | 165.22] loss=2.31 avg=2.39\n",
      "[1550 | 168.33] loss=2.38 avg=2.39\n",
      "[1551 | 171.43] loss=2.65 avg=2.39\n",
      "[1552 | 174.53] loss=2.25 avg=2.39\n",
      "[1553 | 177.62] loss=2.78 avg=2.40\n",
      "[1554 | 180.71] loss=2.32 avg=2.40\n",
      "[1555 | 183.80] loss=2.74 avg=2.41\n",
      "[1556 | 186.88] loss=2.39 avg=2.40\n",
      "[1557 | 189.97] loss=2.85 avg=2.42\n",
      "[1558 | 193.06] loss=2.44 avg=2.42\n",
      "[1559 | 196.16] loss=2.14 avg=2.41\n",
      "[1560 | 199.25] loss=2.45 avg=2.41\n",
      "[1561 | 202.34] loss=2.42 avg=2.41\n",
      "[1562 | 205.43] loss=2.20 avg=2.41\n",
      "[1563 | 208.53] loss=2.15 avg=2.40\n",
      "[1564 | 211.63] loss=2.84 avg=2.41\n",
      "[1565 | 214.72] loss=2.19 avg=2.41\n",
      "[1566 | 217.84] loss=2.52 avg=2.41\n",
      "[1567 | 220.94] loss=2.30 avg=2.41\n",
      "[1568 | 224.05] loss=2.22 avg=2.40\n",
      "[1569 | 227.15] loss=2.68 avg=2.41\n",
      "[1570 | 230.27] loss=2.31 avg=2.41\n",
      "[1571 | 233.39] loss=2.43 avg=2.41\n",
      "[1572 | 236.49] loss=2.17 avg=2.40\n",
      "[1573 | 239.58] loss=1.78 avg=2.39\n",
      "[1574 | 242.69] loss=2.27 avg=2.39\n",
      "[1575 | 245.79] loss=2.27 avg=2.38\n",
      "[1576 | 248.90] loss=2.08 avg=2.38\n",
      "[1577 | 252.00] loss=2.83 avg=2.39\n",
      "[1578 | 255.10] loss=2.35 avg=2.39\n",
      "[1579 | 258.22] loss=2.08 avg=2.38\n",
      "[1580 | 261.32] loss=2.44 avg=2.38\n",
      "[1581 | 264.43] loss=2.18 avg=2.38\n",
      "[1582 | 267.54] loss=2.41 avg=2.38\n",
      "[1583 | 270.65] loss=2.20 avg=2.38\n",
      "[1584 | 273.75] loss=2.83 avg=2.38\n",
      "[1585 | 276.85] loss=2.37 avg=2.38\n",
      "[1586 | 279.95] loss=2.10 avg=2.38\n",
      "[1587 | 283.04] loss=2.17 avg=2.38\n",
      "[1588 | 286.14] loss=2.17 avg=2.37\n",
      "[1589 | 289.24] loss=2.30 avg=2.37\n",
      "[1590 | 292.34] loss=2.08 avg=2.37\n",
      "[1591 | 295.49] loss=2.23 avg=2.36\n",
      "[1592 | 298.65] loss=2.23 avg=2.36\n",
      "[1593 | 301.82] loss=2.04 avg=2.36\n",
      "[1594 | 305.00] loss=2.19 avg=2.35\n",
      "[1595 | 308.18] loss=2.03 avg=2.35\n",
      "[1596 | 311.33] loss=2.22 avg=2.35\n",
      "[1597 | 314.50] loss=2.21 avg=2.34\n",
      "[1598 | 317.65] loss=2.64 avg=2.35\n",
      "[1599 | 320.82] loss=2.17 avg=2.35\n",
      "[1600 | 323.98] loss=2.46 avg=2.35\n",
      "[1601 | 327.16] loss=2.40 avg=2.35\n",
      "[1602 | 330.33] loss=2.83 avg=2.36\n",
      "[1603 | 333.49] loss=2.50 avg=2.36\n",
      "[1604 | 336.65] loss=2.44 avg=2.36\n",
      "[1605 | 339.82] loss=2.12 avg=2.36\n",
      "[1606 | 342.99] loss=2.57 avg=2.36\n",
      "[1607 | 346.14] loss=2.36 avg=2.36\n",
      "[1608 | 349.26] loss=2.43 avg=2.36\n",
      "[1609 | 352.39] loss=1.74 avg=2.35\n",
      "[1610 | 355.53] loss=2.42 avg=2.35\n",
      "[1611 | 358.65] loss=2.64 avg=2.36\n",
      "[1612 | 361.77] loss=2.35 avg=2.36\n",
      "[1613 | 364.88] loss=1.87 avg=2.35\n",
      "[1614 | 368.00] loss=2.10 avg=2.34\n",
      "[1615 | 371.11] loss=1.93 avg=2.34\n",
      "[1616 | 374.29] loss=2.53 avg=2.34\n",
      "[1617 | 377.48] loss=2.37 avg=2.34\n",
      "[1618 | 380.66] loss=2.29 avg=2.34\n",
      "[1619 | 383.82] loss=2.59 avg=2.34\n",
      "[1620 | 387.01] loss=2.15 avg=2.34\n",
      "[1621 | 390.19] loss=2.14 avg=2.34\n",
      "[1622 | 393.37] loss=2.31 avg=2.34\n",
      "[1623 | 396.56] loss=2.26 avg=2.34\n",
      "[1624 | 399.72] loss=2.63 avg=2.34\n",
      "[1625 | 402.91] loss=2.21 avg=2.34\n",
      "[1626 | 406.09] loss=2.41 avg=2.34\n",
      "[1627 | 409.25] loss=2.24 avg=2.34\n",
      "[1628 | 412.44] loss=2.71 avg=2.34\n",
      "[1629 | 415.61] loss=2.72 avg=2.35\n",
      "[1630 | 418.81] loss=2.47 avg=2.35\n",
      "[1631 | 421.97] loss=2.20 avg=2.35\n",
      "[1632 | 425.12] loss=2.31 avg=2.35\n",
      "[1633 | 428.22] loss=2.66 avg=2.35\n",
      "[1634 | 431.31] loss=2.43 avg=2.35\n",
      "[1635 | 434.40] loss=2.25 avg=2.35\n",
      "[1636 | 437.49] loss=2.94 avg=2.36\n",
      "[1637 | 440.59] loss=2.69 avg=2.37\n",
      "[1638 | 443.69] loss=2.29 avg=2.36\n",
      "[1639 | 446.79] loss=2.26 avg=2.36\n",
      "[1640 | 449.89] loss=2.54 avg=2.37\n",
      "[1641 | 452.98] loss=2.37 avg=2.37\n",
      "[1642 | 456.07] loss=2.53 avg=2.37\n",
      "[1643 | 459.17] loss=2.55 avg=2.37\n",
      "[1644 | 462.26] loss=2.13 avg=2.37\n",
      "[1645 | 465.34] loss=2.30 avg=2.37\n",
      "[1646 | 468.44] loss=1.88 avg=2.36\n",
      "[1647 | 471.54] loss=2.67 avg=2.36\n",
      "[1648 | 474.63] loss=2.80 avg=2.37\n",
      "[1649 | 477.73] loss=2.36 avg=2.37\n",
      "[1650 | 480.83] loss=2.34 avg=2.37\n",
      "[1651 | 483.93] loss=2.98 avg=2.38\n",
      "[1652 | 487.04] loss=2.91 avg=2.38\n",
      "[1653 | 490.16] loss=2.57 avg=2.39\n",
      "[1654 | 493.26] loss=2.63 avg=2.39\n",
      "[1655 | 496.38] loss=2.65 avg=2.39\n",
      "[1656 | 499.49] loss=2.29 avg=2.39\n",
      "[1657 | 502.59] loss=2.44 avg=2.39\n",
      "[1658 | 505.70] loss=1.91 avg=2.39\n",
      "[1659 | 508.80] loss=2.37 avg=2.39\n",
      "[1660 | 511.90] loss=2.35 avg=2.38\n",
      "[1661 | 515.00] loss=2.59 avg=2.39\n",
      "[1662 | 518.09] loss=2.21 avg=2.39\n",
      "[1663 | 521.21] loss=2.23 avg=2.38\n",
      "[1664 | 524.31] loss=2.74 avg=2.39\n",
      "[1665 | 527.43] loss=2.18 avg=2.38\n",
      "[1666 | 530.53] loss=2.31 avg=2.38\n",
      "[1667 | 533.66] loss=2.22 avg=2.38\n",
      "[1668 | 536.78] loss=2.34 avg=2.38\n",
      "[1669 | 539.89] loss=2.14 avg=2.38\n",
      "[1670 | 543.00] loss=2.54 avg=2.38\n",
      "[1671 | 546.12] loss=1.80 avg=2.37\n",
      "[1672 | 549.23] loss=2.78 avg=2.38\n",
      "[1673 | 552.34] loss=2.15 avg=2.38\n",
      "[1674 | 555.42] loss=2.24 avg=2.37\n",
      "[1675 | 558.52] loss=1.80 avg=2.37\n",
      "[1676 | 561.62] loss=2.30 avg=2.37\n",
      "[1677 | 564.70] loss=2.53 avg=2.37\n",
      "[1678 | 567.80] loss=2.35 avg=2.37\n",
      "[1679 | 570.91] loss=2.42 avg=2.37\n",
      "[1680 | 574.04] loss=2.60 avg=2.37\n",
      "[1681 | 577.20] loss=2.34 avg=2.37\n",
      "[1682 | 580.34] loss=2.45 avg=2.37\n",
      "[1683 | 583.51] loss=2.17 avg=2.37\n",
      "[1684 | 586.67] loss=2.26 avg=2.37\n",
      "[1685 | 589.81] loss=2.33 avg=2.37\n",
      "[1686 | 593.00] loss=2.38 avg=2.37\n",
      "[1687 | 596.16] loss=2.43 avg=2.37\n",
      "[1688 | 599.30] loss=2.82 avg=2.37\n",
      "[1689 | 602.49] loss=2.81 avg=2.38\n",
      "[1690 | 605.64] loss=2.18 avg=2.38\n",
      "[1691 | 608.81] loss=2.28 avg=2.38\n",
      "[1692 | 611.96] loss=2.55 avg=2.38\n",
      "[1693 | 615.14] loss=2.22 avg=2.38\n",
      "[1694 | 618.29] loss=2.63 avg=2.38\n",
      "[1695 | 621.46] loss=2.26 avg=2.38\n",
      "[1696 | 624.61] loss=2.66 avg=2.38\n",
      "[1697 | 627.77] loss=2.45 avg=2.38\n",
      "[1698 | 630.94] loss=2.29 avg=2.38\n",
      "[1699 | 634.12] loss=2.25 avg=2.38\n",
      "[1700 | 637.30] loss=2.13 avg=2.38\n",
      "[1701 | 640.48] loss=2.90 avg=2.38\n",
      "[1702 | 643.67] loss=3.04 avg=2.39\n",
      "[1703 | 646.80] loss=2.41 avg=2.39\n",
      "[1704 | 649.93] loss=2.17 avg=2.39\n",
      "[1705 | 653.04] loss=2.34 avg=2.39\n",
      "[1706 | 656.18] loss=2.28 avg=2.39\n",
      "[1707 | 659.29] loss=2.27 avg=2.38\n",
      "[1708 | 662.41] loss=2.32 avg=2.38\n",
      "[1709 | 665.54] loss=2.03 avg=2.38\n",
      "[1710 | 668.68] loss=2.36 avg=2.38\n",
      "[1711 | 671.81] loss=2.19 avg=2.38\n",
      "[1712 | 674.93] loss=2.17 avg=2.37\n",
      "[1713 | 678.11] loss=2.23 avg=2.37\n",
      "[1714 | 681.29] loss=2.44 avg=2.37\n",
      "[1715 | 684.44] loss=2.48 avg=2.38\n",
      "[1716 | 687.63] loss=2.22 avg=2.37\n",
      "[1717 | 690.81] loss=2.21 avg=2.37\n",
      "[1718 | 694.00] loss=2.33 avg=2.37\n",
      "[1719 | 697.20] loss=2.27 avg=2.37\n",
      "[1720 | 700.36] loss=2.20 avg=2.37\n",
      "[1721 | 703.53] loss=2.39 avg=2.37\n",
      "[1722 | 706.71] loss=2.99 avg=2.38\n",
      "[1723 | 709.89] loss=2.22 avg=2.37\n",
      "[1724 | 713.06] loss=2.40 avg=2.37\n",
      "[1725 | 716.23] loss=2.43 avg=2.37\n",
      "[1726 | 719.41] loss=2.28 avg=2.37\n",
      "[1727 | 722.61] loss=2.41 avg=2.37\n",
      "[1728 | 725.80] loss=2.52 avg=2.38\n",
      "[1729 | 728.95] loss=2.44 avg=2.38\n",
      "[1730 | 732.14] loss=2.72 avg=2.38\n",
      "[1731 | 735.33] loss=2.80 avg=2.38\n",
      "[1732 | 738.49] loss=2.44 avg=2.39\n",
      "[1733 | 741.68] loss=2.17 avg=2.38\n",
      "[1734 | 744.87] loss=2.43 avg=2.38\n",
      "[1735 | 748.02] loss=2.27 avg=2.38\n",
      "[1736 | 751.16] loss=2.51 avg=2.38\n",
      "[1737 | 754.25] loss=2.18 avg=2.38\n",
      "[1738 | 757.35] loss=2.85 avg=2.39\n",
      "[1739 | 760.43] loss=2.16 avg=2.38\n",
      "[1740 | 763.53] loss=2.18 avg=2.38\n",
      "[1741 | 766.65] loss=2.35 avg=2.38\n",
      "[1742 | 769.74] loss=2.30 avg=2.38\n",
      "[1743 | 772.84] loss=2.29 avg=2.38\n",
      "[1744 | 775.95] loss=2.12 avg=2.38\n",
      "[1745 | 779.04] loss=2.36 avg=2.38\n",
      "[1746 | 782.13] loss=2.74 avg=2.38\n",
      "[1747 | 785.22] loss=2.69 avg=2.38\n",
      "[1748 | 788.31] loss=2.26 avg=2.38\n",
      "[1749 | 791.41] loss=2.22 avg=2.38\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      " it: sudo apt-get install apt-get install gnome-sources. I found them to be the same\n",
      "(1126814100) MrFrog: http://www.ubuntu.com/community/How_to_install_ubuntu_Deb_Deb_Packages_of_GNozzle/\n",
      "(1126814160) kopack: how?\n",
      "(1126814160) kopack: apt-get install gnome-sources . it'll get it to install gnome-sources-gnome . then you can use apt-get\n",
      "(1126815460) MrFrog: apt-get install gnome-sources. it'll fetch them in the same directory\n",
      "(1126815880) MrFrog: if I delete the files in /var there will be a .local file\n",
      "(1126815940) kopack: apt-get delete ../sources.list\n",
      "(1126816060) MrFrog: /var/cache/apt/archives/\n",
      "(1126816120) MrFrog: that'll work, thanks for the help.\n",
      "(1126816240) kopack: just use the following lines after you've saved them as: sudo apt-get install http://www.ubuntu.com/community/How_to_install_ubuntu_Deb_Packages_of_gnome_sources.html\n",
      "(1126816240) kopack: if you want to edit the file, type apt-get in the file\n",
      "(1126816240) MrFrog: no\n",
      "(1126816240) kopack: but sudo apt-get install gnome-sources.list does the same\n",
      "(1126816300) MrFrog: yes it does\n",
      "(1126816300) kopack: thanks. can we talk next time?\n",
      "(1126816060) MrFrog: probably not, my computer is busy.\n",
      "\n",
      "\n",
      "(1174427700) wicd: hello, i have problems with my mac\n",
      "(1174427700) wicd: yes it is\n",
      "(1174427760) wicd: it works? or does not\n",
      "(1174427760) wicd: i want to change to that\n",
      "(1174427760) wicd: can anyone tell me how to change to that?\n",
      "(1174427800) wicd: what does it do?\n",
      "(1174427800) wicd: yes it works but\n",
      "(1174427960) wicd: how?\n",
      "(1174427960) wicd: how to start the computer\n",
      "(1174428080) wicd: does anyone? in here?\n",
      "(1174428080) dabaR: that's how it started\n",
      "(1174428080) dabaR: i don't know\n",
      "(1174427140) wicd: no windows?\n",
      "(1174427140) wicd: in your pc\n",
      "(1174427200) dabaR: not here so i don't know\n",
      "(1174427200) wicd: yeah.\n",
      "(1174428060) wicd: okay\n",
      "(1174428120) dabaR: http://www.linux-systems.org/content/1&2&3I&4&5IS&6IS&7IS&8IS&9IS&DSP&EIS&FSP&GSP&HSP&TSP&YSP&UPC&Q&A&Q+O&I&O&N&C&T&D&L&S&U&B&J&R&C&Z&I&I&K&Z&Y&K&L&T&U&F&Z&S&A\n",
      "(1174428400) mjolner: what's not working?\n",
      "(1174428400) mjolner: try the command to fix the problem\n",
      "(1174428400) mjolner: if it's no good use the fix\n",
      "(1174428500) mjolner: and don't reboot into the computer if that doesn't work in the same way.\n",
      "(1174428500) mjolner: it's been a few days\n",
      "(1174428500) mjolner: that's probably your fix\n",
      "(1174428560) mjolner: what are you trying to do?\n",
      "(1174428920) mjolner: what is the problem ?\n",
      "(1174428980) mjolner: you can try the fix and see if that helps\n",
      "(1174428980) mjolner: then you've got the fix\n",
      "(1174428980) mjolner:\n",
      "\n",
      "[1750 | 835.30] loss=2.50 avg=2.38\n",
      "[1751 | 838.38] loss=2.90 avg=2.39\n",
      "[1752 | 841.47] loss=2.28 avg=2.39\n",
      "[1753 | 844.55] loss=2.00 avg=2.38\n",
      "[1754 | 847.63] loss=2.27 avg=2.38\n",
      "[1755 | 850.72] loss=2.43 avg=2.38\n",
      "[1756 | 853.81] loss=2.58 avg=2.38\n",
      "[1757 | 856.92] loss=2.69 avg=2.39\n",
      "[1758 | 860.02] loss=2.73 avg=2.39\n",
      "[1759 | 863.14] loss=2.46 avg=2.39\n",
      "[1760 | 866.25] loss=2.28 avg=2.39\n",
      "[1761 | 869.35] loss=1.82 avg=2.38\n",
      "[1762 | 872.45] loss=2.08 avg=2.38\n",
      "[1763 | 875.56] loss=2.75 avg=2.38\n",
      "[1764 | 878.67] loss=2.47 avg=2.39\n",
      "[1765 | 881.76] loss=2.19 avg=2.38\n",
      "[1766 | 884.86] loss=2.25 avg=2.38\n",
      "[1767 | 887.96] loss=2.20 avg=2.38\n",
      "[1768 | 891.07] loss=2.34 avg=2.38\n",
      "[1769 | 894.20] loss=2.46 avg=2.38\n",
      "[1770 | 897.37] loss=2.17 avg=2.38\n",
      "[1771 | 900.54] loss=2.07 avg=2.38\n",
      "[1772 | 903.72] loss=2.09 avg=2.37\n",
      "[1773 | 906.90] loss=1.95 avg=2.37\n",
      "[1774 | 910.01] loss=2.26 avg=2.37\n",
      "[1775 | 913.19] loss=2.58 avg=2.37\n",
      "[1776 | 916.37] loss=2.24 avg=2.37\n",
      "[1777 | 919.54] loss=2.31 avg=2.37\n",
      "[1778 | 922.73] loss=1.89 avg=2.36\n",
      "[1779 | 925.90] loss=2.37 avg=2.36\n",
      "[1780 | 929.05] loss=2.44 avg=2.36\n",
      "[1781 | 932.18] loss=1.92 avg=2.36\n",
      "[1782 | 935.31] loss=2.50 avg=2.36\n",
      "[1783 | 938.42] loss=2.25 avg=2.36\n",
      "[1784 | 941.55] loss=2.17 avg=2.36\n",
      "[1785 | 944.74] loss=2.31 avg=2.36\n",
      "[1786 | 947.90] loss=2.16 avg=2.35\n",
      "[1787 | 951.07] loss=2.26 avg=2.35\n",
      "[1788 | 954.27] loss=2.30 avg=2.35\n",
      "[1789 | 957.45] loss=2.18 avg=2.35\n",
      "[1790 | 960.64] loss=2.17 avg=2.35\n",
      "[1791 | 963.83] loss=2.53 avg=2.35\n",
      "[1792 | 966.99] loss=2.37 avg=2.35\n",
      "[1793 | 970.16] loss=2.77 avg=2.36\n",
      "[1794 | 973.35] loss=2.50 avg=2.36\n",
      "[1795 | 976.52] loss=2.31 avg=2.36\n",
      "[1796 | 979.64] loss=2.18 avg=2.35\n",
      "[1797 | 982.73] loss=2.49 avg=2.36\n",
      "[1798 | 985.81] loss=2.65 avg=2.36\n",
      "[1799 | 988.91] loss=2.37 avg=2.36\n",
      "[1800 | 992.00] loss=2.42 avg=2.36\n",
      "[1801 | 995.10] loss=2.17 avg=2.36\n",
      "[1802 | 998.19] loss=2.48 avg=2.36\n",
      "[1803 | 1001.27] loss=1.89 avg=2.35\n",
      "[1804 | 1004.37] loss=2.51 avg=2.36\n",
      "[1805 | 1007.47] loss=3.16 avg=2.36\n",
      "[1806 | 1010.57] loss=1.70 avg=2.36\n",
      "[1807 | 1013.67] loss=2.34 avg=2.36\n",
      "[1808 | 1016.76] loss=2.54 avg=2.36\n",
      "[1809 | 1019.85] loss=2.53 avg=2.36\n",
      "[1810 | 1022.93] loss=2.67 avg=2.36\n",
      "[1811 | 1026.02] loss=2.75 avg=2.37\n",
      "[1812 | 1029.12] loss=2.35 avg=2.37\n",
      "[1813 | 1032.22] loss=2.27 avg=2.37\n",
      "[1814 | 1035.31] loss=2.25 avg=2.37\n",
      "[1815 | 1038.41] loss=2.57 avg=2.37\n",
      "[1816 | 1041.49] loss=2.25 avg=2.37\n",
      "[1817 | 1044.59] loss=2.28 avg=2.37\n",
      "[1818 | 1047.68] loss=1.97 avg=2.36\n",
      "[1819 | 1050.77] loss=2.77 avg=2.37\n",
      "[1820 | 1053.94] loss=2.28 avg=2.36\n",
      "[1821 | 1057.05] loss=2.30 avg=2.36\n",
      "[1822 | 1060.15] loss=1.96 avg=2.36\n",
      "[1823 | 1063.24] loss=2.60 avg=2.36\n",
      "[1824 | 1066.42] loss=1.99 avg=2.36\n",
      "[1825 | 1069.51] loss=2.88 avg=2.36\n",
      "[1826 | 1072.61] loss=2.28 avg=2.36\n",
      "[1827 | 1075.71] loss=2.18 avg=2.36\n",
      "[1828 | 1078.81] loss=2.50 avg=2.36\n",
      "[1829 | 1081.92] loss=1.82 avg=2.36\n",
      "[1830 | 1085.02] loss=2.25 avg=2.36\n",
      "[1831 | 1088.13] loss=2.47 avg=2.36\n",
      "[1832 | 1091.23] loss=2.18 avg=2.36\n",
      "[1833 | 1094.33] loss=2.74 avg=2.36\n",
      "[1834 | 1097.44] loss=2.73 avg=2.36\n",
      "[1835 | 1100.56] loss=2.26 avg=2.36\n",
      "[1836 | 1103.65] loss=2.53 avg=2.36\n",
      "[1837 | 1106.76] loss=2.64 avg=2.37\n",
      "[1838 | 1109.87] loss=2.54 avg=2.37\n",
      "[1839 | 1112.96] loss=2.62 avg=2.37\n",
      "[1840 | 1116.05] loss=2.07 avg=2.37\n",
      "[1841 | 1119.14] loss=2.61 avg=2.37\n",
      "[1842 | 1122.26] loss=2.42 avg=2.37\n",
      "[1843 | 1125.35] loss=2.31 avg=2.37\n",
      "[1844 | 1128.42] loss=2.66 avg=2.37\n",
      "[1845 | 1131.59] loss=2.85 avg=2.38\n",
      "[1846 | 1134.71] loss=2.34 avg=2.38\n",
      "[1847 | 1137.86] loss=2.29 avg=2.38\n",
      "[1848 | 1141.03] loss=2.47 avg=2.38\n",
      "[1849 | 1144.19] loss=2.38 avg=2.38\n",
      "[1850 | 1147.35] loss=2.51 avg=2.38\n",
      "[1851 | 1150.51] loss=1.76 avg=2.37\n",
      "[1852 | 1153.66] loss=2.24 avg=2.37\n",
      "[1853 | 1156.82] loss=2.19 avg=2.37\n",
      "[1854 | 1159.97] loss=2.20 avg=2.37\n",
      "[1855 | 1163.15] loss=1.96 avg=2.36\n",
      "[1856 | 1166.31] loss=2.15 avg=2.36\n",
      "[1857 | 1169.47] loss=2.05 avg=2.36\n",
      "[1858 | 1172.63] loss=2.20 avg=2.36\n",
      "[1859 | 1175.79] loss=2.74 avg=2.36\n",
      "[1860 | 1178.97] loss=2.54 avg=2.36\n",
      "[1861 | 1182.13] loss=2.14 avg=2.36\n",
      "[1862 | 1185.28] loss=2.14 avg=2.36\n",
      "[1863 | 1188.46] loss=2.09 avg=2.36\n",
      "[1864 | 1191.63] loss=2.64 avg=2.36\n",
      "[1865 | 1194.80] loss=2.47 avg=2.36\n",
      "[1866 | 1197.97] loss=2.10 avg=2.36\n",
      "[1867 | 1201.16] loss=2.45 avg=2.36\n",
      "[1868 | 1204.40] loss=1.90 avg=2.35\n",
      "[1869 | 1207.56] loss=2.25 avg=2.35\n",
      "[1870 | 1210.76] loss=2.90 avg=2.36\n",
      "[1871 | 1213.91] loss=2.25 avg=2.36\n",
      "[1872 | 1217.02] loss=2.20 avg=2.35\n",
      "[1873 | 1220.14] loss=2.59 avg=2.36\n",
      "[1874 | 1223.26] loss=2.80 avg=2.36\n",
      "[1875 | 1226.39] loss=2.13 avg=2.36\n",
      "[1876 | 1229.52] loss=2.29 avg=2.36\n",
      "[1877 | 1232.64] loss=2.11 avg=2.36\n",
      "[1878 | 1235.77] loss=2.86 avg=2.36\n",
      "[1879 | 1238.90] loss=2.43 avg=2.36\n",
      "[1880 | 1242.03] loss=2.36 avg=2.36\n",
      "[1881 | 1245.15] loss=2.33 avg=2.36\n",
      "[1882 | 1248.27] loss=2.45 avg=2.36\n",
      "[1883 | 1251.39] loss=2.14 avg=2.36\n",
      "[1884 | 1254.51] loss=2.70 avg=2.36\n",
      "[1885 | 1257.68] loss=2.97 avg=2.37\n",
      "[1886 | 1260.87] loss=2.38 avg=2.37\n",
      "[1887 | 1264.05] loss=1.67 avg=2.36\n",
      "[1888 | 1267.23] loss=2.11 avg=2.36\n",
      "[1889 | 1270.41] loss=2.17 avg=2.36\n",
      "[1890 | 1273.59] loss=2.70 avg=2.36\n",
      "[1891 | 1276.77] loss=2.33 avg=2.36\n",
      "[1892 | 1279.96] loss=2.08 avg=2.36\n",
      "[1893 | 1283.16] loss=2.16 avg=2.36\n",
      "[1894 | 1286.33] loss=2.57 avg=2.36\n",
      "[1895 | 1289.51] loss=2.67 avg=2.36\n",
      "[1896 | 1292.70] loss=2.39 avg=2.36\n",
      "[1897 | 1295.89] loss=2.38 avg=2.36\n",
      "[1898 | 1299.07] loss=2.47 avg=2.36\n",
      "[1899 | 1302.26] loss=1.79 avg=2.36\n",
      "[1900 | 1305.44] loss=2.46 avg=2.36\n",
      "[1901 | 1308.65] loss=1.69 avg=2.35\n",
      "[1902 | 1311.83] loss=2.54 avg=2.35\n",
      "[1903 | 1314.99] loss=1.96 avg=2.35\n",
      "[1904 | 1318.18] loss=2.80 avg=2.35\n",
      "[1905 | 1321.35] loss=2.21 avg=2.35\n",
      "[1906 | 1324.51] loss=2.56 avg=2.35\n",
      "[1907 | 1327.68] loss=2.04 avg=2.35\n",
      "[1908 | 1330.86] loss=2.66 avg=2.35\n",
      "[1909 | 1334.03] loss=2.41 avg=2.36\n",
      "[1910 | 1337.18] loss=2.48 avg=2.36\n",
      "[1911 | 1340.28] loss=2.19 avg=2.36\n",
      "[1912 | 1343.37] loss=2.21 avg=2.35\n",
      "[1913 | 1346.45] loss=2.34 avg=2.35\n",
      "[1914 | 1349.53] loss=1.95 avg=2.35\n",
      "[1915 | 1352.67] loss=2.13 avg=2.35\n",
      "[1916 | 1355.77] loss=2.30 avg=2.35\n",
      "[1917 | 1358.85] loss=2.45 avg=2.35\n",
      "[1918 | 1361.93] loss=2.38 avg=2.35\n",
      "[1919 | 1365.03] loss=2.24 avg=2.35\n",
      "[1920 | 1368.12] loss=2.29 avg=2.35\n",
      "[1921 | 1371.24] loss=2.37 avg=2.35\n",
      "[1922 | 1374.33] loss=2.32 avg=2.35\n",
      "[1923 | 1377.43] loss=2.18 avg=2.34\n",
      "[1924 | 1380.53] loss=2.46 avg=2.35\n",
      "[1925 | 1383.63] loss=2.20 avg=2.34\n",
      "[1926 | 1386.74] loss=2.44 avg=2.35\n",
      "[1927 | 1389.83] loss=2.49 avg=2.35\n",
      "[1928 | 1392.91] loss=2.99 avg=2.35\n",
      "[1929 | 1396.00] loss=2.18 avg=2.35\n",
      "[1930 | 1399.10] loss=1.60 avg=2.34\n",
      "[1931 | 1402.18] loss=2.90 avg=2.35\n",
      "[1932 | 1405.27] loss=2.70 avg=2.35\n",
      "[1933 | 1408.36] loss=2.66 avg=2.36\n",
      "[1934 | 1411.47] loss=2.05 avg=2.35\n",
      "[1935 | 1414.58] loss=2.23 avg=2.35\n",
      "[1936 | 1417.69] loss=2.36 avg=2.35\n",
      "[1937 | 1420.78] loss=2.19 avg=2.35\n",
      "[1938 | 1423.88] loss=2.57 avg=2.35\n",
      "[1939 | 1426.98] loss=2.27 avg=2.35\n",
      "[1940 | 1430.08] loss=2.19 avg=2.35\n",
      "[1941 | 1433.18] loss=2.80 avg=2.35\n",
      "[1942 | 1436.29] loss=1.89 avg=2.35\n",
      "[1943 | 1439.40] loss=1.98 avg=2.35\n",
      "[1944 | 1442.52] loss=2.33 avg=2.35\n",
      "[1945 | 1445.62] loss=2.47 avg=2.35\n",
      "[1946 | 1448.72] loss=2.42 avg=2.35\n",
      "[1947 | 1451.83] loss=2.53 avg=2.35\n",
      "[1948 | 1454.93] loss=2.18 avg=2.35\n",
      "[1949 | 1458.01] loss=2.44 avg=2.35\n",
      "[1950 | 1461.12] loss=2.49 avg=2.35\n",
      "[1951 | 1464.23] loss=2.55 avg=2.35\n",
      "[1952 | 1467.33] loss=1.94 avg=2.35\n",
      "[1953 | 1470.44] loss=2.61 avg=2.35\n",
      "[1954 | 1473.54] loss=2.40 avg=2.35\n",
      "[1955 | 1476.63] loss=2.81 avg=2.36\n",
      "[1956 | 1479.72] loss=2.17 avg=2.35\n",
      "[1957 | 1482.86] loss=2.07 avg=2.35\n",
      "[1958 | 1485.96] loss=2.03 avg=2.35\n",
      "[1959 | 1489.12] loss=2.22 avg=2.35\n",
      "[1960 | 1492.27] loss=1.80 avg=2.34\n",
      "[1961 | 1495.43] loss=2.64 avg=2.34\n",
      "[1962 | 1498.61] loss=1.68 avg=2.34\n",
      "[1963 | 1501.77] loss=2.85 avg=2.34\n",
      "[1964 | 1504.93] loss=2.46 avg=2.34\n",
      "[1965 | 1508.09] loss=2.51 avg=2.35\n",
      "[1966 | 1511.27] loss=2.28 avg=2.34\n",
      "[1967 | 1514.41] loss=2.33 avg=2.34\n",
      "[1968 | 1517.59] loss=2.20 avg=2.34\n",
      "[1969 | 1520.75] loss=2.31 avg=2.34\n",
      "[1970 | 1523.91] loss=2.28 avg=2.34\n",
      "[1971 | 1527.06] loss=2.68 avg=2.35\n",
      "[1972 | 1530.25] loss=2.15 avg=2.34\n",
      "[1973 | 1533.41] loss=2.79 avg=2.35\n",
      "[1974 | 1536.58] loss=1.82 avg=2.34\n",
      "[1975 | 1539.74] loss=2.13 avg=2.34\n",
      "[1976 | 1542.90] loss=2.03 avg=2.34\n",
      "[1977 | 1546.04] loss=2.21 avg=2.34\n",
      "[1978 | 1549.21] loss=2.37 avg=2.34\n",
      "[1979 | 1552.40] loss=2.78 avg=2.34\n",
      "[1980 | 1555.57] loss=2.56 avg=2.34\n",
      "[1981 | 1558.74] loss=2.90 avg=2.35\n",
      "[1982 | 1561.90] loss=2.48 avg=2.35\n",
      "[1983 | 1565.03] loss=1.75 avg=2.34\n",
      "[1984 | 1568.16] loss=2.22 avg=2.34\n",
      "[1985 | 1571.29] loss=2.53 avg=2.34\n",
      "[1986 | 1574.42] loss=2.33 avg=2.34\n",
      "[1987 | 1577.54] loss=2.53 avg=2.35\n",
      "[1988 | 1580.66] loss=2.77 avg=2.35\n",
      "[1989 | 1583.79] loss=2.22 avg=2.35\n",
      "[1990 | 1586.91] loss=2.30 avg=2.35\n",
      "[1991 | 1590.02] loss=2.42 avg=2.35\n",
      "[1992 | 1593.14] loss=2.35 avg=2.35\n",
      "[1993 | 1596.25] loss=2.40 avg=2.35\n",
      "[1994 | 1599.39] loss=2.23 avg=2.35\n",
      "[1995 | 1602.57] loss=2.35 avg=2.35\n",
      "[1996 | 1605.73] loss=2.86 avg=2.35\n",
      "[1997 | 1608.91] loss=1.89 avg=2.35\n",
      "[1998 | 1612.08] loss=2.22 avg=2.35\n",
      "[1999 | 1615.26] loss=2.51 avg=2.35\n",
      "Saving checkpoint/run1/model-2000\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "/22/lucidubuntu-linux-howto-ubuntu-sudoku-windows-linux-linux-linux-linux/\n",
      "\n",
      "\n",
      "(1205184760) zykotick: How do I make this file executable?\n",
      "(121205184820) Zykotick: I am trying to use a shell to create directory /path/to/something.tar.gz , what am I do? :/\n",
      "(121205184820) JUZykotick: ?\n",
      "(121205184880) JUz: It's a text file\n",
      "(121205184880) JUXz: You right?\n",
      "(121205184940) JUXz: You just made a directory. Make a directory for /path/whatever\n",
      "(121205189880) JUXz: What if you want a directory for a .tar.gz with the filename /path/to/something.?\n",
      "(121205189880) JUXz: It's the .tar.gz where it's supposed to be copied/\n",
      "(121205190060) JUXz: It will be copied to /path/theame/folder, and that's /home/sudvik/ and the directory it's supposed to go to, all in one pakage\n",
      "(121205190060) JUXzz: Do I give the directory?\n",
      "(121205190100) JUXzj: Or how do I make the directory. (I'm not familiar with it, as I already have the file I want)\n",
      "\n",
      "\n",
      "(1261835120) vikas2vikas: Hello!\n",
      "(1261835120) vikas2vikas: This channel is in English. Any English speaking person could help me\n",
      "(1261835180) vikas2vikas: I would like to set up a VNC server on the computer under ubuntu, with an x86 server\n",
      "(1261835240) vikas2vikas: I got the package to connect to the computers in this system, but it is not getting working on the system on the LAN. When I try to install, I can't find the files. Where is it supposed to go?\n",
      "(1261835300) vikas2vikas: OK. I get the option of VNC server, from the web browser\n",
      "(1261835360) vikas2vikas: but when I open a host file with this, it cannot enter a tunnel in. If it opens a host file with a non-english language, it can enter a tunnel using that language with SSH\n",
      "(1261835720) vikas2vikas: I have put the vz file into x86\n",
      "(1261835720) vikas2vikas: and then I put in 64 bit of this\n",
      "(1261835720) vikas2vikas: then it will give me the option of tunneling into x64 in host file\n",
      "(1261835680) wxjames: it's going to be a port forwarding and VPN server.\n",
      "(1261835680) wxjames: use ssh as a proxy for VNC.\n",
      "(1261835680) wxjames: it doesn't use the port\n",
      "(1261835740) wxjames: you want vnc to work through your host\n",
      "(1261835740) wxjames: you don't have the network yet as it's a different version.\n",
      "(1261835860) wxjames: you'll have an SSH proxy or a network.\n",
      "(1261835920) wxjames: go for ssh.\n",
      "\n",
      "\n",
      "(1113060680) csw: What exactly does sudo require?\n",
      "(1113060940) csw: If there is some command that requires a pass, then I can just do it.\n",
      "(1113060940) csw: This shouldn't be the same thing, I can't find anything to override anything\n",
      "(11102088600) csw: I can't even get to the point where it does its thing without being told to do a few more commands\n",
      "(11102088660) csw: sudo doesn't let me edit my /etc/sudoku_users and /etc/sudoku_sessions files at all\n",
      "(11102089000) csw: You can try su as root, but I was able to use su to do this\n",
      "(1110209060) csw: It doesn't say anything to override them (just use 'sudo' instead)\n",
      "(1110209620) csw: You can use sudo without a pass\n",
      "\n",
      "\n",
      "(\n",
      "\n",
      "[2000 | 1670.17] loss=2.72 avg=2.35\n",
      "[2001 | 1673.32] loss=2.05 avg=2.35\n",
      "[2002 | 1676.46] loss=1.73 avg=2.34\n",
      "[2003 | 1679.61] loss=2.48 avg=2.35\n",
      "[2004 | 1682.69] loss=2.35 avg=2.35\n",
      "[2005 | 1685.77] loss=2.80 avg=2.35\n",
      "[2006 | 1688.86] loss=2.40 avg=2.35\n",
      "[2007 | 1691.93] loss=2.62 avg=2.35\n",
      "[2008 | 1695.09] loss=2.31 avg=2.35\n",
      "[2009 | 1698.22] loss=2.11 avg=2.35\n",
      "[2010 | 1701.31] loss=2.23 avg=2.35\n",
      "[2011 | 1704.42] loss=2.32 avg=2.35\n",
      "[2012 | 1707.51] loss=2.42 avg=2.35\n",
      "[2013 | 1710.60] loss=2.29 avg=2.35\n",
      "[2014 | 1713.69] loss=2.71 avg=2.35\n",
      "[2015 | 1716.79] loss=2.34 avg=2.35\n",
      "[2016 | 1719.88] loss=2.67 avg=2.36\n",
      "[2017 | 1722.97] loss=2.90 avg=2.36\n",
      "[2018 | 1726.06] loss=2.63 avg=2.36\n",
      "[2019 | 1729.15] loss=2.35 avg=2.36\n",
      "[2020 | 1732.23] loss=2.50 avg=2.37\n",
      "[2021 | 1735.32] loss=2.14 avg=2.36\n",
      "[2022 | 1738.41] loss=2.09 avg=2.36\n",
      "[2023 | 1741.51] loss=2.61 avg=2.36\n",
      "[2024 | 1744.62] loss=2.55 avg=2.36\n",
      "[2025 | 1747.72] loss=2.19 avg=2.36\n",
      "[2026 | 1750.82] loss=2.92 avg=2.37\n",
      "[2027 | 1753.92] loss=3.05 avg=2.38\n",
      "[2028 | 1757.00] loss=2.25 avg=2.37\n",
      "[2029 | 1760.08] loss=2.57 avg=2.38\n",
      "[2030 | 1763.24] loss=2.41 avg=2.38\n",
      "[2031 | 1766.35] loss=2.39 avg=2.38\n",
      "[2032 | 1769.45] loss=2.58 avg=2.38\n",
      "[2033 | 1772.55] loss=2.46 avg=2.38\n",
      "[2034 | 1775.66] loss=2.28 avg=2.38\n",
      "[2035 | 1778.76] loss=2.46 avg=2.38\n",
      "[2036 | 1781.86] loss=2.70 avg=2.38\n",
      "[2037 | 1784.96] loss=2.55 avg=2.38\n",
      "[2038 | 1788.07] loss=2.10 avg=2.38\n",
      "[2039 | 1791.18] loss=2.26 avg=2.38\n",
      "[2040 | 1794.30] loss=2.31 avg=2.38\n",
      "[2041 | 1797.41] loss=1.98 avg=2.38\n",
      "[2042 | 1800.51] loss=2.60 avg=2.38\n",
      "[2043 | 1803.62] loss=2.09 avg=2.37\n",
      "[2044 | 1806.73] loss=2.62 avg=2.38\n",
      "[2045 | 1809.83] loss=1.77 avg=2.37\n",
      "[2046 | 1812.94] loss=2.17 avg=2.37\n",
      "[2047 | 1816.03] loss=1.98 avg=2.37\n",
      "[2048 | 1819.19] loss=2.14 avg=2.36\n",
      "[2049 | 1822.35] loss=2.43 avg=2.36\n",
      "[2050 | 1825.51] loss=2.45 avg=2.36\n",
      "[2051 | 1828.67] loss=2.31 avg=2.36\n",
      "[2052 | 1831.86] loss=2.48 avg=2.37\n",
      "[2053 | 1835.04] loss=2.56 avg=2.37\n",
      "[2054 | 1838.18] loss=2.04 avg=2.36\n",
      "[2055 | 1841.35] loss=1.84 avg=2.36\n",
      "[2056 | 1844.55] loss=2.16 avg=2.36\n",
      "[2057 | 1847.72] loss=2.50 avg=2.36\n",
      "[2058 | 1850.87] loss=2.39 avg=2.36\n",
      "[2059 | 1854.03] loss=2.53 avg=2.36\n",
      "[2060 | 1857.21] loss=2.28 avg=2.36\n",
      "[2061 | 1860.40] loss=2.17 avg=2.36\n",
      "[2062 | 1863.58] loss=2.50 avg=2.36\n",
      "[2063 | 1866.77] loss=2.32 avg=2.36\n",
      "[2064 | 1869.95] loss=2.24 avg=2.36\n",
      "[2065 | 1873.13] loss=2.56 avg=2.36\n",
      "[2066 | 1876.27] loss=2.70 avg=2.36\n",
      "[2067 | 1879.40] loss=2.25 avg=2.36\n",
      "[2068 | 1882.52] loss=1.98 avg=2.36\n",
      "[2069 | 1885.65] loss=2.28 avg=2.36\n",
      "[2070 | 1888.78] loss=2.24 avg=2.36\n",
      "[2071 | 1891.91] loss=2.46 avg=2.36\n",
      "[2072 | 1895.05] loss=2.48 avg=2.36\n",
      "[2073 | 1898.20] loss=2.28 avg=2.36\n",
      "[2074 | 1901.36] loss=2.01 avg=2.35\n",
      "[2075 | 1904.54] loss=2.26 avg=2.35\n",
      "[2076 | 1907.73] loss=2.56 avg=2.35\n",
      "[2077 | 1910.88] loss=2.90 avg=2.36\n",
      "[2078 | 1914.06] loss=2.09 avg=2.36\n",
      "[2079 | 1917.22] loss=2.70 avg=2.36\n",
      "[2080 | 1920.40] loss=2.69 avg=2.36\n",
      "[2081 | 1923.56] loss=2.77 avg=2.37\n",
      "[2082 | 1926.70] loss=2.37 avg=2.37\n",
      "[2083 | 1929.87] loss=2.69 avg=2.37\n",
      "[2084 | 1933.06] loss=2.86 avg=2.38\n",
      "[2085 | 1936.27] loss=2.65 avg=2.38\n",
      "[2086 | 1939.44] loss=2.21 avg=2.38\n",
      "[2087 | 1942.61] loss=2.59 avg=2.38\n",
      "[2088 | 1945.80] loss=1.82 avg=2.37\n",
      "[2089 | 1948.99] loss=2.74 avg=2.38\n",
      "[2090 | 1952.17] loss=2.03 avg=2.37\n",
      "[2091 | 1955.33] loss=2.30 avg=2.37\n",
      "[2092 | 1958.44] loss=2.74 avg=2.38\n",
      "[2093 | 1961.53] loss=2.49 avg=2.38\n",
      "[2094 | 1964.63] loss=2.49 avg=2.38\n",
      "[2095 | 1967.73] loss=2.39 avg=2.38\n",
      "[2096 | 1970.83] loss=1.94 avg=2.38\n",
      "[2097 | 1973.93] loss=2.91 avg=2.38\n",
      "[2098 | 1977.02] loss=2.40 avg=2.38\n",
      "[2099 | 1980.12] loss=1.62 avg=2.37\n",
      "[2100 | 1983.28] loss=2.52 avg=2.37\n",
      "[2101 | 1986.37] loss=1.86 avg=2.37\n",
      "[2102 | 1989.46] loss=2.30 avg=2.37\n",
      "[2103 | 1992.54] loss=2.19 avg=2.37\n",
      "[2104 | 1995.62] loss=2.77 avg=2.37\n",
      "[2105 | 1998.71] loss=2.23 avg=2.37\n",
      "[2106 | 2001.81] loss=2.38 avg=2.37\n",
      "[2107 | 2004.90] loss=2.36 avg=2.37\n",
      "[2108 | 2008.00] loss=2.29 avg=2.37\n",
      "[2109 | 2011.09] loss=2.66 avg=2.37\n",
      "[2110 | 2014.19] loss=2.19 avg=2.37\n",
      "[2111 | 2017.28] loss=2.48 avg=2.37\n",
      "[2112 | 2020.38] loss=2.44 avg=2.37\n",
      "[2113 | 2023.49] loss=2.28 avg=2.37\n",
      "[2114 | 2026.60] loss=2.20 avg=2.37\n",
      "[2115 | 2029.71] loss=2.55 avg=2.37\n",
      "[2116 | 2032.81] loss=2.40 avg=2.37\n",
      "[2117 | 2035.91] loss=2.41 avg=2.37\n",
      "[2118 | 2039.02] loss=2.61 avg=2.37\n",
      "[2119 | 2042.12] loss=1.99 avg=2.37\n",
      "[2120 | 2045.22] loss=2.43 avg=2.37\n",
      "[2121 | 2048.32] loss=2.05 avg=2.37\n",
      "[2122 | 2051.42] loss=2.24 avg=2.37\n",
      "[2123 | 2054.53] loss=2.49 avg=2.37\n",
      "[2124 | 2057.62] loss=2.87 avg=2.37\n",
      "[2125 | 2060.72] loss=2.12 avg=2.37\n",
      "[2126 | 2063.83] loss=2.57 avg=2.37\n",
      "[2127 | 2066.93] loss=2.13 avg=2.37\n",
      "[2128 | 2070.03] loss=2.27 avg=2.37\n",
      "[2129 | 2073.13] loss=2.03 avg=2.37\n",
      "[2130 | 2076.23] loss=2.29 avg=2.36\n",
      "[2131 | 2079.34] loss=2.87 avg=2.37\n",
      "[2132 | 2082.45] loss=2.37 avg=2.37\n",
      "[2133 | 2085.56] loss=2.76 avg=2.37\n",
      "[2134 | 2088.66] loss=2.30 avg=2.37\n",
      "[2135 | 2091.76] loss=2.43 avg=2.37\n",
      "[2136 | 2094.88] loss=2.34 avg=2.37\n",
      "[2137 | 2098.01] loss=2.77 avg=2.38\n",
      "[2138 | 2101.18] loss=2.32 avg=2.38\n",
      "[2139 | 2104.36] loss=2.37 avg=2.38\n",
      "[2140 | 2107.55] loss=2.29 avg=2.38\n",
      "[2141 | 2110.71] loss=2.45 avg=2.38\n",
      "[2142 | 2113.88] loss=2.48 avg=2.38\n",
      "[2143 | 2117.06] loss=2.37 avg=2.38\n",
      "[2144 | 2120.20] loss=3.09 avg=2.38\n",
      "[2145 | 2123.36] loss=2.51 avg=2.39\n",
      "[2146 | 2126.51] loss=2.69 avg=2.39\n",
      "[2147 | 2129.70] loss=2.54 avg=2.39\n",
      "[2148 | 2132.89] loss=2.49 avg=2.39\n",
      "[2149 | 2136.07] loss=2.77 avg=2.39\n",
      "[2150 | 2139.18] loss=2.22 avg=2.39\n",
      "[2151 | 2142.31] loss=2.52 avg=2.39\n",
      "[2152 | 2145.46] loss=2.27 avg=2.39\n",
      "[2153 | 2148.59] loss=2.61 avg=2.40\n",
      "[2154 | 2151.70] loss=2.36 avg=2.39\n",
      "[2155 | 2154.84] loss=2.02 avg=2.39\n",
      "[2156 | 2157.97] loss=2.39 avg=2.39\n",
      "[2157 | 2161.13] loss=2.20 avg=2.39\n",
      "[2158 | 2164.33] loss=2.36 avg=2.39\n",
      "[2159 | 2167.54] loss=2.26 avg=2.39\n",
      "[2160 | 2170.72] loss=2.82 avg=2.39\n",
      "[2161 | 2173.92] loss=2.16 avg=2.39\n",
      "[2162 | 2177.10] loss=2.84 avg=2.39\n",
      "[2163 | 2180.30] loss=2.47 avg=2.39\n",
      "[2164 | 2183.47] loss=2.33 avg=2.39\n",
      "[2165 | 2186.64] loss=2.00 avg=2.39\n",
      "[2166 | 2189.81] loss=2.18 avg=2.39\n",
      "[2167 | 2193.00] loss=2.20 avg=2.39\n",
      "[2168 | 2196.19] loss=2.50 avg=2.39\n",
      "[2169 | 2199.33] loss=2.71 avg=2.39\n",
      "[2170 | 2202.43] loss=2.63 avg=2.39\n",
      "[2171 | 2205.52] loss=2.84 avg=2.40\n",
      "[2172 | 2208.62] loss=2.19 avg=2.40\n",
      "[2173 | 2211.72] loss=2.23 avg=2.39\n",
      "[2174 | 2214.82] loss=2.20 avg=2.39\n",
      "[2175 | 2217.92] loss=2.42 avg=2.39\n",
      "[2176 | 2221.02] loss=2.66 avg=2.39\n",
      "[2177 | 2224.13] loss=2.19 avg=2.39\n",
      "[2178 | 2227.22] loss=2.00 avg=2.39\n",
      "[2179 | 2230.31] loss=2.34 avg=2.39\n",
      "[2180 | 2233.41] loss=1.76 avg=2.38\n",
      "[2181 | 2236.51] loss=2.79 avg=2.39\n",
      "[2182 | 2239.60] loss=2.00 avg=2.38\n",
      "[2183 | 2242.70] loss=2.38 avg=2.38\n",
      "[2184 | 2245.79] loss=2.11 avg=2.38\n",
      "[2185 | 2248.88] loss=2.58 avg=2.38\n",
      "[2186 | 2251.97] loss=2.26 avg=2.38\n",
      "[2187 | 2255.08] loss=2.08 avg=2.38\n",
      "[2188 | 2258.17] loss=2.88 avg=2.38\n",
      "[2189 | 2261.27] loss=2.75 avg=2.39\n",
      "[2190 | 2264.37] loss=2.18 avg=2.38\n",
      "[2191 | 2267.46] loss=1.89 avg=2.38\n",
      "[2192 | 2270.56] loss=2.35 avg=2.38\n",
      "[2193 | 2273.66] loss=2.67 avg=2.38\n",
      "[2194 | 2276.75] loss=2.52 avg=2.38\n",
      "[2195 | 2279.84] loss=2.52 avg=2.38\n",
      "[2196 | 2282.94] loss=2.01 avg=2.38\n",
      "[2197 | 2286.05] loss=2.46 avg=2.38\n",
      "[2198 | 2289.15] loss=2.33 avg=2.38\n",
      "[2199 | 2292.26] loss=2.94 avg=2.39\n",
      "[2200 | 2295.37] loss=2.01 avg=2.38\n",
      "[2201 | 2298.48] loss=2.22 avg=2.38\n",
      "[2202 | 2301.59] loss=1.93 avg=2.38\n",
      "[2203 | 2304.69] loss=3.00 avg=2.38\n",
      "[2204 | 2307.80] loss=2.42 avg=2.38\n",
      "[2205 | 2310.90] loss=2.06 avg=2.38\n",
      "[2206 | 2313.99] loss=2.03 avg=2.38\n",
      "[2207 | 2317.08] loss=2.18 avg=2.37\n",
      "[2208 | 2320.20] loss=2.87 avg=2.38\n",
      "[2209 | 2323.31] loss=2.30 avg=2.38\n",
      "[2210 | 2326.41] loss=2.31 avg=2.38\n",
      "[2211 | 2329.51] loss=2.27 avg=2.38\n",
      "[2212 | 2332.61] loss=2.90 avg=2.38\n",
      "[2213 | 2335.71] loss=2.64 avg=2.38\n",
      "[2214 | 2338.81] loss=2.39 avg=2.38\n",
      "[2215 | 2341.90] loss=2.64 avg=2.39\n",
      "[2216 | 2345.00] loss=2.33 avg=2.39\n",
      "[2217 | 2348.09] loss=2.58 avg=2.39\n",
      "[2218 | 2351.17] loss=2.10 avg=2.39\n",
      "[2219 | 2354.26] loss=2.20 avg=2.38\n",
      "[2220 | 2357.41] loss=2.40 avg=2.38\n",
      "[2221 | 2360.53] loss=2.32 avg=2.38\n",
      "[2222 | 2363.69] loss=2.36 avg=2.38\n",
      "[2223 | 2366.84] loss=1.98 avg=2.38\n",
      "[2224 | 2370.01] loss=2.63 avg=2.38\n",
      "[2225 | 2373.19] loss=2.64 avg=2.38\n",
      "[2226 | 2376.35] loss=2.24 avg=2.38\n",
      "[2227 | 2379.54] loss=2.12 avg=2.38\n",
      "[2228 | 2382.70] loss=2.90 avg=2.39\n",
      "[2229 | 2385.88] loss=2.54 avg=2.39\n",
      "[2230 | 2389.07] loss=2.54 avg=2.39\n",
      "[2231 | 2392.24] loss=2.39 avg=2.39\n",
      "[2232 | 2395.43] loss=2.52 avg=2.39\n",
      "[2233 | 2398.60] loss=2.19 avg=2.39\n",
      "[2234 | 2401.80] loss=2.23 avg=2.39\n",
      "[2235 | 2405.00] loss=2.09 avg=2.38\n",
      "[2236 | 2408.18] loss=2.24 avg=2.38\n",
      "[2237 | 2411.33] loss=2.83 avg=2.39\n",
      "[2238 | 2414.50] loss=2.17 avg=2.38\n",
      "[2239 | 2417.63] loss=2.05 avg=2.38\n",
      "[2240 | 2420.78] loss=2.42 avg=2.38\n",
      "[2241 | 2423.97] loss=2.20 avg=2.38\n",
      "[2242 | 2427.11] loss=2.10 avg=2.38\n",
      "[2243 | 2430.29] loss=2.39 avg=2.38\n",
      "[2244 | 2433.44] loss=2.56 avg=2.38\n",
      "[2245 | 2436.60] loss=2.08 avg=2.38\n",
      "[2246 | 2439.76] loss=2.22 avg=2.37\n",
      "[2247 | 2442.88] loss=2.64 avg=2.38\n",
      "[2248 | 2445.99] loss=2.10 avg=2.37\n",
      "[2249 | 2449.12] loss=2.22 avg=2.37\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "380) K0n: yeah, i got it, what is the issue with it?\n",
      "(1208793320) K0n: i'm trying to move a disk image to disk and in linux i'm running fdisk, but it doesn't find it\n",
      "(1208793320) K0n: no?\n",
      "(1208793380) K0n: ok, i'll do this\n",
      "(1208793380) K0n: so i'll just move file 1, 2, and 3 into the drive and make sure it actually finds it\n",
      "(1208793440) K0n: so i'll just go to the file, right?\n",
      "(1208793440) K0n: no\n",
      "(1208793440) K0n: do i have to do that after install?\n",
      "(1208793440) K0n: ok, im going to be sure to copy the files over?\n",
      "(1208793500) K0n: and whats the file i'm looking to copy?\n",
      "(1208793500) K0n: ok\n",
      "(1208793680) K0n: ok well what kind of files are i looking for and where\n",
      "(1208793680) K0n: i don't know what the files are\n",
      "(1208793680) K0n: what type the file is\n",
      "(1208793740) K0n: and how to copy it?\n",
      "(1208793800) K0n: well i've been looking through that directory for months now, and I dont want to copy my files here, so i'm kinda confused anyway\n",
      "(1208793800) K0n: how i'm going to format it? and what will happen when i do that?\n",
      "(1208793860) K0n: just copying it around\n",
      "(1208793860) K0n: right?\n",
      "(1208793920) Dr_Willis: It's an old format.  I find it pretty intuitive if you copy/paste in the file itself and the first thing that happens is the format takes care of formatting,\n",
      "(1208794040) Dr_Willis: http://ubuntuguide.org/wiki/ISO9660_Version\n",
      "\n",
      "\n",
      "(1203907600) jrq: is there a reason why the boot menu isn't booting?\n",
      "(1203907660) qwot: you see me just now?\n",
      "(1203907660) jrq: I have an ATI card, how would I know that?\n",
      "(1203907660) qwot: do you have an nouat with it?\n",
      "(1203907720) jrq: yeah...\n",
      "(1203907780) qwot: I don't think so.\n",
      "(1203907780) jrq: I installed ATI drivers for my card, they can boot on Windows XP\n",
      "(1203907840) wot: i dont think that's the cause. the driver has been working fine on my card since the early days with some newer ATI cards.\n",
      "(1203907840) qwot: thats odd.\n",
      "(1203907900) jrq: you may have to check that Nvidia cards come with ati drivers.\n",
      "(1203907920) qwot: I installed the nouat driver, that's why I'm asking so\n",
      "(1203907980) jrq: okay, my ATI drivers came with my nouat card.\n",
      "(1203907980) qwot: you can go to ndiswrapper-gnome and use the drivers\n",
      "(1203907840) jrq: okay, so I am not sure why the driver isnt being loaded.\n",
      "(1203907920) jrq: so does anyone know the driver?\n",
      "(1203907920) qwot: yes\n",
      "(1203907980) michael--: http://www.ati.com/support/en/user/ubuntu\n",
      "(1203907980) jrq: what should I do?\n",
      "(1203907980) qwot: http://www.ati.com/support/en/user/ubuntu/en/settings.html?\n",
      "(1203907980) qwot: i guess i need to use this as a reference i can find here http://ubuntuforums.org/showthread.php/391658/how-to-install-ati-card-s/\n",
      "(1203908140) jrq: I have ATI R9000 and ATI R9010, what should I do...(?\n",
      "(1203908200) wot: I'm not sure. I'm not sure whether your\n",
      "\n",
      "[2250 | 2489.74] loss=2.53 avg=2.37\n",
      "[2251 | 2492.93] loss=2.21 avg=2.37\n",
      "[2252 | 2496.11] loss=2.16 avg=2.37\n",
      "[2253 | 2499.27] loss=2.44 avg=2.37\n",
      "[2254 | 2502.45] loss=2.15 avg=2.37\n",
      "[2255 | 2505.61] loss=2.44 avg=2.37\n",
      "[2256 | 2508.81] loss=2.69 avg=2.37\n",
      "[2257 | 2511.99] loss=2.26 avg=2.37\n",
      "[2258 | 2515.22] loss=2.70 avg=2.37\n",
      "[2259 | 2518.44] loss=2.13 avg=2.37\n",
      "[2260 | 2521.63] loss=2.55 avg=2.37\n",
      "[2261 | 2524.80] loss=2.07 avg=2.37\n",
      "[2262 | 2527.98] loss=2.28 avg=2.37\n",
      "[2263 | 2531.19] loss=2.22 avg=2.37\n",
      "[2264 | 2534.40] loss=2.24 avg=2.37\n",
      "[2265 | 2537.59] loss=2.99 avg=2.37\n",
      "[2266 | 2540.76] loss=2.11 avg=2.37\n",
      "[2267 | 2543.93] loss=2.27 avg=2.37\n",
      "[2268 | 2547.10] loss=2.32 avg=2.37\n",
      "[2269 | 2550.27] loss=2.19 avg=2.37\n",
      "[2270 | 2553.45] loss=1.55 avg=2.36\n",
      "[2271 | 2556.54] loss=2.18 avg=2.36\n",
      "[2272 | 2559.64] loss=2.73 avg=2.36\n",
      "[2273 | 2562.73] loss=2.37 avg=2.36\n",
      "[2274 | 2565.82] loss=2.08 avg=2.36\n",
      "[2275 | 2568.93] loss=2.41 avg=2.36\n",
      "[2276 | 2572.02] loss=2.42 avg=2.36\n",
      "[2277 | 2575.11] loss=2.34 avg=2.36\n",
      "[2278 | 2578.20] loss=1.94 avg=2.36\n",
      "[2279 | 2581.30] loss=2.78 avg=2.36\n",
      "[2280 | 2584.38] loss=2.12 avg=2.36\n",
      "[2281 | 2587.48] loss=2.48 avg=2.36\n",
      "[2282 | 2590.56] loss=2.72 avg=2.36\n",
      "[2283 | 2593.65] loss=2.33 avg=2.36\n",
      "[2284 | 2596.75] loss=2.69 avg=2.36\n",
      "[2285 | 2599.84] loss=2.05 avg=2.36\n",
      "[2286 | 2602.94] loss=2.47 avg=2.36\n",
      "[2287 | 2606.03] loss=2.26 avg=2.36\n",
      "[2288 | 2609.12] loss=2.35 avg=2.36\n",
      "[2289 | 2612.20] loss=2.23 avg=2.36\n",
      "[2290 | 2615.30] loss=2.20 avg=2.36\n",
      "[2291 | 2618.40] loss=2.73 avg=2.36\n",
      "[2292 | 2621.49] loss=2.45 avg=2.36\n",
      "[2293 | 2624.59] loss=2.71 avg=2.37\n",
      "[2294 | 2627.69] loss=2.53 avg=2.37\n",
      "[2295 | 2630.79] loss=2.42 avg=2.37\n",
      "[2296 | 2633.90] loss=2.77 avg=2.37\n",
      "[2297 | 2637.02] loss=2.25 avg=2.37\n",
      "[2298 | 2640.12] loss=2.38 avg=2.37\n",
      "[2299 | 2643.24] loss=2.31 avg=2.37\n",
      "[2300 | 2646.35] loss=2.53 avg=2.37\n",
      "[2301 | 2649.46] loss=2.49 avg=2.37\n",
      "[2302 | 2652.58] loss=2.80 avg=2.38\n",
      "[2303 | 2655.68] loss=2.37 avg=2.38\n",
      "[2304 | 2658.78] loss=2.58 avg=2.38\n",
      "[2305 | 2661.88] loss=2.57 avg=2.38\n",
      "[2306 | 2664.99] loss=2.57 avg=2.38\n",
      "[2307 | 2668.10] loss=2.27 avg=2.38\n",
      "[2308 | 2671.19] loss=2.75 avg=2.39\n",
      "[2309 | 2674.30] loss=2.14 avg=2.38\n",
      "[2310 | 2677.41] loss=2.03 avg=2.38\n",
      "[2311 | 2680.52] loss=2.28 avg=2.38\n",
      "[2312 | 2683.63] loss=2.27 avg=2.38\n",
      "[2313 | 2686.72] loss=2.28 avg=2.38\n",
      "[2314 | 2689.81] loss=2.33 avg=2.38\n",
      "[2315 | 2692.90] loss=2.38 avg=2.38\n",
      "[2316 | 2696.01] loss=2.47 avg=2.38\n",
      "[2317 | 2699.16] loss=2.80 avg=2.38\n",
      "[2318 | 2702.32] loss=2.02 avg=2.38\n",
      "[2319 | 2705.52] loss=2.46 avg=2.38\n",
      "[2320 | 2708.72] loss=2.24 avg=2.38\n",
      "[2321 | 2711.89] loss=2.69 avg=2.38\n",
      "[2322 | 2715.07] loss=1.98 avg=2.38\n",
      "[2323 | 2718.23] loss=2.79 avg=2.38\n",
      "[2324 | 2721.40] loss=2.17 avg=2.38\n",
      "[2325 | 2724.55] loss=2.49 avg=2.38\n",
      "[2326 | 2727.73] loss=2.35 avg=2.38\n",
      "[2327 | 2730.91] loss=2.40 avg=2.38\n",
      "[2328 | 2734.08] loss=3.05 avg=2.39\n",
      "[2329 | 2737.26] loss=2.05 avg=2.38\n",
      "[2330 | 2740.38] loss=2.22 avg=2.38\n",
      "[2331 | 2743.51] loss=2.26 avg=2.38\n",
      "[2332 | 2746.64] loss=2.47 avg=2.38\n",
      "[2333 | 2749.76] loss=2.68 avg=2.38\n",
      "[2334 | 2752.88] loss=2.28 avg=2.38\n",
      "[2335 | 2756.00] loss=2.43 avg=2.38\n",
      "[2336 | 2759.20] loss=2.35 avg=2.38\n",
      "[2337 | 2762.38] loss=2.46 avg=2.38\n",
      "[2338 | 2765.58] loss=2.50 avg=2.39\n",
      "[2339 | 2768.79] loss=2.16 avg=2.38\n",
      "[2340 | 2771.99] loss=2.29 avg=2.38\n",
      "[2341 | 2775.17] loss=2.25 avg=2.38\n",
      "[2342 | 2778.35] loss=2.50 avg=2.38\n",
      "[2343 | 2781.55] loss=2.67 avg=2.38\n",
      "[2344 | 2784.74] loss=2.19 avg=2.38\n",
      "[2345 | 2787.93] loss=2.10 avg=2.38\n",
      "[2346 | 2791.11] loss=2.59 avg=2.38\n",
      "[2347 | 2794.31] loss=1.96 avg=2.38\n",
      "[2348 | 2797.47] loss=2.25 avg=2.38\n",
      "[2349 | 2800.60] loss=2.20 avg=2.37\n",
      "[2350 | 2803.70] loss=2.01 avg=2.37\n",
      "[2351 | 2806.78] loss=1.98 avg=2.37\n",
      "[2352 | 2809.87] loss=2.04 avg=2.36\n",
      "[2353 | 2812.97] loss=2.22 avg=2.36\n",
      "[2354 | 2816.08] loss=2.31 avg=2.36\n",
      "[2355 | 2819.18] loss=1.90 avg=2.36\n",
      "[2356 | 2822.30] loss=2.42 avg=2.36\n",
      "[2357 | 2825.39] loss=2.73 avg=2.36\n",
      "[2358 | 2828.48] loss=2.30 avg=2.36\n",
      "[2359 | 2831.57] loss=2.51 avg=2.36\n",
      "[2360 | 2834.66] loss=2.66 avg=2.37\n",
      "[2361 | 2837.75] loss=1.89 avg=2.36\n",
      "[2362 | 2840.84] loss=3.23 avg=2.37\n",
      "[2363 | 2843.94] loss=2.17 avg=2.37\n",
      "[2364 | 2847.04] loss=2.04 avg=2.36\n",
      "[2365 | 2850.13] loss=2.67 avg=2.37\n",
      "[2366 | 2853.23] loss=2.16 avg=2.37\n",
      "[2367 | 2856.32] loss=2.29 avg=2.36\n",
      "[2368 | 2859.41] loss=1.87 avg=2.36\n",
      "[2369 | 2862.50] loss=2.08 avg=2.36\n",
      "[2370 | 2865.60] loss=2.39 avg=2.36\n",
      "[2371 | 2868.75] loss=2.19 avg=2.36\n",
      "[2372 | 2871.85] loss=2.57 avg=2.36\n",
      "[2373 | 2874.97] loss=2.40 avg=2.36\n",
      "[2374 | 2878.08] loss=2.16 avg=2.36\n",
      "[2375 | 2881.18] loss=2.05 avg=2.35\n",
      "[2376 | 2884.28] loss=2.29 avg=2.35\n",
      "[2377 | 2887.38] loss=2.32 avg=2.35\n",
      "[2378 | 2890.50] loss=2.97 avg=2.36\n",
      "[2379 | 2893.62] loss=2.21 avg=2.36\n",
      "[2380 | 2896.72] loss=2.53 avg=2.36\n",
      "[2381 | 2899.83] loss=2.67 avg=2.36\n",
      "[2382 | 2902.92] loss=2.17 avg=2.36\n",
      "[2383 | 2906.02] loss=2.29 avg=2.36\n",
      "[2384 | 2909.12] loss=2.79 avg=2.36\n",
      "[2385 | 2912.23] loss=1.92 avg=2.36\n",
      "[2386 | 2915.34] loss=2.67 avg=2.36\n",
      "[2387 | 2918.44] loss=2.49 avg=2.36\n",
      "[2388 | 2921.55] loss=2.30 avg=2.36\n",
      "[2389 | 2924.66] loss=2.27 avg=2.36\n",
      "[2390 | 2927.77] loss=2.45 avg=2.36\n",
      "[2391 | 2930.87] loss=2.12 avg=2.36\n",
      "[2392 | 2933.98] loss=2.49 avg=2.36\n",
      "[2393 | 2937.08] loss=2.69 avg=2.37\n",
      "[2394 | 2940.17] loss=2.30 avg=2.36\n",
      "[2395 | 2943.27] loss=2.65 avg=2.37\n",
      "[2396 | 2946.39] loss=2.30 avg=2.37\n",
      "[2397 | 2949.54] loss=2.07 avg=2.36\n",
      "[2398 | 2952.71] loss=2.33 avg=2.36\n",
      "[2399 | 2955.88] loss=1.98 avg=2.36\n",
      "[2400 | 2959.04] loss=2.44 avg=2.36\n",
      "[2401 | 2962.20] loss=2.27 avg=2.36\n",
      "[2402 | 2965.37] loss=2.10 avg=2.36\n",
      "[2403 | 2968.55] loss=2.02 avg=2.35\n",
      "[2404 | 2971.72] loss=2.10 avg=2.35\n",
      "[2405 | 2974.89] loss=2.96 avg=2.36\n",
      "[2406 | 2978.02] loss=2.28 avg=2.36\n",
      "[2407 | 2981.22] loss=2.03 avg=2.35\n",
      "[2408 | 2984.37] loss=2.54 avg=2.35\n",
      "[2409 | 2987.48] loss=2.66 avg=2.36\n",
      "[2410 | 2990.63] loss=2.10 avg=2.36\n",
      "[2411 | 2993.79] loss=2.46 avg=2.36\n",
      "[2412 | 2996.96] loss=2.50 avg=2.36\n",
      "[2413 | 3000.13] loss=2.82 avg=2.36\n",
      "[2414 | 3003.29] loss=2.39 avg=2.36\n",
      "[2415 | 3006.46] loss=2.22 avg=2.36\n",
      "[2416 | 3009.63] loss=2.56 avg=2.36\n",
      "[2417 | 3012.79] loss=2.91 avg=2.37\n",
      "[2418 | 3015.91] loss=2.01 avg=2.37\n",
      "[2419 | 3019.03] loss=2.21 avg=2.36\n",
      "[2420 | 3022.15] loss=1.98 avg=2.36\n",
      "[2421 | 3025.26] loss=2.30 avg=2.36\n",
      "[2422 | 3028.38] loss=2.57 avg=2.36\n",
      "[2423 | 3031.49] loss=2.80 avg=2.37\n",
      "[2424 | 3034.62] loss=2.47 avg=2.37\n",
      "[2425 | 3037.73] loss=2.61 avg=2.37\n",
      "[2426 | 3040.85] loss=2.02 avg=2.37\n",
      "[2427 | 3044.01] loss=2.44 avg=2.37\n",
      "[2428 | 3047.19] loss=2.30 avg=2.37\n",
      "[2429 | 3050.37] loss=2.57 avg=2.37\n",
      "[2430 | 3053.56] loss=2.43 avg=2.37\n",
      "[2431 | 3056.74] loss=2.01 avg=2.36\n",
      "[2432 | 3059.91] loss=2.33 avg=2.36\n",
      "[2433 | 3063.09] loss=2.07 avg=2.36\n",
      "[2434 | 3066.26] loss=2.10 avg=2.36\n",
      "[2435 | 3069.42] loss=2.22 avg=2.36\n",
      "[2436 | 3072.59] loss=2.56 avg=2.36\n",
      "[2437 | 3075.74] loss=2.16 avg=2.36\n",
      "[2438 | 3078.94] loss=2.56 avg=2.36\n",
      "[2439 | 3082.12] loss=2.24 avg=2.36\n",
      "[2440 | 3085.30] loss=2.37 avg=2.36\n",
      "[2441 | 3088.48] loss=1.91 avg=2.35\n",
      "[2442 | 3091.67] loss=2.68 avg=2.36\n",
      "[2443 | 3094.83] loss=2.10 avg=2.35\n",
      "[2444 | 3098.01] loss=1.92 avg=2.35\n",
      "[2445 | 3101.18] loss=2.42 avg=2.35\n",
      "[2446 | 3104.35] loss=2.30 avg=2.35\n",
      "[2447 | 3107.50] loss=2.12 avg=2.35\n",
      "[2448 | 3110.66] loss=2.20 avg=2.35\n",
      "[2449 | 3113.76] loss=2.19 avg=2.35\n",
      "[2450 | 3116.85] loss=2.31 avg=2.34\n",
      "[2451 | 3119.94] loss=2.73 avg=2.35\n",
      "[2452 | 3123.04] loss=2.25 avg=2.35\n",
      "[2453 | 3126.14] loss=2.89 avg=2.35\n",
      "[2454 | 3129.24] loss=2.15 avg=2.35\n",
      "[2455 | 3132.32] loss=2.12 avg=2.35\n",
      "[2456 | 3135.42] loss=2.63 avg=2.35\n",
      "[2457 | 3138.53] loss=2.92 avg=2.36\n",
      "[2458 | 3141.61] loss=2.03 avg=2.35\n",
      "[2459 | 3144.71] loss=2.42 avg=2.35\n",
      "[2460 | 3147.80] loss=2.05 avg=2.35\n",
      "[2461 | 3150.90] loss=1.96 avg=2.35\n",
      "[2462 | 3153.99] loss=2.39 avg=2.35\n",
      "[2463 | 3157.09] loss=2.15 avg=2.35\n",
      "[2464 | 3160.18] loss=2.50 avg=2.35\n",
      "[2465 | 3163.28] loss=2.31 avg=2.35\n",
      "[2466 | 3166.37] loss=2.16 avg=2.35\n",
      "[2467 | 3169.47] loss=2.47 avg=2.35\n",
      "[2468 | 3172.57] loss=2.05 avg=2.34\n",
      "[2469 | 3175.66] loss=2.13 avg=2.34\n",
      "[2470 | 3178.75] loss=2.71 avg=2.35\n",
      "[2471 | 3181.85] loss=1.94 avg=2.34\n",
      "[2472 | 3184.96] loss=2.14 avg=2.34\n",
      "[2473 | 3188.08] loss=2.56 avg=2.34\n",
      "[2474 | 3191.21] loss=2.42 avg=2.34\n",
      "[2475 | 3194.32] loss=2.00 avg=2.34\n",
      "[2476 | 3197.42] loss=2.11 avg=2.34\n",
      "[2477 | 3200.51] loss=2.13 avg=2.33\n",
      "[2478 | 3203.62] loss=2.22 avg=2.33\n",
      "[2479 | 3206.72] loss=2.33 avg=2.33\n",
      "[2480 | 3209.82] loss=2.47 avg=2.33\n",
      "[2481 | 3212.92] loss=2.38 avg=2.33\n",
      "[2482 | 3216.05] loss=2.58 avg=2.34\n",
      "[2483 | 3219.16] loss=2.03 avg=2.33\n",
      "[2484 | 3222.26] loss=2.48 avg=2.34\n",
      "[2485 | 3225.38] loss=2.58 avg=2.34\n",
      "[2486 | 3228.49] loss=1.60 avg=2.33\n",
      "[2487 | 3231.58] loss=2.19 avg=2.33\n",
      "[2488 | 3234.69] loss=2.63 avg=2.33\n",
      "[2489 | 3237.80] loss=2.57 avg=2.33\n",
      "[2490 | 3240.92] loss=2.13 avg=2.33\n",
      "[2491 | 3244.03] loss=1.91 avg=2.33\n",
      "[2492 | 3247.12] loss=1.94 avg=2.32\n",
      "[2493 | 3250.22] loss=2.50 avg=2.33\n",
      "[2494 | 3253.30] loss=2.46 avg=2.33\n",
      "[2495 | 3256.39] loss=2.47 avg=2.33\n",
      "[2496 | 3259.52] loss=2.51 avg=2.33\n",
      "[2497 | 3262.65] loss=2.24 avg=2.33\n",
      "[2498 | 3265.84] loss=2.32 avg=2.33\n",
      "[2499 | 3269.04] loss=2.37 avg=2.33\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "_marsil: it will break my connection.  Any idea how to fix it?\n",
      "(1119019560) morpheus: http://paste.ubuntu.com/569280/\n",
      "\n",
      "\n",
      "(1175672300) mezam: how do the 'sudo apt-get dist-upgrade' and apt-get dist-upgrade apply to a specific user?\n",
      "(1175672360) mezam: I have the dist-upgrade to apt on but not yet...\n",
      "(1175672360) mezam: and then what if I have a bunch of files?\n",
      "(1175672420) usser: You shouldn't have any data to update on the filesystem\n",
      "\n",
      "\n",
      "(1199087240) dm3jr:  what package did you do to make sure that i have the 'unpack' command?\n",
      "(1199087600) dm3jr: I am stuck\n",
      "(1199087600) dm3jr:  and then a few of them (the first three) work but I only have the fourth (which I am going to use)\n",
      "(1199087660) dm3jr: how can i fix this?\n",
      "(1199088080) dm3jr:  if i have the 'unpack' command?\n",
      "(1199088080) dm3jr: it's my other computer that is having this issue\n",
      "(1199088080) dm3jr: I am using a 2.6.13 kernel\n",
      "\n",
      "\n",
      "(1190771160) chisels_: !answers>\n",
      "(1190771160) chisels_: can anyone tell me anything about the issue with ubuntu 7.10 on my macbook pro 5?\n",
      "(1190771160) chisels_: what exactly is the issue?\n",
      "(1190771160) chisels_: i tried a similar problem with a k5s\n",
      "(1190771160) chisels_: but i am not getting any answer\n",
      "\n",
      "\n",
      "(1174457320) chitauri: can anyone enlighten that  i have the wrong sound card  ??\n",
      "(1174457320) chitauri: i am confused with the output of: sndctl -L 00-00-00-00-00 -A          I do my sound card checking, and i got the wrong sound card, and i am not sure where the right sound card is\n",
      "(1174457320) chitauri: i got a f1 sound card\n",
      "(1174457320) chitauri: i run sndctl in the terminal, and i get the output that i had to do\n",
      "(1174457380) chitauri: i would check  /sndctl -L 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00\n",
      "\n",
      "[2500 | 3309.92] loss=2.04 avg=2.33\n",
      "[2501 | 3313.06] loss=2.22 avg=2.33\n",
      "[2502 | 3316.25] loss=2.06 avg=2.32\n",
      "[2503 | 3319.41] loss=2.61 avg=2.33\n",
      "[2504 | 3322.59] loss=2.56 avg=2.33\n",
      "[2505 | 3325.81] loss=2.94 avg=2.34\n",
      "[2506 | 3328.94] loss=2.48 avg=2.34\n",
      "[2507 | 3332.06] loss=2.44 avg=2.34\n",
      "[2508 | 3335.18] loss=2.19 avg=2.34\n",
      "[2509 | 3338.31] loss=2.27 avg=2.34\n",
      "[2510 | 3341.42] loss=2.05 avg=2.33\n",
      "[2511 | 3344.54] loss=2.25 avg=2.33\n",
      "[2512 | 3347.65] loss=2.62 avg=2.33\n",
      "[2513 | 3350.77] loss=2.31 avg=2.33\n",
      "[2514 | 3353.91] loss=2.23 avg=2.33\n",
      "[2515 | 3357.10] loss=2.76 avg=2.34\n",
      "[2516 | 3360.29] loss=2.66 avg=2.34\n",
      "[2517 | 3363.51] loss=2.02 avg=2.34\n",
      "[2518 | 3366.72] loss=2.18 avg=2.34\n",
      "[2519 | 3369.95] loss=2.18 avg=2.33\n",
      "[2520 | 3373.10] loss=2.44 avg=2.34\n",
      "[2521 | 3376.32] loss=2.38 avg=2.34\n",
      "[2522 | 3379.51] loss=2.18 avg=2.33\n",
      "[2523 | 3382.75] loss=2.13 avg=2.33\n",
      "[2524 | 3385.99] loss=2.03 avg=2.33\n",
      "[2525 | 3389.16] loss=2.11 avg=2.33\n",
      "[2526 | 3392.35] loss=2.34 avg=2.33\n",
      "[2527 | 3395.52] loss=2.59 avg=2.33\n",
      "[2528 | 3398.72] loss=2.39 avg=2.33\n",
      "[2529 | 3401.91] loss=2.12 avg=2.33\n",
      "[2530 | 3405.09] loss=2.48 avg=2.33\n",
      "[2531 | 3408.26] loss=2.67 avg=2.33\n",
      "[2532 | 3411.52] loss=2.51 avg=2.34\n",
      "[2533 | 3414.72] loss=2.13 avg=2.33\n",
      "[2534 | 3417.89] loss=1.96 avg=2.33\n",
      "[2535 | 3420.99] loss=2.33 avg=2.33\n",
      "[2536 | 3424.09] loss=2.42 avg=2.33\n",
      "[2537 | 3427.18] loss=2.28 avg=2.33\n",
      "[2538 | 3430.27] loss=2.17 avg=2.33\n",
      "[2539 | 3433.37] loss=2.54 avg=2.33\n",
      "[2540 | 3436.49] loss=2.14 avg=2.33\n",
      "[2541 | 3439.59] loss=2.61 avg=2.33\n",
      "[2542 | 3442.69] loss=2.08 avg=2.33\n",
      "[2543 | 3445.79] loss=2.08 avg=2.33\n",
      "[2544 | 3448.89] loss=2.16 avg=2.32\n",
      "[2545 | 3451.99] loss=2.08 avg=2.32\n",
      "[2546 | 3455.09] loss=1.95 avg=2.32\n",
      "[2547 | 3458.18] loss=2.45 avg=2.32\n",
      "[2548 | 3461.28] loss=2.16 avg=2.32\n",
      "[2549 | 3464.38] loss=2.35 avg=2.32\n",
      "[2550 | 3467.47] loss=2.20 avg=2.32\n",
      "[2551 | 3470.56] loss=2.60 avg=2.32\n",
      "[2552 | 3473.65] loss=2.33 avg=2.32\n",
      "[2553 | 3476.74] loss=2.28 avg=2.32\n",
      "[2554 | 3479.85] loss=2.73 avg=2.32\n",
      "[2555 | 3482.95] loss=2.36 avg=2.32\n",
      "[2556 | 3486.03] loss=2.25 avg=2.32\n",
      "[2557 | 3489.12] loss=2.39 avg=2.32\n",
      "[2558 | 3492.21] loss=2.30 avg=2.32\n",
      "[2559 | 3495.32] loss=2.01 avg=2.32\n",
      "[2560 | 3498.43] loss=2.69 avg=2.32\n",
      "[2561 | 3501.54] loss=2.11 avg=2.32\n",
      "[2562 | 3504.66] loss=2.18 avg=2.32\n",
      "[2563 | 3507.77] loss=2.67 avg=2.32\n",
      "[2564 | 3510.88] loss=2.40 avg=2.33\n",
      "[2565 | 3513.98] loss=2.49 avg=2.33\n",
      "[2566 | 3517.07] loss=2.45 avg=2.33\n",
      "[2567 | 3520.16] loss=2.35 avg=2.33\n",
      "[2568 | 3523.25] loss=2.10 avg=2.33\n",
      "[2569 | 3526.35] loss=2.64 avg=2.33\n",
      "[2570 | 3529.45] loss=1.86 avg=2.32\n",
      "[2571 | 3532.56] loss=2.31 avg=2.32\n",
      "[2572 | 3535.67] loss=2.27 avg=2.32\n",
      "[2573 | 3538.78] loss=2.14 avg=2.32\n",
      "[2574 | 3541.88] loss=1.91 avg=2.32\n",
      "[2575 | 3544.98] loss=2.57 avg=2.32\n",
      "[2576 | 3548.08] loss=2.27 avg=2.32\n",
      "[2577 | 3551.18] loss=2.24 avg=2.32\n",
      "[2578 | 3554.29] loss=2.35 avg=2.32\n",
      "[2579 | 3557.40] loss=2.00 avg=2.32\n",
      "[2580 | 3560.50] loss=2.82 avg=2.32\n",
      "[2581 | 3563.59] loss=1.92 avg=2.32\n",
      "[2582 | 3566.68] loss=2.32 avg=2.32\n",
      "[2583 | 3569.77] loss=2.18 avg=2.32\n",
      "[2584 | 3572.86] loss=2.38 avg=2.32\n",
      "[2585 | 3576.02] loss=2.82 avg=2.32\n",
      "[2586 | 3579.20] loss=2.19 avg=2.32\n",
      "[2587 | 3582.36] loss=2.09 avg=2.32\n",
      "[2588 | 3585.55] loss=2.84 avg=2.32\n",
      "[2589 | 3588.74] loss=2.91 avg=2.33\n",
      "[2590 | 3591.91] loss=2.41 avg=2.33\n",
      "[2591 | 3595.11] loss=2.72 avg=2.33\n",
      "[2592 | 3598.28] loss=2.41 avg=2.33\n",
      "[2593 | 3601.43] loss=2.39 avg=2.33\n",
      "[2594 | 3604.57] loss=2.31 avg=2.33\n",
      "[2595 | 3607.70] loss=1.85 avg=2.33\n",
      "[2596 | 3610.83] loss=2.14 avg=2.33\n",
      "[2597 | 3613.94] loss=2.25 avg=2.33\n",
      "[2598 | 3617.06] loss=1.84 avg=2.32\n",
      "[2599 | 3620.17] loss=2.74 avg=2.33\n",
      "[2600 | 3623.35] loss=2.49 avg=2.33\n",
      "[2601 | 3626.54] loss=2.47 avg=2.33\n",
      "[2602 | 3629.73] loss=2.09 avg=2.33\n",
      "[2603 | 3632.94] loss=2.43 avg=2.33\n",
      "[2604 | 3636.10] loss=2.58 avg=2.33\n",
      "[2605 | 3639.28] loss=2.72 avg=2.33\n",
      "[2606 | 3642.44] loss=2.37 avg=2.33\n",
      "[2607 | 3645.61] loss=2.20 avg=2.33\n",
      "[2608 | 3648.70] loss=2.59 avg=2.34\n",
      "[2609 | 3651.78] loss=2.66 avg=2.34\n",
      "[2610 | 3654.88] loss=2.11 avg=2.34\n",
      "[2611 | 3657.98] loss=1.88 avg=2.33\n",
      "[2612 | 3661.08] loss=2.81 avg=2.34\n",
      "[2613 | 3664.18] loss=2.07 avg=2.33\n",
      "[2614 | 3667.27] loss=2.40 avg=2.34\n",
      "[2615 | 3670.36] loss=2.93 avg=2.34\n",
      "[2616 | 3673.44] loss=2.28 avg=2.34\n",
      "[2617 | 3676.53] loss=1.84 avg=2.34\n",
      "[2618 | 3679.62] loss=2.30 avg=2.34\n",
      "[2619 | 3682.71] loss=2.39 avg=2.34\n",
      "[2620 | 3685.80] loss=2.34 avg=2.34\n",
      "[2621 | 3688.91] loss=2.70 avg=2.34\n",
      "[2622 | 3691.99] loss=2.71 avg=2.34\n",
      "[2623 | 3695.08] loss=1.99 avg=2.34\n",
      "[2624 | 3698.19] loss=2.28 avg=2.34\n",
      "[2625 | 3701.28] loss=2.55 avg=2.34\n",
      "[2626 | 3704.38] loss=2.42 avg=2.34\n",
      "[2627 | 3707.48] loss=1.98 avg=2.34\n",
      "[2628 | 3710.57] loss=1.96 avg=2.33\n",
      "[2629 | 3713.66] loss=2.42 avg=2.34\n",
      "[2630 | 3716.76] loss=2.38 avg=2.34\n",
      "[2631 | 3719.85] loss=2.41 avg=2.34\n",
      "[2632 | 3722.95] loss=2.49 avg=2.34\n",
      "[2633 | 3726.06] loss=2.52 avg=2.34\n",
      "[2634 | 3729.16] loss=2.16 avg=2.34\n",
      "[2635 | 3732.27] loss=2.28 avg=2.34\n",
      "[2636 | 3735.37] loss=2.76 avg=2.34\n",
      "[2637 | 3738.49] loss=2.37 avg=2.34\n",
      "[2638 | 3741.60] loss=2.38 avg=2.34\n",
      "[2639 | 3744.70] loss=2.33 avg=2.34\n",
      "[2640 | 3747.80] loss=1.77 avg=2.34\n",
      "[2641 | 3750.90] loss=2.27 avg=2.34\n",
      "[2642 | 3754.00] loss=2.30 avg=2.34\n",
      "[2643 | 3757.11] loss=1.66 avg=2.33\n",
      "[2644 | 3760.21] loss=2.10 avg=2.33\n",
      "[2645 | 3763.31] loss=2.21 avg=2.33\n",
      "[2646 | 3766.41] loss=2.16 avg=2.32\n",
      "[2647 | 3769.52] loss=2.58 avg=2.33\n",
      "[2648 | 3772.61] loss=2.16 avg=2.32\n",
      "[2649 | 3775.73] loss=2.36 avg=2.32\n",
      "[2650 | 3778.83] loss=2.16 avg=2.32\n",
      "[2651 | 3781.93] loss=2.77 avg=2.33\n",
      "[2652 | 3785.03] loss=2.06 avg=2.33\n",
      "[2653 | 3788.12] loss=2.31 avg=2.32\n",
      "[2654 | 3791.20] loss=2.37 avg=2.33\n",
      "[2655 | 3794.30] loss=2.11 avg=2.32\n",
      "[2656 | 3797.46] loss=2.18 avg=2.32\n",
      "[2657 | 3800.63] loss=2.31 avg=2.32\n",
      "[2658 | 3803.83] loss=2.37 avg=2.32\n",
      "[2659 | 3807.01] loss=2.20 avg=2.32\n",
      "[2660 | 3810.18] loss=2.32 avg=2.32\n",
      "[2661 | 3813.34] loss=2.11 avg=2.32\n",
      "[2662 | 3816.51] loss=2.16 avg=2.32\n",
      "[2663 | 3819.70] loss=2.39 avg=2.32\n",
      "[2664 | 3822.85] loss=2.23 avg=2.32\n",
      "[2665 | 3826.04] loss=2.23 avg=2.32\n",
      "[2666 | 3829.19] loss=2.48 avg=2.32\n",
      "[2667 | 3832.32] loss=2.03 avg=2.31\n",
      "[2668 | 3835.46] loss=2.38 avg=2.32\n",
      "[2669 | 3838.58] loss=2.77 avg=2.32\n",
      "[2670 | 3841.71] loss=2.03 avg=2.32\n",
      "[2671 | 3844.85] loss=2.37 avg=2.32\n",
      "[2672 | 3848.05] loss=2.22 avg=2.32\n",
      "[2673 | 3851.23] loss=2.30 avg=2.32\n",
      "[2674 | 3854.39] loss=2.08 avg=2.31\n",
      "[2675 | 3857.56] loss=2.00 avg=2.31\n",
      "[2676 | 3860.73] loss=2.11 avg=2.31\n",
      "[2677 | 3863.91] loss=2.04 avg=2.31\n",
      "[2678 | 3867.07] loss=1.94 avg=2.30\n",
      "[2679 | 3870.25] loss=2.32 avg=2.30\n",
      "[2680 | 3873.45] loss=2.83 avg=2.31\n",
      "[2681 | 3876.65] loss=2.47 avg=2.31\n",
      "[2682 | 3879.80] loss=2.49 avg=2.31\n",
      "[2683 | 3882.89] loss=2.03 avg=2.31\n",
      "[2684 | 3885.98] loss=2.47 avg=2.31\n",
      "[2685 | 3889.06] loss=2.47 avg=2.31\n",
      "[2686 | 3892.17] loss=2.07 avg=2.31\n",
      "[2687 | 3895.27] loss=2.34 avg=2.31\n",
      "[2688 | 3898.35] loss=2.37 avg=2.31\n",
      "[2689 | 3901.45] loss=2.51 avg=2.31\n",
      "[2690 | 3904.54] loss=2.40 avg=2.31\n",
      "[2691 | 3907.63] loss=1.99 avg=2.31\n",
      "[2692 | 3910.72] loss=2.38 avg=2.31\n",
      "[2693 | 3913.81] loss=2.26 avg=2.31\n",
      "[2694 | 3916.91] loss=2.47 avg=2.31\n",
      "[2695 | 3920.01] loss=2.45 avg=2.31\n",
      "[2696 | 3923.11] loss=2.10 avg=2.31\n",
      "[2697 | 3926.20] loss=2.09 avg=2.31\n",
      "[2698 | 3929.30] loss=2.13 avg=2.31\n",
      "[2699 | 3932.38] loss=2.11 avg=2.31\n",
      "[2700 | 3935.48] loss=2.20 avg=2.30\n",
      "[2701 | 3938.57] loss=2.32 avg=2.30\n",
      "[2702 | 3941.67] loss=2.38 avg=2.30\n",
      "[2703 | 3944.77] loss=2.58 avg=2.31\n",
      "[2704 | 3947.88] loss=2.17 avg=2.31\n",
      "[2705 | 3950.98] loss=2.33 avg=2.31\n",
      "[2706 | 3954.10] loss=2.65 avg=2.31\n",
      "[2707 | 3957.19] loss=2.69 avg=2.31\n",
      "[2708 | 3960.30] loss=2.67 avg=2.32\n",
      "[2709 | 3963.40] loss=2.61 avg=2.32\n",
      "[2710 | 3966.51] loss=2.33 avg=2.32\n",
      "[2711 | 3969.62] loss=2.81 avg=2.33\n",
      "[2712 | 3972.73] loss=2.07 avg=2.32\n",
      "[2713 | 3975.84] loss=1.95 avg=2.32\n",
      "[2714 | 3978.95] loss=2.70 avg=2.32\n",
      "[2715 | 3982.06] loss=2.42 avg=2.32\n",
      "[2716 | 3985.15] loss=2.45 avg=2.32\n",
      "[2717 | 3988.25] loss=2.41 avg=2.33\n",
      "[2718 | 3991.36] loss=2.35 avg=2.33\n",
      "[2719 | 3994.46] loss=2.56 avg=2.33\n",
      "[2720 | 3997.57] loss=2.04 avg=2.33\n",
      "[2721 | 4000.67] loss=2.24 avg=2.32\n",
      "[2722 | 4003.77] loss=2.00 avg=2.32\n",
      "[2723 | 4006.88] loss=2.39 avg=2.32\n",
      "[2724 | 4009.98] loss=2.70 avg=2.33\n",
      "[2725 | 4013.07] loss=2.13 avg=2.32\n",
      "[2726 | 4016.15] loss=2.26 avg=2.32\n",
      "[2727 | 4019.26] loss=2.14 avg=2.32\n",
      "[2728 | 4022.39] loss=2.37 avg=2.32\n",
      "[2729 | 4025.57] loss=2.42 avg=2.32\n",
      "[2730 | 4028.75] loss=2.31 avg=2.32\n",
      "[2731 | 4031.93] loss=2.18 avg=2.32\n",
      "[2732 | 4035.09] loss=2.18 avg=2.32\n",
      "[2733 | 4038.23] loss=2.48 avg=2.32\n",
      "[2734 | 4041.41] loss=2.42 avg=2.32\n",
      "[2735 | 4044.58] loss=1.98 avg=2.32\n",
      "[2736 | 4047.80] loss=2.13 avg=2.32\n",
      "[2737 | 4050.99] loss=2.20 avg=2.32\n",
      "[2738 | 4054.15] loss=1.75 avg=2.31\n",
      "[2739 | 4057.28] loss=2.52 avg=2.31\n",
      "[2740 | 4060.41] loss=2.21 avg=2.31\n",
      "[2741 | 4063.54] loss=2.34 avg=2.31\n",
      "[2742 | 4066.67] loss=2.04 avg=2.31\n",
      "[2743 | 4069.80] loss=2.08 avg=2.31\n",
      "[2744 | 4072.98] loss=2.98 avg=2.31\n",
      "[2745 | 4076.13] loss=2.29 avg=2.31\n",
      "[2746 | 4079.32] loss=1.77 avg=2.31\n",
      "[2747 | 4082.52] loss=2.00 avg=2.30\n",
      "[2748 | 4085.71] loss=2.35 avg=2.31\n",
      "[2749 | 4088.88] loss=2.25 avg=2.30\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "50) dylan3303_: What's a good program to run from command-line?\n",
      "(1150808900) dylan3303_: There's a python file where you run:\n",
      "(1150808900) juliet: i've been using it since the first time i used it\n",
      "\n",
      "\n",
      "(1206450400) bmckinlay: What is ubuntu's best-in-class desktop app for gnome?\n",
      "(1206450820) bmckinlay: What is ubuntu's best-in-class desktop app for gnome?\n",
      "(1206450820) bmckinlay: What is ubuntu's best-in-class desktop app for gnome?\n",
      "(1206450820) bmckinlay: What is ubuntu's best-in-class desktop app for gnome?\n",
      "(1206450880) mneptok: the gnome app store, if you want to make a new one\n",
      "(1206450900) mneptok: you can certainly get a lot of things for free though\n",
      "(1206841180) mneptok: i'm looking for something the system gives free but it seems to take a lot of thought to use all of it though\n",
      "(1206841240) mneptok: it's kind of like an Apple-style store -- you have to have some thought process going on, some knowledge of the desktop, and a little bit of luck\n",
      "(1206841240) mneptok: you could, but you do need a bit of experience with Linux too\n",
      "(1206841360) mneptok: yeah, so you could probably start somewhere with gtkpython, and use that\n",
      "\n",
      "\n",
      "(1304046200) m0rym3r: hi\n",
      "(1304046200) m0rym3r: i am new\n",
      "(1304046200) m0rym3r: is ubuntu the recommended distro for a 2nd screen\n",
      "(1304046320) m0rym3r: ok\n",
      "(1304046320) m0rym3r: can some one help me with a dual screen problem\n",
      "(1304046380) bazhang: try #ubuntu+1\n",
      "(1304046380) bazhang: did you mean to ask about beryl? if so, try #ubuntu+1 , or #ubuntu-offtopic\n",
      "(1313179720) bazhang: are you a noob? or are you comfortable using the GUI tools? try gedit, vi, vim and bash\n",
      "\n",
      "\n",
      "(1193468000) arrenlex: anyone know how to get my gnome-desktop installed?\n",
      "(1193468120) scunizi: try xserver-xorg\n",
      "(1193468300) scunizi: xserver-xorg is what you want - you probably want to try installing ubuntu first - you can try installing ubuntu, in the xserver-xorg packages you can find in the repositories, that you have downloaded, and start from that\n",
      "(1193468300) maverick: I can do that.\n",
      "(1193468300) maverick: it is the first of four windows, so I don't have to run an xserver app to start the gnome desktop.\n",
      "(1193468860) arrenlex: the problem is that when I do xserver it is a fresh ubuntu\n",
      "(1193468720) arrenlex: I was a bit impatient to wait for my sixtymx10\n",
      "(1193468720) scunizi: try this, sudo apt-get install xserver-xorg\n",
      "(1193468860) arrenlex: you see I have x server-xorg\n",
      "(1193468920) maverick: I was just waiting for gdm, so I can choose between two windows.\n",
      "\n",
      "\n",
      "(1326553400) nixpax9: how can u change /home/theadm/.wget.config\n",
      "(1326553460) nixpax9: in grub? it works fine with my other ubuntu ?\n",
      "(1326553520) lstarnes: I'm sure you could do that.. what problem did it work out to. I've never had a problem and it will do it for you, if you like.\n",
      "(1326553580) nixpax9: i do not see if grub does not work well with gnome and with compiz\n",
      "(1326553640) lstarnes: yes.. I always try to work with what I like.\n",
      "(1326553700) albertmacgregor: you have nothing to do with that.. you have grub installed? If\n",
      "\n",
      "[2750 | 4129.95] loss=2.04 avg=2.30\n",
      "[2751 | 4133.03] loss=2.79 avg=2.31\n",
      "[2752 | 4136.12] loss=2.22 avg=2.31\n",
      "[2753 | 4139.20] loss=2.03 avg=2.30\n",
      "[2754 | 4142.28] loss=2.20 avg=2.30\n",
      "[2755 | 4145.36] loss=2.36 avg=2.30\n",
      "[2756 | 4148.44] loss=2.25 avg=2.30\n",
      "[2757 | 4151.53] loss=2.24 avg=2.30\n",
      "[2758 | 4154.60] loss=2.20 avg=2.30\n",
      "[2759 | 4157.68] loss=2.60 avg=2.30\n",
      "[2760 | 4160.77] loss=2.33 avg=2.30\n",
      "[2761 | 4163.87] loss=2.25 avg=2.30\n",
      "[2762 | 4166.95] loss=2.21 avg=2.30\n",
      "[2763 | 4170.03] loss=2.08 avg=2.30\n",
      "[2764 | 4173.13] loss=2.23 avg=2.30\n",
      "[2765 | 4176.23] loss=2.36 avg=2.30\n",
      "[2766 | 4179.32] loss=2.11 avg=2.30\n",
      "[2767 | 4182.41] loss=2.42 avg=2.30\n",
      "[2768 | 4185.51] loss=2.61 avg=2.30\n",
      "[2769 | 4188.62] loss=1.99 avg=2.30\n",
      "[2770 | 4191.72] loss=2.27 avg=2.30\n",
      "[2771 | 4194.81] loss=2.39 avg=2.30\n",
      "[2772 | 4197.92] loss=2.19 avg=2.30\n",
      "[2773 | 4201.03] loss=2.35 avg=2.30\n",
      "[2774 | 4204.13] loss=2.32 avg=2.30\n",
      "[2775 | 4207.25] loss=1.80 avg=2.29\n",
      "[2776 | 4210.35] loss=2.27 avg=2.29\n",
      "[2777 | 4213.46] loss=2.58 avg=2.30\n",
      "[2778 | 4216.57] loss=2.69 avg=2.30\n",
      "[2779 | 4219.68] loss=2.29 avg=2.30\n",
      "[2780 | 4222.78] loss=2.28 avg=2.30\n",
      "[2781 | 4225.88] loss=2.24 avg=2.30\n",
      "[2782 | 4228.98] loss=2.71 avg=2.30\n",
      "[2783 | 4232.09] loss=2.47 avg=2.31\n",
      "[2784 | 4235.19] loss=2.79 avg=2.31\n",
      "[2785 | 4238.31] loss=2.55 avg=2.31\n",
      "[2786 | 4241.43] loss=2.07 avg=2.31\n",
      "[2787 | 4244.53] loss=2.29 avg=2.31\n",
      "[2788 | 4247.64] loss=2.57 avg=2.31\n",
      "[2789 | 4250.74] loss=2.13 avg=2.31\n",
      "[2790 | 4253.83] loss=2.34 avg=2.31\n",
      "[2791 | 4256.92] loss=2.38 avg=2.31\n",
      "[2792 | 4260.02] loss=2.12 avg=2.31\n",
      "[2793 | 4263.19] loss=2.16 avg=2.31\n",
      "[2794 | 4266.41] loss=2.07 avg=2.31\n",
      "[2795 | 4269.63] loss=2.38 avg=2.31\n",
      "[2796 | 4272.84] loss=1.98 avg=2.30\n",
      "[2797 | 4275.96] loss=2.02 avg=2.30\n",
      "[2798 | 4279.18] loss=1.54 avg=2.29\n",
      "[2799 | 4282.39] loss=2.33 avg=2.29\n",
      "[2800 | 4285.57] loss=2.26 avg=2.29\n",
      "[2801 | 4288.79] loss=2.70 avg=2.30\n",
      "[2802 | 4291.92] loss=2.77 avg=2.30\n",
      "[2803 | 4295.10] loss=2.75 avg=2.31\n",
      "[2804 | 4298.23] loss=2.27 avg=2.31\n",
      "[2805 | 4301.34] loss=2.48 avg=2.31\n",
      "[2806 | 4304.46] loss=2.70 avg=2.31\n",
      "[2807 | 4307.60] loss=2.21 avg=2.31\n",
      "[2808 | 4310.83] loss=2.39 avg=2.31\n",
      "[2809 | 4314.03] loss=2.11 avg=2.31\n",
      "[2810 | 4317.21] loss=2.09 avg=2.31\n",
      "[2811 | 4320.42] loss=2.62 avg=2.31\n",
      "[2812 | 4323.64] loss=1.98 avg=2.31\n",
      "[2813 | 4326.87] loss=2.05 avg=2.30\n",
      "[2814 | 4330.11] loss=2.55 avg=2.31\n",
      "[2815 | 4333.26] loss=2.10 avg=2.30\n",
      "[2816 | 4336.44] loss=1.97 avg=2.30\n",
      "[2817 | 4339.65] loss=2.24 avg=2.30\n",
      "[2818 | 4342.75] loss=2.21 avg=2.30\n",
      "[2819 | 4345.86] loss=2.52 avg=2.30\n",
      "[2820 | 4348.95] loss=2.70 avg=2.31\n",
      "[2821 | 4352.05] loss=2.28 avg=2.31\n",
      "[2822 | 4355.14] loss=2.81 avg=2.31\n",
      "[2823 | 4358.24] loss=2.60 avg=2.31\n",
      "[2824 | 4361.34] loss=2.47 avg=2.32\n",
      "[2825 | 4364.43] loss=2.52 avg=2.32\n",
      "[2826 | 4367.52] loss=2.55 avg=2.32\n",
      "[2827 | 4370.61] loss=2.15 avg=2.32\n",
      "[2828 | 4373.71] loss=2.38 avg=2.32\n",
      "[2829 | 4376.82] loss=2.12 avg=2.32\n",
      "[2830 | 4379.92] loss=2.72 avg=2.32\n",
      "[2831 | 4383.02] loss=2.51 avg=2.32\n",
      "[2832 | 4386.10] loss=2.11 avg=2.32\n",
      "[2833 | 4389.19] loss=2.34 avg=2.32\n",
      "[2834 | 4392.27] loss=2.67 avg=2.32\n",
      "[2835 | 4395.36] loss=2.47 avg=2.33\n",
      "[2836 | 4398.46] loss=1.57 avg=2.32\n",
      "[2837 | 4401.56] loss=2.32 avg=2.32\n",
      "[2838 | 4404.64] loss=2.42 avg=2.32\n",
      "[2839 | 4407.71] loss=2.31 avg=2.32\n",
      "[2840 | 4410.82] loss=2.07 avg=2.32\n",
      "[2841 | 4413.92] loss=2.37 avg=2.32\n",
      "[2842 | 4417.02] loss=2.86 avg=2.32\n",
      "[2843 | 4420.12] loss=2.28 avg=2.32\n",
      "[2844 | 4423.22] loss=2.94 avg=2.33\n",
      "[2845 | 4426.32] loss=2.09 avg=2.33\n",
      "[2846 | 4429.41] loss=2.22 avg=2.33\n",
      "[2847 | 4432.53] loss=2.27 avg=2.32\n",
      "[2848 | 4435.64] loss=2.44 avg=2.33\n",
      "[2849 | 4438.73] loss=1.87 avg=2.32\n",
      "[2850 | 4441.83] loss=2.51 avg=2.32\n",
      "[2851 | 4444.95] loss=2.74 avg=2.33\n",
      "[2852 | 4448.05] loss=2.18 avg=2.33\n",
      "[2853 | 4451.14] loss=2.40 avg=2.33\n",
      "[2854 | 4454.24] loss=2.73 avg=2.33\n",
      "[2855 | 4457.35] loss=2.29 avg=2.33\n",
      "[2856 | 4460.47] loss=2.77 avg=2.33\n",
      "[2857 | 4463.57] loss=2.24 avg=2.33\n",
      "[2858 | 4466.69] loss=2.90 avg=2.34\n",
      "[2859 | 4469.80] loss=2.05 avg=2.34\n",
      "[2860 | 4472.92] loss=2.70 avg=2.34\n",
      "[2861 | 4476.00] loss=2.33 avg=2.34\n",
      "[2862 | 4479.09] loss=2.15 avg=2.34\n",
      "[2863 | 4482.18] loss=2.47 avg=2.34\n",
      "[2864 | 4485.29] loss=2.79 avg=2.34\n",
      "[2865 | 4488.43] loss=2.32 avg=2.34\n",
      "[2866 | 4491.61] loss=1.98 avg=2.34\n",
      "[2867 | 4494.78] loss=2.41 avg=2.34\n",
      "[2868 | 4497.95] loss=2.13 avg=2.34\n",
      "[2869 | 4501.11] loss=2.37 avg=2.34\n",
      "[2870 | 4504.27] loss=2.06 avg=2.34\n",
      "[2871 | 4507.45] loss=1.97 avg=2.33\n",
      "[2872 | 4510.57] loss=2.15 avg=2.33\n",
      "[2873 | 4513.75] loss=2.44 avg=2.33\n",
      "[2874 | 4516.90] loss=2.34 avg=2.33\n",
      "[2875 | 4520.05] loss=2.39 avg=2.33\n",
      "[2876 | 4523.18] loss=2.97 avg=2.34\n",
      "[2877 | 4526.32] loss=2.23 avg=2.34\n",
      "[2878 | 4529.45] loss=2.35 avg=2.34\n",
      "[2879 | 4532.62] loss=2.45 avg=2.34\n",
      "[2880 | 4535.80] loss=2.18 avg=2.34\n",
      "[2881 | 4538.96] loss=2.36 avg=2.34\n",
      "[2882 | 4542.14] loss=2.55 avg=2.34\n",
      "[2883 | 4545.33] loss=2.22 avg=2.34\n",
      "[2884 | 4548.49] loss=2.19 avg=2.34\n",
      "[2885 | 4551.66] loss=2.08 avg=2.33\n",
      "[2886 | 4554.83] loss=2.88 avg=2.34\n",
      "[2887 | 4558.00] loss=2.43 avg=2.34\n",
      "[2888 | 4561.17] loss=2.08 avg=2.34\n",
      "[2889 | 4564.33] loss=2.04 avg=2.34\n",
      "[2890 | 4567.44] loss=2.28 avg=2.33\n",
      "[2891 | 4570.53] loss=2.57 avg=2.34\n",
      "[2892 | 4573.61] loss=2.23 avg=2.34\n",
      "[2893 | 4576.71] loss=2.88 avg=2.34\n",
      "[2894 | 4579.81] loss=2.02 avg=2.34\n",
      "[2895 | 4582.91] loss=1.97 avg=2.33\n",
      "[2896 | 4586.01] loss=2.12 avg=2.33\n",
      "[2897 | 4589.10] loss=2.15 avg=2.33\n",
      "[2898 | 4592.19] loss=2.35 avg=2.33\n",
      "[2899 | 4595.29] loss=2.32 avg=2.33\n",
      "[2900 | 4598.39] loss=2.08 avg=2.33\n",
      "[2901 | 4601.49] loss=2.41 avg=2.33\n",
      "[2902 | 4604.59] loss=2.62 avg=2.33\n",
      "[2903 | 4607.68] loss=2.28 avg=2.33\n",
      "[2904 | 4610.78] loss=2.50 avg=2.33\n",
      "[2905 | 4613.88] loss=2.44 avg=2.33\n",
      "[2906 | 4616.97] loss=2.50 avg=2.34\n",
      "[2907 | 4620.07] loss=2.21 avg=2.33\n",
      "[2908 | 4623.16] loss=2.78 avg=2.34\n",
      "[2909 | 4626.25] loss=2.04 avg=2.34\n",
      "[2910 | 4629.34] loss=1.99 avg=2.33\n",
      "[2911 | 4632.43] loss=2.26 avg=2.33\n",
      "[2912 | 4635.54] loss=2.06 avg=2.33\n",
      "[2913 | 4638.65] loss=2.39 avg=2.33\n",
      "[2914 | 4641.76] loss=2.18 avg=2.33\n",
      "[2915 | 4644.86] loss=2.44 avg=2.33\n",
      "[2916 | 4647.97] loss=2.33 avg=2.33\n",
      "[2917 | 4651.06] loss=1.95 avg=2.33\n",
      "[2918 | 4654.18] loss=2.34 avg=2.33\n",
      "[2919 | 4657.27] loss=2.20 avg=2.32\n",
      "[2920 | 4660.37] loss=1.84 avg=2.32\n",
      "[2921 | 4663.47] loss=2.45 avg=2.32\n",
      "[2922 | 4666.56] loss=2.29 avg=2.32\n",
      "[2923 | 4669.66] loss=2.61 avg=2.32\n",
      "[2924 | 4672.76] loss=2.32 avg=2.32\n",
      "[2925 | 4675.87] loss=2.17 avg=2.32\n",
      "[2926 | 4678.97] loss=2.33 avg=2.32\n",
      "[2927 | 4682.07] loss=2.85 avg=2.33\n",
      "[2928 | 4685.18] loss=2.63 avg=2.33\n",
      "[2929 | 4688.30] loss=2.73 avg=2.33\n",
      "[2930 | 4691.40] loss=2.10 avg=2.33\n",
      "[2931 | 4694.50] loss=2.45 avg=2.33\n",
      "[2932 | 4697.61] loss=2.65 avg=2.34\n",
      "[2933 | 4700.71] loss=2.34 avg=2.34\n",
      "[2934 | 4703.81] loss=2.55 avg=2.34\n",
      "[2935 | 4706.91] loss=1.94 avg=2.33\n",
      "[2936 | 4709.99] loss=2.10 avg=2.33\n",
      "[2937 | 4713.07] loss=2.33 avg=2.33\n",
      "[2938 | 4716.22] loss=2.30 avg=2.33\n",
      "[2939 | 4719.38] loss=2.40 avg=2.33\n",
      "[2940 | 4722.55] loss=2.67 avg=2.34\n",
      "[2941 | 4725.69] loss=2.16 avg=2.33\n",
      "[2942 | 4728.87] loss=2.56 avg=2.34\n",
      "[2943 | 4732.04] loss=1.91 avg=2.33\n",
      "[2944 | 4735.20] loss=2.02 avg=2.33\n",
      "[2945 | 4738.39] loss=2.37 avg=2.33\n",
      "[2946 | 4741.56] loss=2.18 avg=2.33\n",
      "[2947 | 4744.73] loss=2.21 avg=2.33\n",
      "[2948 | 4747.88] loss=2.44 avg=2.33\n",
      "[2949 | 4751.09] loss=2.18 avg=2.33\n",
      "[2950 | 4754.27] loss=2.28 avg=2.33\n",
      "[2951 | 4757.44] loss=2.58 avg=2.33\n",
      "[2952 | 4760.59] loss=2.42 avg=2.33\n",
      "[2953 | 4763.71] loss=2.27 avg=2.33\n",
      "[2954 | 4766.85] loss=1.89 avg=2.32\n",
      "[2955 | 4769.96] loss=2.57 avg=2.33\n",
      "[2956 | 4773.09] loss=2.54 avg=2.33\n",
      "[2957 | 4776.22] loss=2.18 avg=2.33\n",
      "[2958 | 4779.35] loss=2.23 avg=2.33\n",
      "[2959 | 4782.48] loss=2.13 avg=2.32\n",
      "[2960 | 4785.65] loss=2.35 avg=2.32\n",
      "[2961 | 4788.86] loss=1.98 avg=2.32\n",
      "[2962 | 4792.03] loss=2.44 avg=2.32\n",
      "[2963 | 4795.21] loss=3.00 avg=2.33\n",
      "[2964 | 4798.42] loss=2.26 avg=2.33\n",
      "[2965 | 4801.58] loss=2.06 avg=2.33\n",
      "[2966 | 4804.75] loss=2.13 avg=2.32\n",
      "[2967 | 4807.93] loss=2.15 avg=2.32\n",
      "[2968 | 4811.13] loss=2.19 avg=2.32\n",
      "[2969 | 4814.30] loss=2.38 avg=2.32\n",
      "[2970 | 4817.47] loss=2.98 avg=2.33\n",
      "[2971 | 4820.63] loss=2.42 avg=2.33\n",
      "[2972 | 4823.83] loss=2.24 avg=2.33\n",
      "[2973 | 4827.01] loss=1.93 avg=2.32\n",
      "[2974 | 4830.18] loss=2.37 avg=2.32\n",
      "[2975 | 4833.32] loss=2.56 avg=2.33\n",
      "[2976 | 4836.41] loss=2.59 avg=2.33\n",
      "[2977 | 4839.50] loss=2.54 avg=2.33\n",
      "[2978 | 4842.60] loss=2.14 avg=2.33\n",
      "[2979 | 4845.70] loss=2.35 avg=2.33\n",
      "[2980 | 4848.80] loss=2.03 avg=2.33\n",
      "[2981 | 4851.90] loss=1.98 avg=2.32\n",
      "[2982 | 4855.00] loss=2.24 avg=2.32\n",
      "[2983 | 4858.09] loss=2.08 avg=2.32\n",
      "[2984 | 4861.20] loss=2.09 avg=2.32\n",
      "[2985 | 4864.29] loss=2.28 avg=2.32\n",
      "[2986 | 4867.39] loss=2.35 avg=2.32\n",
      "[2987 | 4870.49] loss=1.99 avg=2.31\n",
      "[2988 | 4873.59] loss=2.29 avg=2.31\n",
      "[2989 | 4876.68] loss=2.47 avg=2.32\n",
      "[2990 | 4879.78] loss=2.14 avg=2.31\n",
      "[2991 | 4882.88] loss=2.80 avg=2.32\n",
      "[2992 | 4885.97] loss=2.35 avg=2.32\n",
      "[2993 | 4889.06] loss=2.07 avg=2.32\n",
      "[2994 | 4892.17] loss=2.65 avg=2.32\n",
      "[2995 | 4895.26] loss=2.63 avg=2.32\n",
      "[2996 | 4898.37] loss=2.43 avg=2.32\n",
      "[2997 | 4901.46] loss=2.27 avg=2.32\n",
      "[2998 | 4904.56] loss=2.29 avg=2.32\n",
      "[2999 | 4907.66] loss=2.82 avg=2.33\n",
      "Saving checkpoint/run1/model-3000\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      " following you should be able to start it, it's not required for us though\n",
      "(1154068600) thefool: can't just get a window manager, i have to start it by hand..\n",
      "(1154069660) mfk: if you say that it's just a command to start gnome but 'man kill' says 'man kill xorg-core'\n",
      "(1154069120) thefool: that's just what it is, but does 'man kill mplayer' do it?\n",
      "(1154069120) thefool: but that does not tell me how to start xorg (the only way to do that is to use the 'man' command without an 'xorg.conf' thing in it.. the only way I know of how to start it is to say 'man kill window'\n",
      "(1154069620) mfk:   what do you mean with 'man xorg'.conf'? what command does 'man' do?\n",
      "(1154069620) thefool: man xorg\n",
      "(1154070200) thefool: I'm sorry. I'm trying to figure out where the xorg-core command is located\n",
      "\n",
      "\n",
      "(1247361560) jim_p: I like the gdm look I tried it for the first time in awhile\n",
      "(1247361620) jim_p: I'm pretty sure I found the bug, that ubuntu never updated the live cd with the latest packages\n",
      "(1247361740) jim_p: How would I do that?\n",
      "(1247362500) jim_p: I've been looking over my bios lately, and haven't been getting any results\n",
      "(1247362500) jim_p: The bios has a 'clean' look, but I don't have any real results\n",
      "(1247362560) jim_p: I'm running it on my computer right now with the latest version of Ubuntu... I don't know what I've found yet, so I'm being cautious\n",
      "(1247362560) jim_p: But when I look at a laptop right now, I don't see any results\n",
      "(1247362560) _jason:  i'd be a little scared :)\n",
      "(1247362560) _jason:  is the laptop running ubuntu? do the settings come back?\n",
      "(1247362560) _jason: : what are you doing now?\n",
      "(1247362560) _jason:  is your computer here?\n",
      "(1247362560) _jason:  what's it like now?\n",
      "\n",
      "\n",
      "(1305729100) Xenguy: You know if you just log out your computer and then into the computer then run the bootloader and it boots right out from that point\n",
      "(1305729220) Xenguy: Thats the only thing you are using it for... it is not an application or service for example\n",
      "(1305729280) Xenguy: If that boots up then the problem is not the hardware drivers, the problem is the kernel\n",
      "(1305729280) Xenguy: Thats where you're messing up\n",
      "(1305729280) Xenguy: Or you may have to modify the kernel to do anything with your hardware\n",
      "(1305729280) Xenguy: Not an application it is a service\n",
      "(1305729280) Xenguy: You are using kubuntu 10.04 ?\n",
      "(1305729340) Xenguy: You dont need to manually edit the kernel to change it to your specifications or for the bootloader.\n",
      "(1305631800) Xenguy: Why you should start the console ?\n",
      "(1305631960) Xenguy: How can you boot boot up kubuntu if you have no idea how to use the bootloader?\n",
      "(1305631960) Xenguy: No idea?\n",
      "(1305632200) Xenguy: I think we should go directly to the console and say 'KDE - startx'\n",
      "(1305632260) Xenguy: I am getting a few things wrong here...\n",
      "(1305632260) Xenguy: I thought there was only two\n",
      "(1305632260) Xenguy: You don't need to do that then\n",
      "(1305632740) Xenguy: And in case you forgot you dont need to start the console and start in here, there is only one console, the only line I know of is 'startx startx xorg=0x7fffffffffff0'\n",
      "(1305632740) Xenguy: It is ok I know from using a console that I did not change the kernel\n",
      "(1305632740) Xenguy: And it does not work ?\n",
      "\n",
      "[3000 | 4955.49] loss=2.29 avg=2.33\n",
      "[3001 | 4958.57] loss=2.12 avg=2.33\n",
      "[3002 | 4961.66] loss=2.80 avg=2.33\n",
      "[3003 | 4964.76] loss=2.39 avg=2.33\n",
      "[3004 | 4967.84] loss=2.65 avg=2.33\n",
      "[3005 | 4970.92] loss=2.06 avg=2.33\n",
      "[3006 | 4974.01] loss=2.68 avg=2.34\n",
      "[3007 | 4977.10] loss=2.62 avg=2.34\n",
      "[3008 | 4980.17] loss=2.22 avg=2.34\n",
      "[3009 | 4983.27] loss=2.23 avg=2.34\n",
      "[3010 | 4986.38] loss=2.06 avg=2.33\n",
      "[3011 | 4989.49] loss=2.08 avg=2.33\n",
      "[3012 | 4992.63] loss=1.83 avg=2.33\n",
      "[3013 | 4995.80] loss=2.43 avg=2.33\n",
      "[3014 | 4998.97] loss=2.31 avg=2.33\n",
      "[3015 | 5002.13] loss=2.21 avg=2.33\n",
      "[3016 | 5005.27] loss=2.14 avg=2.32\n",
      "[3017 | 5008.43] loss=2.68 avg=2.33\n",
      "[3018 | 5011.58] loss=2.29 avg=2.33\n",
      "[3019 | 5014.73] loss=2.56 avg=2.33\n",
      "[3020 | 5017.90] loss=2.03 avg=2.33\n",
      "[3021 | 5021.05] loss=2.78 avg=2.33\n",
      "[3022 | 5024.26] loss=2.46 avg=2.33\n",
      "[3023 | 5027.44] loss=2.49 avg=2.33\n",
      "[3024 | 5030.63] loss=2.25 avg=2.33\n",
      "[3025 | 5033.80] loss=2.15 avg=2.33\n",
      "[3026 | 5036.99] loss=2.13 avg=2.33\n",
      "[3027 | 5040.16] loss=2.28 avg=2.33\n",
      "[3028 | 5043.29] loss=2.34 avg=2.33\n",
      "[3029 | 5046.44] loss=2.47 avg=2.33\n",
      "[3030 | 5049.59] loss=2.23 avg=2.33\n",
      "[3031 | 5052.72] loss=2.04 avg=2.33\n",
      "[3032 | 5055.85] loss=2.32 avg=2.33\n",
      "[3033 | 5058.98] loss=2.08 avg=2.32\n",
      "[3034 | 5062.11] loss=1.68 avg=2.32\n",
      "[3035 | 5065.24] loss=1.90 avg=2.31\n",
      "[3036 | 5068.35] loss=2.06 avg=2.31\n",
      "[3037 | 5071.48] loss=2.29 avg=2.31\n",
      "[3038 | 5074.59] loss=1.98 avg=2.31\n",
      "[3039 | 5077.72] loss=2.44 avg=2.31\n",
      "[3040 | 5080.90] loss=2.12 avg=2.31\n",
      "[3041 | 5084.10] loss=2.02 avg=2.30\n",
      "[3042 | 5087.26] loss=2.33 avg=2.30\n",
      "[3043 | 5090.43] loss=2.17 avg=2.30\n",
      "[3044 | 5093.62] loss=2.08 avg=2.30\n",
      "[3045 | 5096.81] loss=1.20 avg=2.29\n",
      "[3046 | 5100.00] loss=2.28 avg=2.29\n",
      "[3047 | 5103.21] loss=2.32 avg=2.29\n",
      "[3048 | 5106.39] loss=2.08 avg=2.29\n",
      "[3049 | 5109.57] loss=2.01 avg=2.28\n",
      "[3050 | 5112.74] loss=2.26 avg=2.28\n",
      "[3051 | 5115.90] loss=2.06 avg=2.28\n",
      "[3052 | 5119.10] loss=2.22 avg=2.28\n",
      "[3053 | 5122.29] loss=2.76 avg=2.29\n",
      "[3054 | 5125.44] loss=2.26 avg=2.29\n",
      "[3055 | 5128.63] loss=2.32 avg=2.29\n",
      "[3056 | 5131.81] loss=2.33 avg=2.29\n",
      "[3057 | 5134.98] loss=2.68 avg=2.29\n",
      "[3058 | 5138.14] loss=2.44 avg=2.29\n",
      "[3059 | 5141.26] loss=2.73 avg=2.30\n",
      "[3060 | 5144.35] loss=2.47 avg=2.30\n",
      "[3061 | 5147.43] loss=2.64 avg=2.30\n",
      "[3062 | 5150.52] loss=2.28 avg=2.30\n",
      "[3063 | 5153.61] loss=2.49 avg=2.30\n",
      "[3064 | 5156.72] loss=2.23 avg=2.30\n",
      "[3065 | 5159.80] loss=2.61 avg=2.31\n",
      "[3066 | 5162.89] loss=1.94 avg=2.30\n",
      "[3067 | 5165.99] loss=2.38 avg=2.30\n",
      "[3068 | 5169.08] loss=2.33 avg=2.30\n",
      "[3069 | 5172.19] loss=1.94 avg=2.30\n",
      "[3070 | 5175.29] loss=2.30 avg=2.30\n",
      "[3071 | 5178.38] loss=2.24 avg=2.30\n",
      "[3072 | 5181.47] loss=2.25 avg=2.30\n",
      "[3073 | 5184.55] loss=2.59 avg=2.30\n",
      "[3074 | 5187.66] loss=2.41 avg=2.30\n",
      "[3075 | 5190.76] loss=2.62 avg=2.31\n",
      "[3076 | 5193.85] loss=2.61 avg=2.31\n",
      "[3077 | 5196.94] loss=2.80 avg=2.31\n",
      "[3078 | 5200.04] loss=2.08 avg=2.31\n",
      "[3079 | 5203.14] loss=2.12 avg=2.31\n",
      "[3080 | 5206.23] loss=2.20 avg=2.31\n",
      "[3081 | 5209.33] loss=2.99 avg=2.32\n",
      "[3082 | 5212.44] loss=2.23 avg=2.31\n",
      "[3083 | 5215.54] loss=2.19 avg=2.31\n",
      "[3084 | 5218.64] loss=2.49 avg=2.31\n",
      "[3085 | 5221.74] loss=2.37 avg=2.32\n",
      "[3086 | 5224.84] loss=2.62 avg=2.32\n",
      "[3087 | 5227.95] loss=2.47 avg=2.32\n",
      "[3088 | 5231.05] loss=2.46 avg=2.32\n",
      "[3089 | 5234.16] loss=2.11 avg=2.32\n",
      "[3090 | 5237.27] loss=1.94 avg=2.32\n",
      "[3091 | 5240.37] loss=2.13 avg=2.31\n",
      "[3092 | 5243.48] loss=2.84 avg=2.32\n",
      "[3093 | 5246.59] loss=2.15 avg=2.32\n",
      "[3094 | 5249.69] loss=2.78 avg=2.32\n",
      "[3095 | 5252.80] loss=2.18 avg=2.32\n",
      "[3096 | 5255.91] loss=2.42 avg=2.32\n",
      "[3097 | 5259.02] loss=2.74 avg=2.33\n",
      "[3098 | 5262.12] loss=2.48 avg=2.33\n",
      "[3099 | 5265.24] loss=2.64 avg=2.33\n",
      "[3100 | 5268.35] loss=2.67 avg=2.33\n",
      "[3101 | 5271.45] loss=1.96 avg=2.33\n",
      "[3102 | 5274.56] loss=2.27 avg=2.33\n",
      "[3103 | 5277.65] loss=2.09 avg=2.33\n",
      "[3104 | 5280.76] loss=2.49 avg=2.33\n",
      "[3105 | 5283.86] loss=2.90 avg=2.33\n",
      "[3106 | 5287.00] loss=2.24 avg=2.33\n",
      "[3107 | 5290.18] loss=2.93 avg=2.34\n",
      "[3108 | 5293.34] loss=2.14 avg=2.34\n",
      "[3109 | 5296.52] loss=2.19 avg=2.34\n",
      "[3110 | 5299.69] loss=2.34 avg=2.34\n",
      "[3111 | 5302.86] loss=2.24 avg=2.33\n",
      "[3112 | 5306.04] loss=2.42 avg=2.34\n",
      "[3113 | 5309.19] loss=2.07 avg=2.33\n",
      "[3114 | 5312.38] loss=2.47 avg=2.33\n",
      "[3115 | 5315.53] loss=2.44 avg=2.34\n",
      "[3116 | 5318.66] loss=2.23 avg=2.33\n",
      "[3117 | 5321.79] loss=2.70 avg=2.34\n",
      "[3118 | 5324.93] loss=2.35 avg=2.34\n",
      "[3119 | 5328.06] loss=2.07 avg=2.34\n",
      "[3120 | 5331.21] loss=2.38 avg=2.34\n",
      "[3121 | 5334.40] loss=2.65 avg=2.34\n",
      "[3122 | 5337.58] loss=2.09 avg=2.34\n",
      "[3123 | 5340.76] loss=2.20 avg=2.34\n",
      "[3124 | 5343.96] loss=2.42 avg=2.34\n",
      "[3125 | 5347.16] loss=1.99 avg=2.33\n",
      "[3126 | 5350.34] loss=2.09 avg=2.33\n",
      "[3127 | 5353.53] loss=2.21 avg=2.33\n",
      "[3128 | 5356.73] loss=2.38 avg=2.33\n",
      "[3129 | 5359.90] loss=1.88 avg=2.33\n",
      "[3130 | 5362.99] loss=1.96 avg=2.32\n",
      "[3131 | 5366.07] loss=2.28 avg=2.32\n",
      "[3132 | 5369.16] loss=2.40 avg=2.32\n",
      "[3133 | 5372.26] loss=2.02 avg=2.32\n",
      "[3134 | 5375.36] loss=2.19 avg=2.32\n",
      "[3135 | 5378.47] loss=2.04 avg=2.31\n",
      "[3136 | 5381.58] loss=2.41 avg=2.32\n",
      "[3137 | 5384.67] loss=2.56 avg=2.32\n",
      "[3138 | 5387.77] loss=2.26 avg=2.32\n",
      "[3139 | 5390.86] loss=2.87 avg=2.32\n",
      "[3140 | 5393.96] loss=2.21 avg=2.32\n",
      "[3141 | 5397.06] loss=2.22 avg=2.32\n",
      "[3142 | 5400.16] loss=2.51 avg=2.32\n",
      "[3143 | 5403.26] loss=2.73 avg=2.33\n",
      "[3144 | 5406.33] loss=2.19 avg=2.33\n",
      "[3145 | 5409.42] loss=2.52 avg=2.33\n",
      "[3146 | 5412.51] loss=1.97 avg=2.32\n",
      "[3147 | 5415.60] loss=2.42 avg=2.32\n",
      "[3148 | 5418.70] loss=1.77 avg=2.32\n",
      "[3149 | 5421.79] loss=2.35 avg=2.32\n",
      "[3150 | 5424.88] loss=2.30 avg=2.32\n",
      "[3151 | 5427.99] loss=1.78 avg=2.31\n",
      "[3152 | 5431.09] loss=2.33 avg=2.31\n",
      "[3153 | 5434.20] loss=2.00 avg=2.31\n",
      "[3154 | 5437.31] loss=2.51 avg=2.31\n",
      "[3155 | 5440.42] loss=2.70 avg=2.32\n",
      "[3156 | 5443.51] loss=2.29 avg=2.32\n",
      "[3157 | 5446.61] loss=1.81 avg=2.31\n",
      "[3158 | 5449.71] loss=2.05 avg=2.31\n",
      "[3159 | 5452.82] loss=2.10 avg=2.31\n",
      "[3160 | 5455.93] loss=2.14 avg=2.31\n",
      "[3161 | 5459.05] loss=2.51 avg=2.31\n",
      "[3162 | 5462.16] loss=2.11 avg=2.31\n",
      "[3163 | 5465.25] loss=2.62 avg=2.31\n",
      "[3164 | 5468.37] loss=2.21 avg=2.31\n",
      "[3165 | 5471.47] loss=2.70 avg=2.31\n",
      "[3166 | 5474.57] loss=2.49 avg=2.31\n",
      "[3167 | 5477.68] loss=2.22 avg=2.31\n",
      "[3168 | 5480.79] loss=2.26 avg=2.31\n",
      "[3169 | 5483.91] loss=2.13 avg=2.31\n",
      "[3170 | 5487.00] loss=2.40 avg=2.31\n",
      "[3171 | 5490.10] loss=2.14 avg=2.31\n",
      "[3172 | 5493.20] loss=1.96 avg=2.31\n",
      "[3173 | 5496.30] loss=2.44 avg=2.31\n",
      "[3174 | 5499.39] loss=2.02 avg=2.30\n",
      "[3175 | 5502.49] loss=2.09 avg=2.30\n",
      "[3176 | 5505.61] loss=2.33 avg=2.30\n",
      "[3177 | 5508.79] loss=2.37 avg=2.30\n",
      "[3178 | 5511.93] loss=2.40 avg=2.30\n",
      "[3179 | 5515.10] loss=2.38 avg=2.30\n",
      "[3180 | 5518.27] loss=1.77 avg=2.30\n",
      "[3181 | 5521.44] loss=2.13 avg=2.30\n",
      "[3182 | 5524.62] loss=2.31 avg=2.30\n",
      "[3183 | 5527.79] loss=1.76 avg=2.29\n",
      "[3184 | 5530.95] loss=2.06 avg=2.29\n",
      "[3185 | 5534.15] loss=2.00 avg=2.29\n",
      "[3186 | 5537.28] loss=2.05 avg=2.28\n",
      "[3187 | 5540.47] loss=2.17 avg=2.28\n",
      "[3188 | 5543.65] loss=2.65 avg=2.29\n",
      "[3189 | 5546.83] loss=2.06 avg=2.28\n",
      "[3190 | 5550.00] loss=2.10 avg=2.28\n",
      "[3191 | 5553.18] loss=2.12 avg=2.28\n",
      "[3192 | 5556.36] loss=2.12 avg=2.28\n",
      "[3193 | 5559.55] loss=2.71 avg=2.28\n",
      "[3194 | 5562.71] loss=2.32 avg=2.28\n",
      "[3195 | 5565.84] loss=2.36 avg=2.29\n",
      "[3196 | 5568.97] loss=2.28 avg=2.29\n",
      "[3197 | 5572.10] loss=2.38 avg=2.29\n",
      "[3198 | 5575.22] loss=2.39 avg=2.29\n",
      "[3199 | 5578.35] loss=2.33 avg=2.29\n",
      "[3200 | 5581.48] loss=2.27 avg=2.29\n",
      "[3201 | 5584.60] loss=2.61 avg=2.29\n",
      "[3202 | 5587.76] loss=1.76 avg=2.29\n",
      "[3203 | 5590.92] loss=2.41 avg=2.29\n",
      "[3204 | 5594.08] loss=1.90 avg=2.28\n",
      "[3205 | 5597.25] loss=2.88 avg=2.29\n",
      "[3206 | 5600.44] loss=2.53 avg=2.29\n",
      "[3207 | 5603.64] loss=2.62 avg=2.29\n",
      "[3208 | 5606.83] loss=2.15 avg=2.29\n",
      "[3209 | 5610.02] loss=2.32 avg=2.29\n",
      "[3210 | 5613.20] loss=2.10 avg=2.29\n",
      "[3211 | 5616.39] loss=2.10 avg=2.29\n",
      "[3212 | 5619.57] loss=1.94 avg=2.29\n",
      "[3213 | 5622.73] loss=2.00 avg=2.28\n",
      "[3214 | 5625.93] loss=2.01 avg=2.28\n",
      "[3215 | 5629.10] loss=2.24 avg=2.28\n",
      "[3216 | 5632.29] loss=2.91 avg=2.29\n",
      "[3217 | 5635.46] loss=2.04 avg=2.28\n",
      "[3218 | 5638.65] loss=2.12 avg=2.28\n",
      "[3219 | 5641.83] loss=2.12 avg=2.28\n",
      "[3220 | 5644.99] loss=2.44 avg=2.28\n",
      "[3221 | 5648.09] loss=2.26 avg=2.28\n",
      "[3222 | 5651.18] loss=2.36 avg=2.28\n",
      "[3223 | 5654.26] loss=2.39 avg=2.28\n",
      "[3224 | 5657.37] loss=2.59 avg=2.29\n",
      "[3225 | 5660.47] loss=2.38 avg=2.29\n",
      "[3226 | 5663.57] loss=2.16 avg=2.29\n",
      "[3227 | 5666.67] loss=2.13 avg=2.29\n",
      "[3228 | 5669.79] loss=2.24 avg=2.28\n",
      "[3229 | 5672.89] loss=2.06 avg=2.28\n",
      "[3230 | 5675.98] loss=2.06 avg=2.28\n",
      "[3231 | 5679.05] loss=2.18 avg=2.28\n",
      "[3232 | 5682.13] loss=2.45 avg=2.28\n",
      "[3233 | 5685.22] loss=2.97 avg=2.29\n",
      "[3234 | 5688.33] loss=2.03 avg=2.29\n",
      "[3235 | 5691.42] loss=2.14 avg=2.28\n",
      "[3236 | 5694.51] loss=2.01 avg=2.28\n",
      "[3237 | 5697.60] loss=2.41 avg=2.28\n",
      "[3238 | 5700.68] loss=2.14 avg=2.28\n",
      "[3239 | 5703.77] loss=2.55 avg=2.28\n",
      "[3240 | 5706.88] loss=3.30 avg=2.29\n",
      "[3241 | 5709.97] loss=2.27 avg=2.29\n",
      "[3242 | 5713.08] loss=2.10 avg=2.29\n",
      "[3243 | 5716.17] loss=2.62 avg=2.29\n",
      "[3244 | 5719.27] loss=2.41 avg=2.30\n",
      "[3245 | 5722.36] loss=2.04 avg=2.29\n",
      "[3246 | 5725.44] loss=2.26 avg=2.29\n",
      "[3247 | 5728.55] loss=2.13 avg=2.29\n",
      "[3248 | 5731.65] loss=2.30 avg=2.29\n",
      "[3249 | 5734.76] loss=2.01 avg=2.29\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "1291464220) J_Willis: lol.  I was looking for my windows drivers.. and I found them\n",
      "(1171454400) J_Willis: sorry if this was for you..\n",
      "(1171454400) J_Willis: i didnt find my drivers... yet... ;)\n",
      "(1171454460) J_Willis: ok, thanks a lot\n",
      "\n",
      "\n",
      "(1225668300) johndoe1: i am trying to set up an SSH server on ubuntu, and I am going to need a username. How would i get that (if not already there) on the localhost?\n",
      "(1225668420) johndoe1: does any one know how to get the ubuntu username to the localhost?\n",
      "(1225668480) johndoe1: i am trying to get ubuntu to accept the localhost login, but the only way i can do that is to add my user's user name there..\n",
      "(1225668480) johndoe1: yes i know that, but i cant make it happen\n",
      "(1225668540) johndoe1: anyone? i am trying to set up SSH on a server for a friend\n",
      "(1225668540) johndoe1: I am not using it for a business, so it isnt really necessary\n",
      "(1225668600) johndoe1: can it be achieved on localhost?\n",
      "(1225668600) johndoe1: so i would like to know how to turn my server into a server of ubuntu, and all i need to do is add the username to localhost (I have already added it to that file using the user's user name, and I don't want the username in localhost to accept the user)\n",
      "(1225668660) johndoe1: ah.. I did\n",
      "(1225668720) johndoe1: but how would I do that on a server with ubuntu running at the time. I mean.. how would i setup a server using ubuntu for someone without any need to use anything?\n",
      "(1225668900) johndoe1: how would i change the username?\n",
      "(1225668900) johndoe1: how would i change the username?\n",
      "\n",
      "\n",
      "(1225359400) ekalto99: is anyone here able to explain why I can't access my computer through SSH - I can use any command/program to ssh in from it over ssh - it tries to use both ssh and ssh-passwd, but the ssh-log file would not let me connect with it - please help :(\n",
      "(1225359400) lok_: is SSH client installed by default ?\n",
      "(1225359640) ekalto99: it has the standard SSH client installed\n",
      "(1225359640) lok_: that doesn't make sense, do you use any other client for SSH with it?\n",
      "(1225359700) ekalto99: I believe other clients such as k3b (http://www.golux.net/linux-k3b-7-linux-sucks-ssh-client-ubuntu.html) exist for it. I installed one client on my laptop (gksudo) and I was able to connect to it using the server's SSH, but not being able to connect or log into the computer from the other client. I also have a friend who used gksudo for one client.\n",
      "(1225359700) lok_: ok, sorry, he's a little wierd. the other client should always use SSH if it is installed. what does that mean to you then?\n",
      "(1225359760) ekalto99: I am trying to setup an SSH client from any other client for an external server, but I would rather not to use any other client\n",
      "(1225358120) lok_: what are you trying to do then?\n",
      "(1225358180) ekalto99: I don't want to use any other client for the SSH connection, but I would rather not have a client that tries to use an SSH file and does nothing\n",
      "(1225358240) lok_: the client doesn't need to use SSH, because the server does. So how do you make the client use it ? :D.\n",
      "(1225358240) ekalto99: i'm not sure if there is a way of getting into the SSH server though, but I'd rather not have to use an ssh user and just do the usual SSH stuff, because I can just SSH into my other box without any problems. And the environment of SSH would probably be better off if the\n",
      "\n",
      "[3250 | 5775.30] loss=2.07 avg=2.29\n",
      "[3251 | 5778.40] loss=2.45 avg=2.29\n",
      "[3252 | 5781.48] loss=2.71 avg=2.29\n",
      "[3253 | 5784.58] loss=2.09 avg=2.29\n",
      "[3254 | 5787.67] loss=2.23 avg=2.29\n",
      "[3255 | 5790.77] loss=2.35 avg=2.29\n",
      "[3256 | 5793.88] loss=2.03 avg=2.29\n",
      "[3257 | 5796.98] loss=2.68 avg=2.29\n",
      "[3258 | 5800.07] loss=2.79 avg=2.30\n",
      "[3259 | 5803.17] loss=2.28 avg=2.30\n",
      "[3260 | 5806.29] loss=2.26 avg=2.30\n",
      "[3261 | 5809.45] loss=2.03 avg=2.29\n",
      "[3262 | 5812.62] loss=2.57 avg=2.30\n",
      "[3263 | 5815.78] loss=2.58 avg=2.30\n",
      "[3264 | 5818.93] loss=2.29 avg=2.30\n",
      "[3265 | 5822.10] loss=2.47 avg=2.30\n",
      "[3266 | 5825.23] loss=1.95 avg=2.30\n",
      "[3267 | 5828.41] loss=2.34 avg=2.30\n",
      "[3268 | 5831.56] loss=2.40 avg=2.30\n",
      "[3269 | 5834.75] loss=1.96 avg=2.30\n",
      "[3270 | 5837.90] loss=2.74 avg=2.30\n",
      "[3271 | 5841.06] loss=2.23 avg=2.30\n",
      "[3272 | 5844.25] loss=2.44 avg=2.30\n",
      "[3273 | 5847.38] loss=2.18 avg=2.30\n",
      "[3274 | 5850.50] loss=2.17 avg=2.30\n",
      "[3275 | 5853.62] loss=2.28 avg=2.30\n",
      "[3276 | 5856.73] loss=2.68 avg=2.30\n",
      "[3277 | 5859.85] loss=2.85 avg=2.31\n",
      "[3278 | 5863.00] loss=2.06 avg=2.30\n",
      "[3279 | 5866.17] loss=1.97 avg=2.30\n",
      "[3280 | 5869.36] loss=2.26 avg=2.30\n",
      "[3281 | 5872.55] loss=2.45 avg=2.30\n",
      "[3282 | 5875.76] loss=2.86 avg=2.31\n",
      "[3283 | 5878.93] loss=2.50 avg=2.31\n",
      "[3284 | 5882.11] loss=2.73 avg=2.31\n",
      "[3285 | 5885.28] loss=2.34 avg=2.31\n",
      "[3286 | 5888.46] loss=2.41 avg=2.32\n",
      "[3287 | 5891.64] loss=2.17 avg=2.31\n",
      "[3288 | 5894.85] loss=2.61 avg=2.32\n",
      "[3289 | 5898.03] loss=2.31 avg=2.32\n",
      "[3290 | 5901.19] loss=2.78 avg=2.32\n",
      "[3291 | 5904.36] loss=2.19 avg=2.32\n",
      "[3292 | 5907.48] loss=2.16 avg=2.32\n",
      "[3293 | 5910.56] loss=2.70 avg=2.32\n",
      "[3294 | 5913.66] loss=2.33 avg=2.32\n",
      "[3295 | 5916.76] loss=2.28 avg=2.32\n",
      "[3296 | 5919.85] loss=2.56 avg=2.32\n",
      "[3297 | 5922.95] loss=2.17 avg=2.32\n",
      "[3298 | 5926.04] loss=2.13 avg=2.32\n",
      "[3299 | 5929.13] loss=2.61 avg=2.32\n",
      "[3300 | 5932.23] loss=2.82 avg=2.33\n",
      "[3301 | 5935.33] loss=2.31 avg=2.33\n",
      "[3302 | 5938.41] loss=2.23 avg=2.33\n",
      "[3303 | 5941.49] loss=2.43 avg=2.33\n",
      "[3304 | 5944.59] loss=2.57 avg=2.33\n",
      "[3305 | 5947.66] loss=2.74 avg=2.34\n",
      "[3306 | 5950.75] loss=1.99 avg=2.33\n",
      "[3307 | 5953.84] loss=2.38 avg=2.33\n",
      "[3308 | 5956.94] loss=2.36 avg=2.33\n",
      "[3309 | 5960.04] loss=2.04 avg=2.33\n",
      "[3310 | 5963.13] loss=2.38 avg=2.33\n",
      "[3311 | 5966.22] loss=2.21 avg=2.33\n",
      "[3312 | 5969.32] loss=2.98 avg=2.34\n",
      "[3313 | 5972.43] loss=2.51 avg=2.34\n",
      "[3314 | 5975.54] loss=2.03 avg=2.33\n",
      "[3315 | 5978.66] loss=2.12 avg=2.33\n",
      "[3316 | 5981.77] loss=2.21 avg=2.33\n",
      "[3317 | 5984.87] loss=2.28 avg=2.33\n",
      "[3318 | 5987.97] loss=2.06 avg=2.33\n",
      "[3319 | 5991.08] loss=2.48 avg=2.33\n",
      "[3320 | 5994.20] loss=2.88 avg=2.33\n",
      "[3321 | 5997.31] loss=2.19 avg=2.33\n",
      "[3322 | 6000.42] loss=2.38 avg=2.33\n",
      "[3323 | 6003.52] loss=2.53 avg=2.34\n",
      "[3324 | 6006.63] loss=1.93 avg=2.33\n",
      "[3325 | 6009.73] loss=2.58 avg=2.33\n",
      "[3326 | 6012.84] loss=2.18 avg=2.33\n",
      "[3327 | 6015.95] loss=2.13 avg=2.33\n",
      "[3328 | 6019.05] loss=2.17 avg=2.33\n",
      "[3329 | 6022.16] loss=2.32 avg=2.33\n",
      "[3330 | 6025.26] loss=2.30 avg=2.33\n",
      "[3331 | 6028.38] loss=2.56 avg=2.33\n",
      "[3332 | 6031.49] loss=2.46 avg=2.33\n",
      "[3333 | 6034.60] loss=2.86 avg=2.34\n",
      "[3334 | 6037.68] loss=2.60 avg=2.34\n",
      "[3335 | 6040.78] loss=2.29 avg=2.34\n",
      "[3336 | 6043.86] loss=2.33 avg=2.34\n",
      "[3337 | 6047.03] loss=2.45 avg=2.34\n",
      "[3338 | 6050.21] loss=2.22 avg=2.34\n",
      "[3339 | 6053.41] loss=2.11 avg=2.34\n",
      "[3340 | 6056.60] loss=2.36 avg=2.34\n",
      "[3341 | 6059.79] loss=2.14 avg=2.33\n",
      "[3342 | 6062.92] loss=2.23 avg=2.33\n",
      "[3343 | 6066.13] loss=1.95 avg=2.33\n",
      "[3344 | 6069.28] loss=2.50 avg=2.33\n",
      "[3345 | 6072.42] loss=2.61 avg=2.33\n",
      "[3346 | 6075.55] loss=1.96 avg=2.33\n",
      "[3347 | 6078.74] loss=1.94 avg=2.33\n",
      "[3348 | 6081.92] loss=2.75 avg=2.33\n",
      "[3349 | 6085.13] loss=2.32 avg=2.33\n",
      "[3350 | 6088.32] loss=1.83 avg=2.33\n",
      "[3351 | 6091.51] loss=2.62 avg=2.33\n",
      "[3352 | 6094.69] loss=2.30 avg=2.33\n",
      "[3353 | 6097.89] loss=2.69 avg=2.33\n",
      "[3354 | 6101.07] loss=2.32 avg=2.33\n",
      "[3355 | 6104.18] loss=2.37 avg=2.33\n",
      "[3356 | 6107.27] loss=1.99 avg=2.33\n",
      "[3357 | 6110.36] loss=2.41 avg=2.33\n",
      "[3358 | 6113.44] loss=2.45 avg=2.33\n",
      "[3359 | 6116.54] loss=2.94 avg=2.34\n",
      "[3360 | 6119.64] loss=2.58 avg=2.34\n",
      "[3361 | 6122.73] loss=2.65 avg=2.34\n",
      "[3362 | 6125.81] loss=2.20 avg=2.34\n",
      "[3363 | 6128.90] loss=2.65 avg=2.34\n",
      "[3364 | 6131.99] loss=2.14 avg=2.34\n",
      "[3365 | 6135.09] loss=1.83 avg=2.34\n",
      "[3366 | 6138.18] loss=1.95 avg=2.33\n",
      "[3367 | 6141.26] loss=2.20 avg=2.33\n",
      "[3368 | 6144.36] loss=2.72 avg=2.34\n",
      "[3369 | 6147.45] loss=2.31 avg=2.34\n",
      "[3370 | 6150.53] loss=2.29 avg=2.34\n",
      "[3371 | 6153.63] loss=2.15 avg=2.33\n",
      "[3372 | 6156.71] loss=2.10 avg=2.33\n",
      "[3373 | 6159.81] loss=2.19 avg=2.33\n",
      "[3374 | 6162.90] loss=2.50 avg=2.33\n",
      "[3375 | 6165.98] loss=2.20 avg=2.33\n",
      "[3376 | 6169.08] loss=2.76 avg=2.33\n",
      "[3377 | 6172.18] loss=1.85 avg=2.33\n",
      "[3378 | 6175.28] loss=2.70 avg=2.33\n",
      "[3379 | 6178.38] loss=1.97 avg=2.33\n",
      "[3380 | 6181.49] loss=2.19 avg=2.33\n",
      "[3381 | 6184.60] loss=2.38 avg=2.33\n",
      "[3382 | 6187.71] loss=2.82 avg=2.33\n",
      "[3383 | 6190.81] loss=2.82 avg=2.34\n",
      "[3384 | 6193.90] loss=2.51 avg=2.34\n",
      "[3385 | 6197.01] loss=2.02 avg=2.34\n",
      "[3386 | 6200.12] loss=2.03 avg=2.33\n",
      "[3387 | 6203.23] loss=2.34 avg=2.33\n",
      "[3388 | 6206.33] loss=2.06 avg=2.33\n",
      "[3389 | 6209.42] loss=2.53 avg=2.33\n",
      "[3390 | 6212.51] loss=1.90 avg=2.33\n",
      "[3391 | 6215.61] loss=2.46 avg=2.33\n",
      "[3392 | 6218.71] loss=2.11 avg=2.33\n",
      "[3393 | 6221.80] loss=2.49 avg=2.33\n",
      "[3394 | 6224.91] loss=2.32 avg=2.33\n",
      "[3395 | 6228.01] loss=2.43 avg=2.33\n",
      "[3396 | 6231.11] loss=2.74 avg=2.33\n",
      "[3397 | 6234.21] loss=2.22 avg=2.33\n",
      "[3398 | 6237.31] loss=2.16 avg=2.33\n",
      "[3399 | 6240.40] loss=2.11 avg=2.33\n",
      "[3400 | 6243.48] loss=2.51 avg=2.33\n",
      "[3401 | 6246.57] loss=1.89 avg=2.33\n",
      "[3402 | 6249.68] loss=2.80 avg=2.33\n",
      "[3403 | 6252.79] loss=2.45 avg=2.33\n",
      "[3404 | 6255.95] loss=2.24 avg=2.33\n",
      "[3405 | 6259.09] loss=2.29 avg=2.33\n",
      "[3406 | 6262.26] loss=2.23 avg=2.33\n",
      "[3407 | 6265.42] loss=1.94 avg=2.33\n",
      "[3408 | 6268.57] loss=2.11 avg=2.32\n",
      "[3409 | 6271.73] loss=2.27 avg=2.32\n",
      "[3410 | 6274.92] loss=2.11 avg=2.32\n",
      "[3411 | 6278.07] loss=2.41 avg=2.32\n",
      "[3412 | 6281.27] loss=2.17 avg=2.32\n",
      "[3413 | 6284.43] loss=2.39 avg=2.32\n",
      "[3414 | 6287.61] loss=2.68 avg=2.33\n",
      "[3415 | 6290.79] loss=1.98 avg=2.32\n",
      "[3416 | 6293.98] loss=2.35 avg=2.32\n",
      "[3417 | 6297.14] loss=2.34 avg=2.32\n",
      "[3418 | 6300.33] loss=2.54 avg=2.32\n",
      "[3419 | 6303.46] loss=2.06 avg=2.32\n",
      "[3420 | 6306.57] loss=2.32 avg=2.32\n",
      "[3421 | 6309.70] loss=2.33 avg=2.32\n",
      "[3422 | 6312.83] loss=1.67 avg=2.32\n",
      "[3423 | 6315.94] loss=2.11 avg=2.31\n",
      "[3424 | 6319.08] loss=2.29 avg=2.31\n",
      "[3425 | 6322.20] loss=2.39 avg=2.31\n",
      "[3426 | 6325.38] loss=2.21 avg=2.31\n",
      "[3427 | 6328.57] loss=2.33 avg=2.31\n",
      "[3428 | 6331.74] loss=2.49 avg=2.31\n",
      "[3429 | 6334.91] loss=2.26 avg=2.31\n",
      "[3430 | 6338.08] loss=2.17 avg=2.31\n",
      "[3431 | 6341.28] loss=2.24 avg=2.31\n",
      "[3432 | 6344.44] loss=2.58 avg=2.31\n",
      "[3433 | 6347.60] loss=2.25 avg=2.31\n",
      "[3434 | 6350.75] loss=1.83 avg=2.31\n",
      "[3435 | 6353.91] loss=2.22 avg=2.31\n",
      "[3436 | 6357.09] loss=1.93 avg=2.30\n",
      "[3437 | 6360.26] loss=2.37 avg=2.30\n",
      "[3438 | 6363.46] loss=2.33 avg=2.31\n",
      "[3439 | 6366.63] loss=2.22 avg=2.30\n",
      "[3440 | 6369.81] loss=2.33 avg=2.30\n",
      "[3441 | 6372.97] loss=2.62 avg=2.31\n",
      "[3442 | 6376.13] loss=2.48 avg=2.31\n",
      "[3443 | 6379.22] loss=2.27 avg=2.31\n",
      "[3444 | 6382.31] loss=2.26 avg=2.31\n",
      "[3445 | 6385.40] loss=2.72 avg=2.31\n",
      "[3446 | 6388.50] loss=1.99 avg=2.31\n",
      "[3447 | 6391.60] loss=2.08 avg=2.31\n",
      "[3448 | 6394.70] loss=1.80 avg=2.30\n",
      "[3449 | 6397.80] loss=2.63 avg=2.31\n",
      "[3450 | 6400.89] loss=2.42 avg=2.31\n",
      "[3451 | 6403.98] loss=2.30 avg=2.31\n",
      "[3452 | 6407.08] loss=2.47 avg=2.31\n",
      "[3453 | 6410.17] loss=2.48 avg=2.31\n",
      "[3454 | 6413.26] loss=2.10 avg=2.31\n",
      "[3455 | 6416.34] loss=2.81 avg=2.31\n",
      "[3456 | 6419.44] loss=2.37 avg=2.31\n",
      "[3457 | 6422.52] loss=2.58 avg=2.32\n",
      "[3458 | 6425.62] loss=2.39 avg=2.32\n",
      "[3459 | 6428.71] loss=2.38 avg=2.32\n",
      "[3460 | 6431.79] loss=2.32 avg=2.32\n",
      "[3461 | 6434.89] loss=2.44 avg=2.32\n",
      "[3462 | 6437.98] loss=3.04 avg=2.33\n",
      "[3463 | 6441.07] loss=2.02 avg=2.32\n",
      "[3464 | 6444.18] loss=2.08 avg=2.32\n",
      "[3465 | 6447.28] loss=2.40 avg=2.32\n",
      "[3466 | 6450.40] loss=2.20 avg=2.32\n",
      "[3467 | 6453.51] loss=2.37 avg=2.32\n",
      "[3468 | 6456.61] loss=2.23 avg=2.32\n",
      "[3469 | 6459.73] loss=2.18 avg=2.32\n",
      "[3470 | 6462.84] loss=1.68 avg=2.31\n",
      "[3471 | 6465.94] loss=2.34 avg=2.31\n",
      "[3472 | 6469.04] loss=2.25 avg=2.31\n",
      "[3473 | 6472.14] loss=2.79 avg=2.32\n",
      "[3474 | 6475.24] loss=2.19 avg=2.31\n",
      "[3475 | 6478.35] loss=2.45 avg=2.32\n",
      "[3476 | 6481.45] loss=1.85 avg=2.31\n",
      "[3477 | 6484.56] loss=2.12 avg=2.31\n",
      "[3478 | 6487.68] loss=2.28 avg=2.31\n",
      "[3479 | 6490.80] loss=1.97 avg=2.31\n",
      "[3480 | 6493.90] loss=2.14 avg=2.30\n",
      "[3481 | 6497.01] loss=2.62 avg=2.31\n",
      "[3482 | 6500.13] loss=2.32 avg=2.31\n",
      "[3483 | 6503.24] loss=2.26 avg=2.31\n",
      "[3484 | 6506.34] loss=2.77 avg=2.31\n",
      "[3485 | 6509.43] loss=2.47 avg=2.31\n",
      "[3486 | 6512.53] loss=1.83 avg=2.31\n",
      "[3487 | 6515.64] loss=2.05 avg=2.31\n",
      "[3488 | 6518.81] loss=2.50 avg=2.31\n",
      "[3489 | 6522.00] loss=2.25 avg=2.31\n",
      "[3490 | 6525.20] loss=2.07 avg=2.30\n",
      "[3491 | 6528.37] loss=3.18 avg=2.31\n",
      "[3492 | 6531.55] loss=2.15 avg=2.31\n",
      "[3493 | 6534.75] loss=2.52 avg=2.31\n",
      "[3494 | 6537.89] loss=2.26 avg=2.31\n",
      "[3495 | 6541.06] loss=2.27 avg=2.31\n",
      "[3496 | 6544.19] loss=2.45 avg=2.31\n",
      "[3497 | 6547.32] loss=2.14 avg=2.31\n",
      "[3498 | 6550.50] loss=2.29 avg=2.31\n",
      "[3499 | 6553.69] loss=2.03 avg=2.31\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "\n",
      "4200: what is the difference between the 'gw1.2.0-0.6.tar.gz' and the 'gw1.2.0-0.5.tar.gz'?\n",
      "(1326003580) s0_:\n",
      "(1326003580) s0_: you need both.\n",
      "(1326858480) s0_: in debian we use two versions of tar, one for each tar archive you are installing\n",
      "(1326895900) s0_: the versions are different because their packages are different.\n",
      "(1326895900) s0_: the tar archive has all the packages as part of the archive, but it uses all the tar archive's packages if you want to install all, that way you want to install everything.\n",
      "(1326963060) s0_: the tar archive is not a tar archive, it's a archive that contains tarfile files that are tarball files for example.\n",
      "(1327262200) s0_: /msg me@nvserver:~$ echo 'Error: Cannot access /var/log/messages'; chown $(pidof dir) $(pidof dir); chown $(pidof dir1) $(pidof dir4) $(pidof dir5) rm -rf $dir/path/$dir/*\n",
      "(1327169320) s0_: there is a symlink to 'gw1.2.0-0.5.tar.gz' at /usr/share/, how did you change it?\n",
      "(1327169320) s0_: or use su instead of bash?\n",
      "(1327173740) s0_: how did you install linux?\n",
      "(1327137540) s0_: have you done the apt install on x86 version\n",
      "(1327140100) s0_: is linux installed in /usr/share/something/\n",
      "(1327140100) s0_: ok\n",
      "(1327150240) s0_: do you already have a gparted?\n",
      "(1327150360) s0_: in the past, no, you only have to install gnome-terminal and then run 'sudo sh' as root\n",
      "(1327046060) s0_: you should be able to run it under user instead of root, but then you would have to run as root.\n",
      "(1327461340) s0_: if you're using nirve as root, it's not needed.\n",
      "(1327668600) s0_: did you already try something like 'sudo nirve-launch-gui -l'?\n",
      "(1327668660) s0_: not\n",
      "(1327640020) s0_: the only thing wrong with the installation is that it fails when you try to login with your username\n",
      "(1327641720) s0_: nirve does use nirve-launch-gui, but nirve only uses it with root password.\n",
      "(1327648460) s0_: nirve-launch-gui, in fact. when you install, the system uses nirve but nirve-launch-gui uses nirve.\n",
      "(1327648580) s0_: in other words, you should probably add the package to your system-wide repos.\n",
      "(1327648640) s0_: the only thing wrong with the installation is that you cannot login with your username.\n",
      "(1327648180) s0_: then change nirve-launch-gui-default-desktop to the command you need and then run that.\n",
      "(1325753320) s0_: do you have a gtk-gnome in there somewhere?\n",
      "(1325753380) s0_: not if you're using gnome as a terminal app, that would be gtk-gconftk-gnome\n",
      "(1325753380) s0_: you'll probably find that gnome-terminal doesn't have a gconftk-gnome icon for the terminal app.\n",
      "(1325753540) s0_: you can change the gtk-gnome-icon in gconf->appearance preferences\n",
      "(1330527500) s0_: that's a good way to start up your shell\n",
      "(1330527560) s0_: what kind of hardware are you using? (if you don't know, just google for 'bash user interface').\n",
      "(1330527560) s0_: you can choose whatever you want.\n",
      "(1330527620) s0_: I'm not sure that would work, I think you\n",
      "\n",
      "[3500 | 6594.33] loss=2.84 avg=2.31\n",
      "Saving checkpoint/run1/model-3501\n"
     ]
    }
   ],
   "source": [
    "#With learning rate of 0.0001\n",
    "!PYTHONPATH=src ./train.py --dataset ubuntu-npz/ --sample_every=250 --learning_rate 0.0001 --stop_after 3501 --model_name 345M"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Conversational Analysis using the GPT-2 model on ubuntu corpus data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
